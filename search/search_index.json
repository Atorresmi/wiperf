{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"wiperf: An Open Source UX Performance Probe Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana Data Server The core focus of this project is the probe platform that gathers the network performance data in which we are interested. However, the data server is a critical component that allows visualization of that performance data. High-level configuration details will be provided to \"get you going\", but detailed information about the operation of these platforms is beyond the scope f this project. Both of the data servers supported are \"NoSQL\" servers, which means that no data structures have to be pre-defined in database tables. This means we can send our data structures, that contain network performance data, to the server with very little set-up compared to traditional database servers. As long as we have a valid set of credentials for the data server, we can just send a data in JSON data structures over HTTPS in whatever structure we choose. A database query language on the data server allows us to retrieve and graph the data collected by the wiperf probe. Splunk Splunk is supported on all popular operating systems and is very easy to set up on your server of choice. It acts as both the data store and visualization platform. Splunk is a commercial, rather than open-source, product. The volume of data returned by the probe is very low, so the free tier of Splunk may be used to gather and report on data. For details on how to set up a Splunk server, visit this document: [Splunk build guide][splunk_build] (it's a lot easier than you might expect...honestly) Splunk product web site: https://www.splunk.com/ InfluxDB/Grafana Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data server from which to pull its network performance data. To meet this requirement, the Influx database server is used. Like Grafana, InfluxDB is also an open-source package. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana web site (v6.7): https://grafana.com/ Influx web site (v.1.8): https://www.influxdata.com Workflow to Setup Wiperf The workflow to get Wiperf fully operational consists if a number of steps that break down in to two main areas: Probe setup (the RPi or WLAN Pi) Data server setup (the Splunk or Influx/Grafana server) The Data server setup tends to be a task that needs completion only once (or at least very infrequently). Conversely, some or all of the probe setup will need to be completed each time a probe is deployed - this is mainly due to the fact that in each environment in which it is deployed, the connectivity for the probe will vary (e.g. different SSID, different network connection type). Here is an overview of the workflow, with links to documentation for each step: Data server setup: Prepare a server platform Obtain the data server application software Install the data server application Configure the data server application Probe setup: Obtain a probe platform (Raspberry Pi or WLAN Pi) Prepare the platform for the wiperf software Install the wiperf software Configure the wiperf software Deploy & test the wiperf probe In addition to the setup and deployment of the components, there may also be a requirement to troubleshoot the setup. This may involve: Troubleshooting steps Review known issues Further Documentation References Configuration file parameters Data points sent by the probe to the data server platform ![Speedtest Report][speedtest_image] Credits Thanks to Kristian Roberts for his invaluable input, testing and guidance on Splunk. He kicked this whole thing off and it definitely wouldn't have happened without him. A top bloke. Caveats This free software is provided for you to use at your own risk. There are no guarantees around its operation, suitability or the data that it provides. Please consult the [license file][license] shipped with this software. Developer Nigel Bowden (WifiNigel): https://twitter.com/wifinigel","title":"Home"},{"location":"#wiperf-an-open-source-ux-performance-probe","text":"Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"wiperf: An Open Source UX Performance Probe"},{"location":"#data-server","text":"The core focus of this project is the probe platform that gathers the network performance data in which we are interested. However, the data server is a critical component that allows visualization of that performance data. High-level configuration details will be provided to \"get you going\", but detailed information about the operation of these platforms is beyond the scope f this project. Both of the data servers supported are \"NoSQL\" servers, which means that no data structures have to be pre-defined in database tables. This means we can send our data structures, that contain network performance data, to the server with very little set-up compared to traditional database servers. As long as we have a valid set of credentials for the data server, we can just send a data in JSON data structures over HTTPS in whatever structure we choose. A database query language on the data server allows us to retrieve and graph the data collected by the wiperf probe.","title":"Data Server"},{"location":"#splunk","text":"Splunk is supported on all popular operating systems and is very easy to set up on your server of choice. It acts as both the data store and visualization platform. Splunk is a commercial, rather than open-source, product. The volume of data returned by the probe is very low, so the free tier of Splunk may be used to gather and report on data. For details on how to set up a Splunk server, visit this document: [Splunk build guide][splunk_build] (it's a lot easier than you might expect...honestly) Splunk product web site: https://www.splunk.com/","title":"Splunk"},{"location":"#influxdbgrafana","text":"Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data server from which to pull its network performance data. To meet this requirement, the Influx database server is used. Like Grafana, InfluxDB is also an open-source package. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana web site (v6.7): https://grafana.com/ Influx web site (v.1.8): https://www.influxdata.com","title":"InfluxDB/Grafana"},{"location":"#workflow-to-setup-wiperf","text":"The workflow to get Wiperf fully operational consists if a number of steps that break down in to two main areas: Probe setup (the RPi or WLAN Pi) Data server setup (the Splunk or Influx/Grafana server) The Data server setup tends to be a task that needs completion only once (or at least very infrequently). Conversely, some or all of the probe setup will need to be completed each time a probe is deployed - this is mainly due to the fact that in each environment in which it is deployed, the connectivity for the probe will vary (e.g. different SSID, different network connection type). Here is an overview of the workflow, with links to documentation for each step: Data server setup: Prepare a server platform Obtain the data server application software Install the data server application Configure the data server application Probe setup: Obtain a probe platform (Raspberry Pi or WLAN Pi) Prepare the platform for the wiperf software Install the wiperf software Configure the wiperf software Deploy & test the wiperf probe In addition to the setup and deployment of the components, there may also be a requirement to troubleshoot the setup. This may involve: Troubleshooting steps Review known issues","title":"Workflow to Setup Wiperf"},{"location":"#further-documentation-references","text":"Configuration file parameters Data points sent by the probe to the data server platform ![Speedtest Report][speedtest_image]","title":"Further Documentation References"},{"location":"#credits","text":"Thanks to Kristian Roberts for his invaluable input, testing and guidance on Splunk. He kicked this whole thing off and it definitely wouldn't have happened without him. A top bloke.","title":"Credits"},{"location":"#caveats","text":"This free software is provided for you to use at your own risk. There are no guarantees around its operation, suitability or the data that it provides. Please consult the [license file][license] shipped with this software.","title":"Caveats"},{"location":"#developer","text":"Nigel Bowden (WifiNigel): https://twitter.com/wifinigel","title":"Developer"},{"location":"about/","text":"About Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"About"},{"location":"about/#about","text":"Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"About"},{"location":"config.ini/","text":"Wiperf - config.ini reference guide (In development - old version) Background The config.ini file controls the operation of the Wiperf utility. It has many options available for maximum flexiblity, but some may need some clarification. Many options will be fine using the defaults that are supplied with the installed package. However, some will definitely require configuration as they may require values such as IP addresses and port numbers which will vary in each instance where Wiperf is used. The config.ini file is located in the directory : /home/wlanpi/wiperf The file is organised in a number of sections that relate to different areas of operation. Each section begins with a name enclosed is square brackets, like this: [Speedtest] Within each section are a number of configurable parameters that are in the format: parameter: value You may also see some lines that begin with a semi-colon. These are comments and have no effect on the operation of Wiperf. You may add, remove or change these as you wish. Here is an example comment: ; wlan interface name set this as per the output of an iwconfig command (usually wlan0) Parameter Reference Guide We'll take a look at each section of the config file and provide some guidance on suitable parameter values: General Section wlan_if mgt_if platform exporter_type splunk_host splunk_port splunk_token influx_host influx_port influx_username influx_password influx_database influx2_host influx2_port influx2_token influx2_bucket influx2_org test_interval test_offset connectivity_lookup location data_format data_dir data_transport debug cfg_url cfg_username cfg_password cfg_token cfg_refresh_interval unit_bouncer Network_Test Section network_data_file Speetest Section enabled server_id http_proxy https_proxy no_proxy speedtest_data_file Ping_Test Section enabled ping_host1 ping_host2 ping_host3 ping_host4 ping_host5 ping_count ping_data_file Iperf3_tcp_test Section enabled server_hostname port duration iperf3_tcp_data_file Iperf3_udp_test Section enabled server_hostname port duration bandwidth iperf3_udp_data_file DNS_test Section enabled dns_target1 dns_target2 dns_target3 dns_target4 dns_target5 dns_data_file HTTP_test Section enabled http_target1 http_target2 http_target3 http_target4 http_target5 http_data_file DHCP_test Section enabled mode dhcp_data_file [General] Section Note: any changes to this section on the WLANPi should only be made when it is running in classic mode (not while in Wiperf mode). wlan_if This parameter contains the name of the WLAN interface on the Pi. This will almost always be 'wlan0', but is provided in case of new use-cases in the future. You can see the WLAN interface name by running the 'ifconfig' command from the CLI of the Pi Default setting: wlan_if: wlan0 top mgt_if When performance tests have been completed, the results data needs to be sent to a reporting server (e.g. Splunk/InfluxDb). This parameter configures the interface over which this management traffic needs to be sent. Getting this parameter correct for your environment is very important to ensure that test results data makes it back to your reporting server. The available options are: wlan0 (the first available WLAN port - usually a USB dongle plugged in to the WLANPi) eth0 (the internal Ethernet port of the WLANPi) zt (Zerotier (the virtual network service) is installed and used to connect to the reporting server) The WANPi is configured to assign a higher cost default route to eth0 by default so that all traffic (tests & test results) will choose the default route provided by wlan0. If eth0 is used as the path to return test results to the reporting server, then a static route is injected in to the WLANPi route table on start-up to ensure correct routing. If this parameter is not correctly set, then results data may not make it back to the reporting server. Default setting: mgt_if: wlan0 top platform (Deprecated) (This setting is now deprecated (and unused) - it has been included for historical reference) Wiperf is supported on both the WLANPi and Raspberry Pi platforms. The available options are: wlanpi rpi Default setting: platform: wlanpi top exporter_type Wiperf supports a number of remote data repositories that can be used as targets to store test result data. The available options are: splunk influxdb influxdb2 Default setting: exporter_type: splunk top splunk_host This is the hostname or IP address of the Splunk platform where test result data is sent to. If the hostname of the Splunk server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): splunk_host: top splunk_port The network port used to send updates to the Splunk server. By default this is 8088, but this may be changed within the Splunk application if an alternative port is required for your environment Default setting: splunk_port: 8088 top splunk_token Splunk will only receive HEC updates from devices that are authorised to send it data. Splunk uses tokens to decide if an update is from a valid device. To view available (or create) tokens within Splunk, view the menu option: \"Settings > Data > Data Inputs > HTTP Event Collector\" Here is example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d Default setting (none): splunk_token: top influx_host This is the hostname or IP address of the Influx (v1.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): influx_host: top influx_port The network port used to send updates to the Influx (v1.x) server. By default this is 8086, but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx_port: 8086 top influx_username The username that will be used to access the Influx (v1.x) server DB to post results data. This username must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_username: top influx_password The password that will be used to access the Influx (v1.x) server DB to post results data. This password must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_password: top influx_database The name of the database on the Influx (v1.x) server DB where wiperf will post results data. This database must have been created on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_database: top influx2_host This is the hostname or IP address of the Influx (v2.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your InfluxDb2 server in the Zerotier dashboard for your network) Default setting (none): influx2_host: top influx2_port The network port used to send updates to the Influx (v2.x) server. By default this is 443 (this assumes the cloud service is used), but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx2_port: 443 top influx2_token InfluxDB2 allows the use of authentication tokens when sending results data to the InfluxDB2 server. This provides an easier authentication methods than using a username and password. Once a token has been created on InfluxDB server, it can be used by wiperf to authenticate the results data sent to the InfluxDB2 server Default setting (none): influx2_token: top influx2_bucket Data sent to the InfluxDB2 server from wiperf is stored in a \"bucket\" in the data store. This field is used to configure the bucket to which wiperf should send it's data. Default setting (none): influx2_bucket: top influx2_org The InfluxDB2 server can be partitioned in to a number of organizations, which contain the buckets where data will be stored. Use this field to configure wiperf to send data to the correct organisation on InfluxDB2. Default setting (none): influx2_org: top test_interval (WLANPi only) This is the interval (in minutes) at which we would like to run the performance tests. The recommened minimum is 5, which is also the default. (Note: if this setting is too low, scheduled tests may try to run before the previous test sequence has completed, which could cause gaps in your data) Default setting: test_interval: 5 top test_offset (WLANPi only) By default test run at the interval specified by the test_interval parameter, which is referenced to the to of the hours (e.g. 5 mins interval will run at 5, 10, 15, 20, 25...etc. mins past the hour). If multiple proes are running, it mau be useful to stagger their start times. By setting test_offset to a value of one, this will offset all test start times by 1 minutes (i.e. 6,11,16,21,26...etc. mins past the hour) The default value is zero which means that the default 5,10,15,20... run pattern will be used. Default setting: test_offset: 0 top connectivity_lookup At the start of each test cycle, a DNS lookup is performed to ensure that DNS is working. By default this is 'google.com' (this was 'bbc.co.uk' on older versions of wiperf). This may be set to any required hostname lookup in instances when the default site may not be available for some reason (e.g. DNS restrictions due to filtering or lack of Internet access) Default setting: connectivity_lookup: google.com top location This is a string that can be added to assist with report filtering, if required. Its default value in an empty string. It could be be used in an expression within your reports to filter units based on a location field (for instance) Default setting: location: top data_format (Not currently operational) Wiperf has the capability to output data in a number of formats. The current options are: csv or json However this field is not currently used, as selecting the 'hec' transport mode (the only supported transport currently) over-rides this field. The value in this filed is currently irrelevant, but it s recommended to leave it at the default setting of json Default setting: data_format: json top data_dir This is the directory on the WLANPi/RPi where test result data is dumped. Do not change this value from the default . This field is provided for future configuration options if required. Default setting: data_dir: /home/wlanpi/wiperf/data top data_transport The currently supported data transport mode is hec . This is the HTTP Event Collector supported natively within the Splunk server. Other transport modes will be suported in the future, but currently this should be left at the default setting of `hec . (Note: the transport method forwarder is also a valid transport method which provides support for very early versions of this code which used the Splunk Univeral Forwarder. Use of this method is deprecated and will be removed in the near future. Anyone still using the UF should move to using hec ASAP) Default setting: data_transport: hec top debug To enable enhanced logging in the agent.log file, change this setting to \"on\" Default setting: debug: off top cfg_url If using centralized configuration file retrieval, this field specifies the full URL of the config file on the remote repo. (Note that on GitHub this is the URL of the raw file itself) If this field is not set, then centralized configuration retrieval is disabled Default setting (none): cfg_url: top cfg_username If username/pasword credentials are used to retrieve the centralized config, this field specifies the usename to be used. Default setting (none): cfg_username: top cfg_password If username/pasword credentials are used to retrieve the centralized config, this field specifies the password to be used. Default setting (none): cfg_password: top cfg_token If a GitHub authentication token is used to retrieve the centralized config, this field specifies the token to be used. (Note: this is used instead of a username/pwd) Default setting (none): cfg_token: top cfg_refresh_interval This field specifies how often (in seconds) the centralized config file should be retrieved . Recommended value: 900 (i.e. 15 mins) Default setting (none): cfg_refresh_interval: top unit_bouncer If you need to bounce (reboot) the unit for some reason on a regular basis, this field can be used to signal to the WLAN Pi each hour at which it must reboot. The field is a comma separated string that lists the hours at which the unit must reboot (in 24-hour format). The number-format and comma separation are important to get right! Note that the reboot is not exactly on the hour, but will occur at the end of the next test cycle that it is within the hour where a reboot is required. It will only happen once per hour. Example: the following config will reboot at midnight, 04:00, 08:00, 12:00, 16:00: unit_bouncer: 00, 06, 12, 18 This parameter is commented out by default as it is obviously not something you necessarilly want to switch on accidentally. Default setting: ; unit_bouncer: 00, 06, 12, 18 top [Network_Test] Section network_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for network tests in remote data repositories (e.g. Splunk, InfluxDB) Default setting: network_data_file: wiperf-network top [Speedtest] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no Speedtest is run. When enabled, a speedtest to the Ookla speedtest service is run each test cycle. Default setting: enabled: yes top server_id If you wish to specify a particular Ookla speedtest server that the test needs to be run against, you can enter its ID here. This must be the (numeric) server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If no value is specified, best server is used (default) Default setting: server_id: top http_proxy https_proxy no_proxy If proxy server access is required to run a speedtest, enter the proxy server details here for https & https e.g. https_proxy: http://10.1.1.1:8080 For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) e.g. no_proxy: \"mail.local, intranet.local\" Default settings: http_proxy: https_proxy: no_proxy: top speedtest_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for Speedtests in the reporting server (e.g. Splunk/InfluxDB) Default setting: speedtest_data_file: wiperf-speedtest top [Ping_Test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no ping tests are run. When enabled, up to 5 entries will be targetted with an ICMP ping and the RRT times recorded Default setting: enabled: yes top ping_host1 IP address or hostname of first ping target. No target details = no test run Default setting: ping_host1: bbc.co.uk top ping_host2 IP address or hostname of second ping target. No target details = no test run Default setting: ping_host2: cisco.com top ping_host3 IP address or hostname of third ping target. No target details = no test run Default setting: ping_host3: google.com top ping_host4 IP address or hostname of fourth ping target. No target details = no test run Default setting: ping_host4: top ping_host5 IP address or hostname of fifth ping target. No target details = no test run Default setting: ping_host2: top ping_count The number of pings to send for each ping target Default setting: ping_count: 10 top ping_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for ping tests in the reporting server (e.g. Splunk.InfuxDB) Default setting: ping_data_file: wiperf-ping top [Iperf3_tcp_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no tcp iperf3 test is run. When enabled, a tcp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs) Default setting: enabled: yes top server_hostname The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: 192.168.0.14 top port The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top duration The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 20 top iperf3_tcp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for tcp iperf3 tests in Splunk Default setting: iperf3_tcp_data_file: wiperf-iperf3-tcp top [Iperf3_udp_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no udp iperf3 test is run. When enabled, a udp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs), attempting to achieve a data transfer rate of bandwidth bps. Default setting: enabled: yes top server_hostname The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: 192.168.0.14 top port The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top duration The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 20 top bandwidth The data rate that will be attempted for the UDP iperf3 test in bps Default setting: bandwidth: 20000000 top iperf3_udp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for udp iperf3 tests in Splunk Default setting: iperf3_udp_data_file: wiperf-iperf3-udp top [DNS_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no DNS tests are run. When enabled, DNS tests are run for each of the dns_target paramters defined in this section. Any targets that have no value entered will be ignored. Default setting: enabled: yes top dns_target1 Hostname of first DNS target. No target details = no test run Default setting: dns_target1: bbc.co.uk top dns_target2 Hostname of second DNS target. No target details = no test run Default setting: dns_target2: cisco.com top dns_target3 Hostname of third DNS target. No target details = no test run Default setting: dns_target3: google.com top dns_target4 Hostname of fourth DNS target. No target details = no test run Default setting: dns_target4: top dns_target5 Hostname of fifth DNS target. No target details = no test run Default setting: dns_target5: top dns_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DNS tests in Splunk Default setting: dns_data_file: wiperf-dns top [HTTP_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no HTTP tests are run. When enabled, HTTP tests are run for each of the http_target paramters defined in this section. Any targets that have no value entered will be ignored. Targets must include the full url of each site to be queried (including http:// or https:// element). Valid site address examples: http://bbc.co.uk https://ebay.com A http get will be performed for each target and the result code returned. Default setting: enabled: yes top http_target1 Hostname of first HTTP target. No target details = no test run Default setting: http_target1: https://ebay.co.uk top http_target2 Hostname of second HTTP target. No target details = no test run Default setting: http_target2: http://twitter.com top http_target3 Hostname of third HTTP target. No target details = no test run Default setting: http_target3: https://facebook.com top http_target4 Hostname of fourth HTTP target. No target details = no test run Default setting: http_target4: https://instagram.com top http_target5 Hostname of fifth HTTP target. No target details = no test run Default setting: https://amazon.com http_target5: top http_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for HTTP tests in Splunk Default setting: http_data_file: wiperf-http top [DHCP_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no DHCP test is run. Note that the DHCP test has 2 modes : passive: only a renewal request is sent (no release of IP) active: a release and renew request is performed. Note that the active setting has shown varying degrees of usefulness in esting. In some scenarios (e.g. when connected via ZeroTier), it has caused connectivity issues, hence the passive setting is a better choice. Obviously, the passive setting does not perform such a rigorous DHCP test and is completed much quicker than the active mode. However, it still provides a useful comparative measure of the reponsivemess of DHCP servers. Default setting: enabled: yes top mode (deprecated) Note: This setting has been removed as it caused probe connectivity issues. The probe now only operates in the passive mode. These notes have bene left in for reference for those who used older versions of code or old configuration file. This setting is silently ignored if supplied. Available options: passive active The active settings performs a full release/request and may be disruptve to connectivity - use with caution. The passive setting is the recommended option for most situations. Default setting: mode: passive top dhcp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DHCP tests in Splunk Default setting: dhcp_data_file: wiperf-dhcp top","title":"Wiperf - config.ini reference guide  (In development - old version)"},{"location":"config.ini/#wiperf-configini-reference-guide-in-development-old-version","text":"","title":"Wiperf - config.ini reference guide  (In development - old version)"},{"location":"config.ini/#background","text":"The config.ini file controls the operation of the Wiperf utility. It has many options available for maximum flexiblity, but some may need some clarification. Many options will be fine using the defaults that are supplied with the installed package. However, some will definitely require configuration as they may require values such as IP addresses and port numbers which will vary in each instance where Wiperf is used. The config.ini file is located in the directory : /home/wlanpi/wiperf The file is organised in a number of sections that relate to different areas of operation. Each section begins with a name enclosed is square brackets, like this: [Speedtest] Within each section are a number of configurable parameters that are in the format: parameter: value You may also see some lines that begin with a semi-colon. These are comments and have no effect on the operation of Wiperf. You may add, remove or change these as you wish. Here is an example comment: ; wlan interface name set this as per the output of an iwconfig command (usually wlan0)","title":"Background"},{"location":"config.ini/#parameter-reference-guide","text":"We'll take a look at each section of the config file and provide some guidance on suitable parameter values: General Section wlan_if mgt_if platform exporter_type splunk_host splunk_port splunk_token influx_host influx_port influx_username influx_password influx_database influx2_host influx2_port influx2_token influx2_bucket influx2_org test_interval test_offset connectivity_lookup location data_format data_dir data_transport debug cfg_url cfg_username cfg_password cfg_token cfg_refresh_interval unit_bouncer Network_Test Section network_data_file Speetest Section enabled server_id http_proxy https_proxy no_proxy speedtest_data_file Ping_Test Section enabled ping_host1 ping_host2 ping_host3 ping_host4 ping_host5 ping_count ping_data_file Iperf3_tcp_test Section enabled server_hostname port duration iperf3_tcp_data_file Iperf3_udp_test Section enabled server_hostname port duration bandwidth iperf3_udp_data_file DNS_test Section enabled dns_target1 dns_target2 dns_target3 dns_target4 dns_target5 dns_data_file HTTP_test Section enabled http_target1 http_target2 http_target3 http_target4 http_target5 http_data_file DHCP_test Section enabled mode dhcp_data_file","title":"Parameter Reference Guide"},{"location":"config.ini/#general-section","text":"Note: any changes to this section on the WLANPi should only be made when it is running in classic mode (not while in Wiperf mode).","title":"[General] Section"},{"location":"config.ini/#wlan_if","text":"This parameter contains the name of the WLAN interface on the Pi. This will almost always be 'wlan0', but is provided in case of new use-cases in the future. You can see the WLAN interface name by running the 'ifconfig' command from the CLI of the Pi Default setting: wlan_if: wlan0 top","title":"wlan_if"},{"location":"config.ini/#mgt_if","text":"When performance tests have been completed, the results data needs to be sent to a reporting server (e.g. Splunk/InfluxDb). This parameter configures the interface over which this management traffic needs to be sent. Getting this parameter correct for your environment is very important to ensure that test results data makes it back to your reporting server. The available options are: wlan0 (the first available WLAN port - usually a USB dongle plugged in to the WLANPi) eth0 (the internal Ethernet port of the WLANPi) zt (Zerotier (the virtual network service) is installed and used to connect to the reporting server) The WANPi is configured to assign a higher cost default route to eth0 by default so that all traffic (tests & test results) will choose the default route provided by wlan0. If eth0 is used as the path to return test results to the reporting server, then a static route is injected in to the WLANPi route table on start-up to ensure correct routing. If this parameter is not correctly set, then results data may not make it back to the reporting server. Default setting: mgt_if: wlan0 top","title":"mgt_if"},{"location":"config.ini/#platform-deprecated","text":"(This setting is now deprecated (and unused) - it has been included for historical reference) Wiperf is supported on both the WLANPi and Raspberry Pi platforms. The available options are: wlanpi rpi Default setting: platform: wlanpi top","title":"platform (Deprecated)"},{"location":"config.ini/#exporter_type","text":"Wiperf supports a number of remote data repositories that can be used as targets to store test result data. The available options are: splunk influxdb influxdb2 Default setting: exporter_type: splunk top","title":"exporter_type"},{"location":"config.ini/#splunk_host","text":"This is the hostname or IP address of the Splunk platform where test result data is sent to. If the hostname of the Splunk server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): splunk_host: top","title":"splunk_host"},{"location":"config.ini/#splunk_port","text":"The network port used to send updates to the Splunk server. By default this is 8088, but this may be changed within the Splunk application if an alternative port is required for your environment Default setting: splunk_port: 8088 top","title":"splunk_port"},{"location":"config.ini/#splunk_token","text":"Splunk will only receive HEC updates from devices that are authorised to send it data. Splunk uses tokens to decide if an update is from a valid device. To view available (or create) tokens within Splunk, view the menu option: \"Settings > Data > Data Inputs > HTTP Event Collector\" Here is example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d Default setting (none): splunk_token: top","title":"splunk_token"},{"location":"config.ini/#influx_host","text":"This is the hostname or IP address of the Influx (v1.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): influx_host: top","title":"influx_host"},{"location":"config.ini/#influx_port","text":"The network port used to send updates to the Influx (v1.x) server. By default this is 8086, but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx_port: 8086 top","title":"influx_port"},{"location":"config.ini/#influx_username","text":"The username that will be used to access the Influx (v1.x) server DB to post results data. This username must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_username: top","title":"influx_username"},{"location":"config.ini/#influx_password","text":"The password that will be used to access the Influx (v1.x) server DB to post results data. This password must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_password: top","title":"influx_password"},{"location":"config.ini/#influx_database","text":"The name of the database on the Influx (v1.x) server DB where wiperf will post results data. This database must have been created on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_database: top","title":"influx_database"},{"location":"config.ini/#influx2_host","text":"This is the hostname or IP address of the Influx (v2.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your InfluxDb2 server in the Zerotier dashboard for your network) Default setting (none): influx2_host: top","title":"influx2_host"},{"location":"config.ini/#influx2_port","text":"The network port used to send updates to the Influx (v2.x) server. By default this is 443 (this assumes the cloud service is used), but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx2_port: 443 top","title":"influx2_port"},{"location":"config.ini/#influx2_token","text":"InfluxDB2 allows the use of authentication tokens when sending results data to the InfluxDB2 server. This provides an easier authentication methods than using a username and password. Once a token has been created on InfluxDB server, it can be used by wiperf to authenticate the results data sent to the InfluxDB2 server Default setting (none): influx2_token: top","title":"influx2_token"},{"location":"config.ini/#influx2_bucket","text":"Data sent to the InfluxDB2 server from wiperf is stored in a \"bucket\" in the data store. This field is used to configure the bucket to which wiperf should send it's data. Default setting (none): influx2_bucket: top","title":"influx2_bucket"},{"location":"config.ini/#influx2_org","text":"The InfluxDB2 server can be partitioned in to a number of organizations, which contain the buckets where data will be stored. Use this field to configure wiperf to send data to the correct organisation on InfluxDB2. Default setting (none): influx2_org: top","title":"influx2_org"},{"location":"config.ini/#test_interval","text":"(WLANPi only) This is the interval (in minutes) at which we would like to run the performance tests. The recommened minimum is 5, which is also the default. (Note: if this setting is too low, scheduled tests may try to run before the previous test sequence has completed, which could cause gaps in your data) Default setting: test_interval: 5 top","title":"test_interval"},{"location":"config.ini/#test_offset","text":"(WLANPi only) By default test run at the interval specified by the test_interval parameter, which is referenced to the to of the hours (e.g. 5 mins interval will run at 5, 10, 15, 20, 25...etc. mins past the hour). If multiple proes are running, it mau be useful to stagger their start times. By setting test_offset to a value of one, this will offset all test start times by 1 minutes (i.e. 6,11,16,21,26...etc. mins past the hour) The default value is zero which means that the default 5,10,15,20... run pattern will be used. Default setting: test_offset: 0 top","title":"test_offset"},{"location":"config.ini/#connectivity_lookup","text":"At the start of each test cycle, a DNS lookup is performed to ensure that DNS is working. By default this is 'google.com' (this was 'bbc.co.uk' on older versions of wiperf). This may be set to any required hostname lookup in instances when the default site may not be available for some reason (e.g. DNS restrictions due to filtering or lack of Internet access) Default setting: connectivity_lookup: google.com top","title":"connectivity_lookup"},{"location":"config.ini/#location","text":"This is a string that can be added to assist with report filtering, if required. Its default value in an empty string. It could be be used in an expression within your reports to filter units based on a location field (for instance) Default setting: location: top","title":"location"},{"location":"config.ini/#data_format","text":"(Not currently operational) Wiperf has the capability to output data in a number of formats. The current options are: csv or json However this field is not currently used, as selecting the 'hec' transport mode (the only supported transport currently) over-rides this field. The value in this filed is currently irrelevant, but it s recommended to leave it at the default setting of json Default setting: data_format: json top","title":"data_format"},{"location":"config.ini/#data_dir","text":"This is the directory on the WLANPi/RPi where test result data is dumped. Do not change this value from the default . This field is provided for future configuration options if required. Default setting: data_dir: /home/wlanpi/wiperf/data top","title":"data_dir"},{"location":"config.ini/#data_transport","text":"The currently supported data transport mode is hec . This is the HTTP Event Collector supported natively within the Splunk server. Other transport modes will be suported in the future, but currently this should be left at the default setting of `hec . (Note: the transport method forwarder is also a valid transport method which provides support for very early versions of this code which used the Splunk Univeral Forwarder. Use of this method is deprecated and will be removed in the near future. Anyone still using the UF should move to using hec ASAP) Default setting: data_transport: hec top","title":"data_transport"},{"location":"config.ini/#debug","text":"To enable enhanced logging in the agent.log file, change this setting to \"on\" Default setting: debug: off top","title":"debug"},{"location":"config.ini/#cfg_url","text":"If using centralized configuration file retrieval, this field specifies the full URL of the config file on the remote repo. (Note that on GitHub this is the URL of the raw file itself) If this field is not set, then centralized configuration retrieval is disabled Default setting (none): cfg_url: top","title":"cfg_url"},{"location":"config.ini/#cfg_username","text":"If username/pasword credentials are used to retrieve the centralized config, this field specifies the usename to be used. Default setting (none): cfg_username: top","title":"cfg_username"},{"location":"config.ini/#cfg_password","text":"If username/pasword credentials are used to retrieve the centralized config, this field specifies the password to be used. Default setting (none): cfg_password: top","title":"cfg_password"},{"location":"config.ini/#cfg_token","text":"If a GitHub authentication token is used to retrieve the centralized config, this field specifies the token to be used. (Note: this is used instead of a username/pwd) Default setting (none): cfg_token: top","title":"cfg_token"},{"location":"config.ini/#cfg_refresh_interval","text":"This field specifies how often (in seconds) the centralized config file should be retrieved . Recommended value: 900 (i.e. 15 mins) Default setting (none): cfg_refresh_interval: top","title":"cfg_refresh_interval"},{"location":"config.ini/#unit_bouncer","text":"If you need to bounce (reboot) the unit for some reason on a regular basis, this field can be used to signal to the WLAN Pi each hour at which it must reboot. The field is a comma separated string that lists the hours at which the unit must reboot (in 24-hour format). The number-format and comma separation are important to get right! Note that the reboot is not exactly on the hour, but will occur at the end of the next test cycle that it is within the hour where a reboot is required. It will only happen once per hour. Example: the following config will reboot at midnight, 04:00, 08:00, 12:00, 16:00: unit_bouncer: 00, 06, 12, 18 This parameter is commented out by default as it is obviously not something you necessarilly want to switch on accidentally. Default setting: ; unit_bouncer: 00, 06, 12, 18 top","title":"unit_bouncer"},{"location":"config.ini/#network_test-section","text":"","title":"[Network_Test] Section"},{"location":"config.ini/#network_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for network tests in remote data repositories (e.g. Splunk, InfluxDB) Default setting: network_data_file: wiperf-network top","title":"network_data_file"},{"location":"config.ini/#speedtest-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[Speedtest] Section"},{"location":"config.ini/#enabled","text":"Options: yes or no. If set to no, entire section is ignored and no Speedtest is run. When enabled, a speedtest to the Ookla speedtest service is run each test cycle. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_id","text":"If you wish to specify a particular Ookla speedtest server that the test needs to be run against, you can enter its ID here. This must be the (numeric) server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If no value is specified, best server is used (default) Default setting: server_id: top","title":"server_id"},{"location":"config.ini/#http_proxy","text":"","title":"http_proxy"},{"location":"config.ini/#https_proxy","text":"","title":"https_proxy"},{"location":"config.ini/#no_proxy","text":"If proxy server access is required to run a speedtest, enter the proxy server details here for https & https e.g. https_proxy: http://10.1.1.1:8080 For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) e.g. no_proxy: \"mail.local, intranet.local\" Default settings: http_proxy: https_proxy: no_proxy: top","title":"no_proxy"},{"location":"config.ini/#speedtest_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for Speedtests in the reporting server (e.g. Splunk/InfluxDB) Default setting: speedtest_data_file: wiperf-speedtest top","title":"speedtest_data_file"},{"location":"config.ini/#ping_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[Ping_Test] Section"},{"location":"config.ini/#enabled_1","text":"Options: yes or no. If set to no, entire section is ignored and no ping tests are run. When enabled, up to 5 entries will be targetted with an ICMP ping and the RRT times recorded Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#ping_host1","text":"IP address or hostname of first ping target. No target details = no test run Default setting: ping_host1: bbc.co.uk top","title":"ping_host1"},{"location":"config.ini/#ping_host2","text":"IP address or hostname of second ping target. No target details = no test run Default setting: ping_host2: cisco.com top","title":"ping_host2"},{"location":"config.ini/#ping_host3","text":"IP address or hostname of third ping target. No target details = no test run Default setting: ping_host3: google.com top","title":"ping_host3"},{"location":"config.ini/#ping_host4","text":"IP address or hostname of fourth ping target. No target details = no test run Default setting: ping_host4: top","title":"ping_host4"},{"location":"config.ini/#ping_host5","text":"IP address or hostname of fifth ping target. No target details = no test run Default setting: ping_host2: top","title":"ping_host5"},{"location":"config.ini/#ping_count","text":"The number of pings to send for each ping target Default setting: ping_count: 10 top","title":"ping_count"},{"location":"config.ini/#ping_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for ping tests in the reporting server (e.g. Splunk.InfuxDB) Default setting: ping_data_file: wiperf-ping top","title":"ping_data_file"},{"location":"config.ini/#iperf3_tcp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[Iperf3_tcp_test] Section"},{"location":"config.ini/#enabled_2","text":"Options: yes or no. If set to no, entire section is ignored and no tcp iperf3 test is run. When enabled, a tcp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs) Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_hostname","text":"The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: 192.168.0.14 top","title":"server_hostname"},{"location":"config.ini/#port","text":"The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top","title":"port"},{"location":"config.ini/#duration","text":"The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 20 top","title":"duration"},{"location":"config.ini/#iperf3_tcp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for tcp iperf3 tests in Splunk Default setting: iperf3_tcp_data_file: wiperf-iperf3-tcp top","title":"iperf3_tcp_data_file"},{"location":"config.ini/#iperf3_udp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[Iperf3_udp_test] Section"},{"location":"config.ini/#enabled_3","text":"Options: yes or no. If set to no, entire section is ignored and no udp iperf3 test is run. When enabled, a udp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs), attempting to achieve a data transfer rate of bandwidth bps. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_hostname_1","text":"The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: 192.168.0.14 top","title":"server_hostname"},{"location":"config.ini/#port_1","text":"The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top","title":"port"},{"location":"config.ini/#duration_1","text":"The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 20 top","title":"duration"},{"location":"config.ini/#bandwidth","text":"The data rate that will be attempted for the UDP iperf3 test in bps Default setting: bandwidth: 20000000 top","title":"bandwidth"},{"location":"config.ini/#iperf3_udp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for udp iperf3 tests in Splunk Default setting: iperf3_udp_data_file: wiperf-iperf3-udp top","title":"iperf3_udp_data_file"},{"location":"config.ini/#dns_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[DNS_test] Section"},{"location":"config.ini/#enabled_4","text":"Options: yes or no. If set to no, entire section is ignored and no DNS tests are run. When enabled, DNS tests are run for each of the dns_target paramters defined in this section. Any targets that have no value entered will be ignored. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#dns_target1","text":"Hostname of first DNS target. No target details = no test run Default setting: dns_target1: bbc.co.uk top","title":"dns_target1"},{"location":"config.ini/#dns_target2","text":"Hostname of second DNS target. No target details = no test run Default setting: dns_target2: cisco.com top","title":"dns_target2"},{"location":"config.ini/#dns_target3","text":"Hostname of third DNS target. No target details = no test run Default setting: dns_target3: google.com top","title":"dns_target3"},{"location":"config.ini/#dns_target4","text":"Hostname of fourth DNS target. No target details = no test run Default setting: dns_target4: top","title":"dns_target4"},{"location":"config.ini/#dns_target5","text":"Hostname of fifth DNS target. No target details = no test run Default setting: dns_target5: top","title":"dns_target5"},{"location":"config.ini/#dns_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DNS tests in Splunk Default setting: dns_data_file: wiperf-dns top","title":"dns_data_file"},{"location":"config.ini/#http_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[HTTP_test] Section"},{"location":"config.ini/#enabled_5","text":"Options: yes or no. If set to no, entire section is ignored and no HTTP tests are run. When enabled, HTTP tests are run for each of the http_target paramters defined in this section. Any targets that have no value entered will be ignored. Targets must include the full url of each site to be queried (including http:// or https:// element). Valid site address examples: http://bbc.co.uk https://ebay.com A http get will be performed for each target and the result code returned. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#http_target1","text":"Hostname of first HTTP target. No target details = no test run Default setting: http_target1: https://ebay.co.uk top","title":"http_target1"},{"location":"config.ini/#http_target2","text":"Hostname of second HTTP target. No target details = no test run Default setting: http_target2: http://twitter.com top","title":"http_target2"},{"location":"config.ini/#http_target3","text":"Hostname of third HTTP target. No target details = no test run Default setting: http_target3: https://facebook.com top","title":"http_target3"},{"location":"config.ini/#http_target4","text":"Hostname of fourth HTTP target. No target details = no test run Default setting: http_target4: https://instagram.com top","title":"http_target4"},{"location":"config.ini/#http_target5","text":"Hostname of fifth HTTP target. No target details = no test run Default setting: https://amazon.com http_target5: top","title":"http_target5"},{"location":"config.ini/#http_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for HTTP tests in Splunk Default setting: http_data_file: wiperf-http top","title":"http_data_file"},{"location":"config.ini/#dhcp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[DHCP_test] Section"},{"location":"config.ini/#enabled_6","text":"Options: yes or no. If set to no, entire section is ignored and no DHCP test is run. Note that the DHCP test has 2 modes : passive: only a renewal request is sent (no release of IP) active: a release and renew request is performed. Note that the active setting has shown varying degrees of usefulness in esting. In some scenarios (e.g. when connected via ZeroTier), it has caused connectivity issues, hence the passive setting is a better choice. Obviously, the passive setting does not perform such a rigorous DHCP test and is completed much quicker than the active mode. However, it still provides a useful comparative measure of the reponsivemess of DHCP servers. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#mode-deprecated","text":"Note: This setting has been removed as it caused probe connectivity issues. The probe now only operates in the passive mode. These notes have bene left in for reference for those who used older versions of code or old configuration file. This setting is silently ignored if supplied. Available options: passive active The active settings performs a full release/request and may be disruptve to connectivity - use with caution. The passive setting is the recommended option for most situations. Default setting: mode: passive top","title":"mode (deprecated)"},{"location":"config.ini/#dhcp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DHCP tests in Splunk Default setting: dhcp_data_file: wiperf-dhcp top","title":"dhcp_data_file"},{"location":"data_points/","text":"Wiperf - Data Points Reference Guide (In development - old version) Background The wiperf probe collects a variety of data points about various aspects of network connectivity and performance. It then makes those data points available to a number of databases via their standard API (e.g. Splunk, InfluxDB etc.). The data collected in all instances is the same, but the format of the data presented to each type of database varies depending on the their API and formatting rules and syntax. This document details the data points collected by the probe. These field names and the data values should be the same no matter which database is used. The probe may collect data for the following network tests, depending upon its configuration: Wireless network connectivity details Speedtest testing results data ICMP ping tests to various destinations DNS lookup tests to various destinations ] HTTP (web) tests to various destinations iperf3 TCP test to a nominated iperf3 server iperf3 UDP test to a nominated iperf3 server DHCP renewal test to test DHCP performance on network to which the WLAN Pi is connected The tests are run each time the wiperf process is triggered (usually every 5 minutes from a local cron job). The tests that are run, together with test configuration parameters are configured in the config.ini file. Here are the data points that may be collected, displayed by test type: Data Points Details Wireless Network Connectivity (Data source: wiperf-network) time : Unix timestamp of time test was performed ssid : The network name of the wireless network to which the wiperf probe is currently connected bssid : The basic service set identifier (i.e. MAC address) of the radio to which the wiperf probe is currently connected freq_ghz : The centre frequency of the channel on which the probe is operating (note this may be different to the primary channel centre freq if a bonded channel is in use) center_freq_ghz : The centre frequency of the primary channel on which the probe is operating channel : The channel number on which the probe is operating channel_width: The channel width (e.g. 20MHz, 40MHz, 80MHz) of the channel on which the probe is operating tx_rate_mbps : The PHY rate at which data is being sent from the probe to the AP (note this is not a throughput rate, just a physical connection rate) rx_rate_mbps : The PHY rate at which data is being sent from the AP to the probe (note this is not a throughput rate, just a physical connection rate) tx_mcs : For HT & VHT connections, this is the the MCS value used by the probe to the AP rx_mcs : For HT & VHT connections, this is the the MCS value used by the AP to the probe signal_level_dbm : The power level of the AP radio signal as observed by the probe (in dBm) tx_retries : The number of transmitted frames that have had to be sent gain (retried) ip_address : The IP address assigned to the probe WLAN NIC Speedtest Results (Data source: wiperf-speedtest) time : Unix timestamp of time test was performed ping_time : The RTT of a ping test to the speedtest server download_rate_mbps : The throughput rate achieved when receiving data from the speedtest server in megabits per second upload_rate_mbps : The throughput rate achieved when sending data to the speedtest server in megabits per second server_name : The name of the speedtest server used for this test Ping Results (Data source: wiperf-ping) time : Unix timestamp of time test was performed ping_index : wiperf runs up to 5 instances of ping test via its configuration file. This index uniquely identifies each instance. ping_host : The IP address or hostname of the target host/site being pinged pkts_tx : The number of ping request packets sent during the ping test pkts_rx : The number of ping response packets received back during the ping test percent_loss : The percentage (%) of packets lost during the test (i.e. how many responses were received compared to requests sent) test_time_ms : How long the ping test took in total rtt_min_ms : The minimum round trip time of all ping tests to this test instance in milliseconds rtt_avg_ms : The average round trip time of all ping tests to this test instance in milliseconds rtt_max_ms : The maximum round trip time of all ping tests to this test instance in milliseconds rtt_mdev_ms : Standard deviation of all ping tests to this test instance (...no, I don't know either...but you'll look cool at dinner parties if you mention it.) DNS Results (Data source: wiperf-ping) time : Unix timestamp of time test was performed dns_index : wiperf runs up to 5 instances of DNS test via its configuration file. This index uniquely identifies each instance. dns_target : The domain name of the target host/site which is the subject of the DNS lookup test lookup_time_ms : The time taken to perform the DNS lookup in milliseconds HTTP Results (Data source: wiperf-http) time : Unix timestamp of time test was performed http_index : wiperf runs up to 5 instances of HTTP test via its configuration file. This index uniquely identifies each instance. http_target : The domain name (or IP address) of the target site which is the subject of the HTTP test lookup_time_ms : The time taken to retrieve the html page from the target site in milliseconds http_status_code : The HTTP status code returned from the target site in this test instance (200 is good, other values have varying meanings: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes ) iperf3 TCP Results (Data source: wiperf-iperf3-tcp) time : Unix timestamp of time test was performed sent_mbps : The transmit throughput achieved (in megabits per seconds) during the TCP iperf test received_mbps : The receive throughput achieved (in megabits per seconds) during the TCP iperf test sent_bytes : The number of bytes (i.e. data volume) sent from the probe to the iperf server during the test received_bytes : The number of bytes (i.e. data volume) received by the probe from the iperf server during the test retransmits : The number of times frames had to be re=transmitted during the test iperf3 UDP Results (Data source: wiperf-iperf3-udp) time : Unix timestamp of time test was performed bytes : The number of bytes transferred from the probe to the iperf server during the test mbps : The throughput achieved (in megabits per second) during the iperf test when sending data to the iperf server jitter_ms : The level of jitter measured (in milliseconds) during the test packets : The number of packets sent from the probe to the iperf server during the test lost_packets : The number of transmitted packets lost during the test lost_percent : The percentage of transmitted packets lost during the test DHCP Test Results (Data source: wiperf-dhcp) time : Unix timestamp of time test was performed renewal_time_ms : The time taken for the probe to renew it's IP address in milliseconds","title":"Wiperf - Data Points Reference Guide  (In development - old version)"},{"location":"data_points/#wiperf-data-points-reference-guide-in-development-old-version","text":"","title":"Wiperf - Data Points Reference Guide  (In development - old version)"},{"location":"data_points/#background","text":"The wiperf probe collects a variety of data points about various aspects of network connectivity and performance. It then makes those data points available to a number of databases via their standard API (e.g. Splunk, InfluxDB etc.). The data collected in all instances is the same, but the format of the data presented to each type of database varies depending on the their API and formatting rules and syntax. This document details the data points collected by the probe. These field names and the data values should be the same no matter which database is used. The probe may collect data for the following network tests, depending upon its configuration: Wireless network connectivity details Speedtest testing results data ICMP ping tests to various destinations DNS lookup tests to various destinations ] HTTP (web) tests to various destinations iperf3 TCP test to a nominated iperf3 server iperf3 UDP test to a nominated iperf3 server DHCP renewal test to test DHCP performance on network to which the WLAN Pi is connected The tests are run each time the wiperf process is triggered (usually every 5 minutes from a local cron job). The tests that are run, together with test configuration parameters are configured in the config.ini file. Here are the data points that may be collected, displayed by test type:","title":"Background"},{"location":"data_points/#data-points-details","text":"","title":"Data Points Details"},{"location":"data_points/#wireless-network-connectivity","text":"(Data source: wiperf-network) time : Unix timestamp of time test was performed ssid : The network name of the wireless network to which the wiperf probe is currently connected bssid : The basic service set identifier (i.e. MAC address) of the radio to which the wiperf probe is currently connected freq_ghz : The centre frequency of the channel on which the probe is operating (note this may be different to the primary channel centre freq if a bonded channel is in use) center_freq_ghz : The centre frequency of the primary channel on which the probe is operating channel : The channel number on which the probe is operating channel_width: The channel width (e.g. 20MHz, 40MHz, 80MHz) of the channel on which the probe is operating tx_rate_mbps : The PHY rate at which data is being sent from the probe to the AP (note this is not a throughput rate, just a physical connection rate) rx_rate_mbps : The PHY rate at which data is being sent from the AP to the probe (note this is not a throughput rate, just a physical connection rate) tx_mcs : For HT & VHT connections, this is the the MCS value used by the probe to the AP rx_mcs : For HT & VHT connections, this is the the MCS value used by the AP to the probe signal_level_dbm : The power level of the AP radio signal as observed by the probe (in dBm) tx_retries : The number of transmitted frames that have had to be sent gain (retried) ip_address : The IP address assigned to the probe WLAN NIC","title":"Wireless Network Connectivity"},{"location":"data_points/#speedtest-results","text":"(Data source: wiperf-speedtest) time : Unix timestamp of time test was performed ping_time : The RTT of a ping test to the speedtest server download_rate_mbps : The throughput rate achieved when receiving data from the speedtest server in megabits per second upload_rate_mbps : The throughput rate achieved when sending data to the speedtest server in megabits per second server_name : The name of the speedtest server used for this test","title":"Speedtest Results"},{"location":"data_points/#ping-results","text":"(Data source: wiperf-ping) time : Unix timestamp of time test was performed ping_index : wiperf runs up to 5 instances of ping test via its configuration file. This index uniquely identifies each instance. ping_host : The IP address or hostname of the target host/site being pinged pkts_tx : The number of ping request packets sent during the ping test pkts_rx : The number of ping response packets received back during the ping test percent_loss : The percentage (%) of packets lost during the test (i.e. how many responses were received compared to requests sent) test_time_ms : How long the ping test took in total rtt_min_ms : The minimum round trip time of all ping tests to this test instance in milliseconds rtt_avg_ms : The average round trip time of all ping tests to this test instance in milliseconds rtt_max_ms : The maximum round trip time of all ping tests to this test instance in milliseconds rtt_mdev_ms : Standard deviation of all ping tests to this test instance (...no, I don't know either...but you'll look cool at dinner parties if you mention it.)","title":"Ping Results"},{"location":"data_points/#dns-results","text":"(Data source: wiperf-ping) time : Unix timestamp of time test was performed dns_index : wiperf runs up to 5 instances of DNS test via its configuration file. This index uniquely identifies each instance. dns_target : The domain name of the target host/site which is the subject of the DNS lookup test lookup_time_ms : The time taken to perform the DNS lookup in milliseconds","title":"DNS Results"},{"location":"data_points/#http-results","text":"(Data source: wiperf-http) time : Unix timestamp of time test was performed http_index : wiperf runs up to 5 instances of HTTP test via its configuration file. This index uniquely identifies each instance. http_target : The domain name (or IP address) of the target site which is the subject of the HTTP test lookup_time_ms : The time taken to retrieve the html page from the target site in milliseconds http_status_code : The HTTP status code returned from the target site in this test instance (200 is good, other values have varying meanings: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes )","title":"HTTP Results"},{"location":"data_points/#iperf3-tcp-results","text":"(Data source: wiperf-iperf3-tcp) time : Unix timestamp of time test was performed sent_mbps : The transmit throughput achieved (in megabits per seconds) during the TCP iperf test received_mbps : The receive throughput achieved (in megabits per seconds) during the TCP iperf test sent_bytes : The number of bytes (i.e. data volume) sent from the probe to the iperf server during the test received_bytes : The number of bytes (i.e. data volume) received by the probe from the iperf server during the test retransmits : The number of times frames had to be re=transmitted during the test","title":"iperf3 TCP Results"},{"location":"data_points/#iperf3-udp-results","text":"(Data source: wiperf-iperf3-udp) time : Unix timestamp of time test was performed bytes : The number of bytes transferred from the probe to the iperf server during the test mbps : The throughput achieved (in megabits per second) during the iperf test when sending data to the iperf server jitter_ms : The level of jitter measured (in milliseconds) during the test packets : The number of packets sent from the probe to the iperf server during the test lost_packets : The number of transmitted packets lost during the test lost_percent : The percentage of transmitted packets lost during the test","title":"iperf3 UDP Results"},{"location":"data_points/#dhcp-test-results","text":"(Data source: wiperf-dhcp) time : Unix timestamp of time test was performed renewal_time_ms : The time taken for the probe to renew it's IP address in milliseconds","title":"DHCP Test Results"},{"location":"faq/","text":"FAQ (In development - old version) Proxy Connectivity check (internal) I see bounce command error messages in the agent.log file, what is going on? The following error messages may be seen in the agent.log file of wiperf if using the WLAN Pi v1.9.1 (or earlier) image: ERROR - if bounce command appears to have failed. Error: sudo: no tty present and no askpass program specified This occurs when connectivity issues are experienced and wiperf attempt to bounce the wireless interface to recover the wireless connection. To fix this issue, add the following entries to the /etc/sudoers.d/wlanpidump file: /sbin/ifdown /sbin/ifup The modified file content should be as follows: wlanpi ALL = (root) NOPASSWD: /sbin/iwconfig, /usr/sbin/iw, /sbin/dhclient, /sbin/ifconfig, /sbin/reboot, /bin/kill, /bin/date, /sbin/ifdown, /sbin/ifup This will ensure that the wireless interface may be correctly bounced by wiperf if required. Where do I get the dashboard reports for Splunk? Use SFTP/SCP and pull the xml files in /home/wlanpi/wiperf/dashboards from your WLAN Pi. See the [Splunk build guide][splunk_build] for details of how to add them to Splunk. The dashboard reports show no MCS data and RX PHY rate data - why not? Various WLAN NICs that use both Realtek and Mediatek WLAN chips are now supported by the WLAN Pi. Unfortunately, the Realtek chipsets (e.g. our old favourite the CF-912) do not report as much data as the Mediatek chips, so this data is missing. As I am not aware of any way of making the dashboard reports show data conditional on the chipset used, some graphs are shown but not fully populate - sorry. How do I get more reports or customize the supplied Splunk reports? Sorry, you'll have to roll up your sleeves and have a look at this for yourself: https://docs.splunk.com/Documentation/Splunk/8.0.1/SearchTutorial/Createnewdashboard Can I make a feature suggestion? Yes, get along to the GitHub site and post your suggestion in the 'Issues' section: https://github.com/wifinigel/wiperf/issues. It will be added to my \"todo\" list. Can I run tests over the Ethernet interface of the WLAN Pi? No, not at present. It was originally designed as a WLAN test device, so I need to do a bit of code re-writing to get tests going over Ethernet. Stay tuned. I'm running the v1.9 WLAN Pi image and the iperf tests don't work....what's going on? There was an issue with the code distributed with image v1.9. Try the following: ssh to the WLAN Pi Run the following commands (assuming the WLAN Pi has Internet connectivity): cd ~/wiperf git pull https://github.com/wifinigel/wiperf.git (It's best to do this is classic mode and redo you Wiperf configuration again after this operation - note that the config.default.ini file has new options you will probably like to use. Don't forget to check /home/wlanpi/wiperf/config/etc/wpa_supplicant/wpa_supplicant.conf too.)","title":"FAQ"},{"location":"faq/#faq-in-development-old-version","text":"","title":"FAQ  (In development - old version)"},{"location":"faq/#proxy","text":"","title":"Proxy"},{"location":"faq/#connectivity-check-internal","text":"","title":"Connectivity check (internal)"},{"location":"faq/#i-see-bounce-command-error-messages-in-the-agentlog-file-what-is-going-on","text":"The following error messages may be seen in the agent.log file of wiperf if using the WLAN Pi v1.9.1 (or earlier) image: ERROR - if bounce command appears to have failed. Error: sudo: no tty present and no askpass program specified This occurs when connectivity issues are experienced and wiperf attempt to bounce the wireless interface to recover the wireless connection. To fix this issue, add the following entries to the /etc/sudoers.d/wlanpidump file: /sbin/ifdown /sbin/ifup The modified file content should be as follows: wlanpi ALL = (root) NOPASSWD: /sbin/iwconfig, /usr/sbin/iw, /sbin/dhclient, /sbin/ifconfig, /sbin/reboot, /bin/kill, /bin/date, /sbin/ifdown, /sbin/ifup This will ensure that the wireless interface may be correctly bounced by wiperf if required.","title":"I see bounce command error messages in the agent.log file, what is going on?"},{"location":"faq/#where-do-i-get-the-dashboard-reports-for-splunk","text":"Use SFTP/SCP and pull the xml files in /home/wlanpi/wiperf/dashboards from your WLAN Pi. See the [Splunk build guide][splunk_build] for details of how to add them to Splunk.","title":"Where do I get the dashboard reports for Splunk?"},{"location":"faq/#the-dashboard-reports-show-no-mcs-data-and-rx-phy-rate-data-why-not","text":"Various WLAN NICs that use both Realtek and Mediatek WLAN chips are now supported by the WLAN Pi. Unfortunately, the Realtek chipsets (e.g. our old favourite the CF-912) do not report as much data as the Mediatek chips, so this data is missing. As I am not aware of any way of making the dashboard reports show data conditional on the chipset used, some graphs are shown but not fully populate - sorry.","title":"The dashboard reports show no MCS data and RX PHY rate data - why not?"},{"location":"faq/#how-do-i-get-more-reports-or-customize-the-supplied-splunk-reports","text":"Sorry, you'll have to roll up your sleeves and have a look at this for yourself: https://docs.splunk.com/Documentation/Splunk/8.0.1/SearchTutorial/Createnewdashboard","title":"How do I get more reports or customize the supplied Splunk reports?"},{"location":"faq/#can-i-make-a-feature-suggestion","text":"Yes, get along to the GitHub site and post your suggestion in the 'Issues' section: https://github.com/wifinigel/wiperf/issues. It will be added to my \"todo\" list.","title":"Can I make a feature suggestion?"},{"location":"faq/#can-i-run-tests-over-the-ethernet-interface-of-the-wlan-pi","text":"No, not at present. It was originally designed as a WLAN test device, so I need to do a bit of code re-writing to get tests going over Ethernet. Stay tuned.","title":"Can I run tests over the Ethernet interface of the WLAN Pi?"},{"location":"faq/#im-running-the-v19-wlan-pi-image-and-the-iperf-tests-dont-workwhats-going-on","text":"There was an issue with the code distributed with image v1.9. Try the following: ssh to the WLAN Pi Run the following commands (assuming the WLAN Pi has Internet connectivity): cd ~/wiperf git pull https://github.com/wifinigel/wiperf.git (It's best to do this is classic mode and redo you Wiperf configuration again after this operation - note that the config.default.ini file has new options you will probably like to use. Don't forget to check /home/wlanpi/wiperf/config/etc/wpa_supplicant/wpa_supplicant.conf too.)","title":"I'm running the v1.9 WLAN Pi image and the iperf tests don't work....what's going on?"},{"location":"grafana_configure/","text":"Grafana Configuration Once the Granafa installation is complete, there are two main tasks remaining: Integration of Grafana with InfluxDB Addition of the wiperf dashboards Integration of Grafana with InfluxDB Integration with InfluxDB is required to allow Grafana to send data queries to the InfluxDB database and turn the data in to graphical reports. To do this, Grafana needs to know: the data source is an InfluxDB database where it is (IP address & port) the name of the database within Influx DB (as we previously configured) the access credentials to be used to pull the data out of InfluxDB The screen-shots below show the required steps: Configuration > Data Sources > Added Data Source: Select InfluxDB: Enter the name to be referenced for the connection, the URL, database name, username & password (all highlighted below) - note the InfluxDB values use those configure previously when we set up InfluxDB (these settings must match those used in the InfluxDB setup): Once completed, if you hit 'Save and Test', the database connection test should indicate success if all information has been correctly entered. Adding Wiperf Dashboards Dashboards can be obtained from the '/usr/share/wiperf/dashboards' folder of the probe. These may be downloaded to your local laptop using a utlity such as SCP or SFTP. Alternatively, the dashboard files may be accessed in the main wiperf repo at : folder Once the dashboard files have been downloaded, they may be imported to Grafana using the following steps: Use the menu option Dashboards > Manage > Import: Hit the 'Upload Json' button and select the dashboard file on your local laptop that you'd like to upload The name of the imported report will be shown as indicated. If you'd like to create or select a folder, this can be done in the 'Folder' field. The database connection previously created as the data source must also be selected to ensure the InfluxDB database can be queried: Once 'Import' is hit, the dashboard will be available in the main Grafana GUI.","title":"Grafana Configuration"},{"location":"grafana_configure/#grafana-configuration","text":"Once the Granafa installation is complete, there are two main tasks remaining: Integration of Grafana with InfluxDB Addition of the wiperf dashboards","title":"Grafana Configuration"},{"location":"grafana_configure/#integration-of-grafana-with-influxdb","text":"Integration with InfluxDB is required to allow Grafana to send data queries to the InfluxDB database and turn the data in to graphical reports. To do this, Grafana needs to know: the data source is an InfluxDB database where it is (IP address & port) the name of the database within Influx DB (as we previously configured) the access credentials to be used to pull the data out of InfluxDB The screen-shots below show the required steps: Configuration > Data Sources > Added Data Source: Select InfluxDB: Enter the name to be referenced for the connection, the URL, database name, username & password (all highlighted below) - note the InfluxDB values use those configure previously when we set up InfluxDB (these settings must match those used in the InfluxDB setup): Once completed, if you hit 'Save and Test', the database connection test should indicate success if all information has been correctly entered.","title":"Integration of Grafana with InfluxDB"},{"location":"grafana_configure/#adding-wiperf-dashboards","text":"Dashboards can be obtained from the '/usr/share/wiperf/dashboards' folder of the probe. These may be downloaded to your local laptop using a utlity such as SCP or SFTP. Alternatively, the dashboard files may be accessed in the main wiperf repo at : folder Once the dashboard files have been downloaded, they may be imported to Grafana using the following steps: Use the menu option Dashboards > Manage > Import: Hit the 'Upload Json' button and select the dashboard file on your local laptop that you'd like to upload The name of the imported report will be shown as indicated. If you'd like to create or select a folder, this can be done in the 'Folder' field. The database connection previously created as the data source must also be selected to ensure the InfluxDB database can be queried: Once 'Import' is hit, the dashboard will be available in the main Grafana GUI.","title":"Adding Wiperf Dashboards"},{"location":"grafana_install/","text":"Grafana Installation Obtaining and installing the Grafana software is very straightforward. The following notes provide a high level overview of the steps required. Note that these instructions are for Grafana version 6.7 (other versions may work, but have not been tested): Visit the Grafana 6.7 installation guide at https://grafana.com/docs/grafana/v6.7/ . This provides acces to a wide variety of information about Grafanam including supported OS'es and platform concepts To download the code and to see the commands required for installation on the server CLI, visit the following download page: https://grafana.com/grafana/download Select the required version, Enterprise Edition and your OS (most likely a Linux variant) Make a copy of the CLI commands provided to download and install the software for your OS SSH to the server that will be used to host Grafana Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start grafana Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status grafana (ensure the process is \"active (running)\" ) As a final check, ensure that the Grafana web GUI is available using the URL: http:// :3000/","title":"Grafana Installation"},{"location":"grafana_install/#grafana-installation","text":"Obtaining and installing the Grafana software is very straightforward. The following notes provide a high level overview of the steps required. Note that these instructions are for Grafana version 6.7 (other versions may work, but have not been tested): Visit the Grafana 6.7 installation guide at https://grafana.com/docs/grafana/v6.7/ . This provides acces to a wide variety of information about Grafanam including supported OS'es and platform concepts To download the code and to see the commands required for installation on the server CLI, visit the following download page: https://grafana.com/grafana/download Select the required version, Enterprise Edition and your OS (most likely a Linux variant) Make a copy of the CLI commands provided to download and install the software for your OS SSH to the server that will be used to host Grafana Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start grafana Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status grafana (ensure the process is \"active (running)\" ) As a final check, ensure that the Grafana web GUI is available using the URL: http:// :3000/","title":"Grafana Installation"},{"location":"grafana_platform/","text":"Grafana Platform Grafana is an open-source visualization tool that allows us to create reports around the data sent from wiperf probes to InfluxDB. It integrates with a number of data sources to query raw data and provides a wide variety of graphical report options. This guide does not cover all installation details of the software package, these may be obtained from the official Grafana web site: https://grafana.com/docs/grafana/latest/ . Installation instructions are available for all major operating systems. Note that although Windows is supported, if you intend to install Grafana on the same platform as InfuxDB, Windows is not an option as InfluxDB v1.8 does not support Windows. To install Grafana and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"Grafana Platform"},{"location":"grafana_platform/#grafana-platform","text":"Grafana is an open-source visualization tool that allows us to create reports around the data sent from wiperf probes to InfluxDB. It integrates with a number of data sources to query raw data and provides a wide variety of graphical report options. This guide does not cover all installation details of the software package, these may be obtained from the official Grafana web site: https://grafana.com/docs/grafana/latest/ . Installation instructions are available for all major operating systems. Note that although Windows is supported, if you intend to install Grafana on the same platform as InfuxDB, Windows is not an option as InfluxDB v1.8 does not support Windows. To install Grafana and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"Grafana Platform"},{"location":"influx_configure/","text":"Influx Configuration Now that we have the InfluxDB software installed, the next step is to create a database into which data from our wiperf probes will be stored. To create the database, we need to execute a series of commands on the CLI of the Influx DB server. Follow the following steps to create the required database: Check the InfluxDB service is running before starting sudo systemctl status influxdb (ensure the process is \"active (running)\" ) Enter the InfluxDB shell using the following command: sudo influx (indicated by the new \">\" prompt) Create an admin user to administer Influx: CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES Exit the InfluxDB shell with the command exit Edit the InfluxDB configuration file (/etc/influxdb/influxdb.conf): sudo nano /etc/influxdb/influxdb.conf uncomment the line # auth-enabled = false in the [http] section and change to auth-enabled = true to enable authentication of access to the database restart the InfluxDB process for the change to take effect: sudo systemctl restart influxdb Enter the InfluxDB shell using the following command: sudo influx -username admin -password letmein (now using authentication) Create a new database with the following commands: CREATE DATABASE wiperf Check the new database exists using: SHOW DATABASES (the database \"wiperf\" should be shown in the list) Create and assign a user who can write to the wiperf database (i.e. a probe) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" Create and assign a user who can read fro the wiperf database (i.e. the Grafana program) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" Exit the InfluxDB shell with the command exit At this point, the InfluxDB service is ready to receive data from a probe. If you have any probes ready to go, make sure they used the \"wiperf_probe\" user credentials in their configuration file so that they can add their data to the database. If you believe you have a probe that has successfully sent data, you can check the database contents using the following commands in the InfluxDB shell: USE wiperf SHOW SERIES SELECT * FROM \"wiperf-speedtest\" SHOW FIELD KEYS ON \"wiperf\" FROM \"wiperf-speedtest\" To find out more details, please checkout the official getting started guide: https://docs.influxdata.com/influxdb/v1.8/introduction/get-started/ For more information about adding users, check out: https://docs.influxdata.com/influxdb/v1.8/administration/authentication_and_authorization/","title":"Influx Configuration"},{"location":"influx_configure/#influx-configuration","text":"Now that we have the InfluxDB software installed, the next step is to create a database into which data from our wiperf probes will be stored. To create the database, we need to execute a series of commands on the CLI of the Influx DB server. Follow the following steps to create the required database: Check the InfluxDB service is running before starting sudo systemctl status influxdb (ensure the process is \"active (running)\" ) Enter the InfluxDB shell using the following command: sudo influx (indicated by the new \">\" prompt) Create an admin user to administer Influx: CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES Exit the InfluxDB shell with the command exit Edit the InfluxDB configuration file (/etc/influxdb/influxdb.conf): sudo nano /etc/influxdb/influxdb.conf uncomment the line # auth-enabled = false in the [http] section and change to auth-enabled = true to enable authentication of access to the database restart the InfluxDB process for the change to take effect: sudo systemctl restart influxdb Enter the InfluxDB shell using the following command: sudo influx -username admin -password letmein (now using authentication) Create a new database with the following commands: CREATE DATABASE wiperf Check the new database exists using: SHOW DATABASES (the database \"wiperf\" should be shown in the list) Create and assign a user who can write to the wiperf database (i.e. a probe) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" Create and assign a user who can read fro the wiperf database (i.e. the Grafana program) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" Exit the InfluxDB shell with the command exit At this point, the InfluxDB service is ready to receive data from a probe. If you have any probes ready to go, make sure they used the \"wiperf_probe\" user credentials in their configuration file so that they can add their data to the database. If you believe you have a probe that has successfully sent data, you can check the database contents using the following commands in the InfluxDB shell: USE wiperf SHOW SERIES SELECT * FROM \"wiperf-speedtest\" SHOW FIELD KEYS ON \"wiperf\" FROM \"wiperf-speedtest\" To find out more details, please checkout the official getting started guide: https://docs.influxdata.com/influxdb/v1.8/introduction/get-started/ For more information about adding users, check out: https://docs.influxdata.com/influxdb/v1.8/administration/authentication_and_authorization/","title":"Influx Configuration"},{"location":"influx_install/","text":"Influx Installation Obtaining and installing the InfluxDB software is very straightforward. The following steps provide a high level over view of the steps required: Visit the InfluxDB v1.8 installation guide at https://docs.influxdata.com/influxdb/v1.8/introduction/install/ Scroll down to the \"Installing InfluxDB OSS\" section Select the OS of the platform that you will be using to host your instance of InfluxDB Copy the commands provided for your server OS to add the required software repository SSH to the server that will be used to host InfluxDB Paste in the commands copied from the installation page on to the CLI of your server. These will ensure your server can find the required repository to pull the InfluxDB software To get the required commands to download & install the software, visit the following web page and select the v1.8 download option: https://portal.influxdata.com/downloads/ Copy the install commands provided for your OS Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start influxdb Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status influxdb (ensure the process is \"active (running)\" ) The next step is to create a database to drop our incoming data (from wiperf probes) into.","title":"Influx Installation"},{"location":"influx_install/#influx-installation","text":"Obtaining and installing the InfluxDB software is very straightforward. The following steps provide a high level over view of the steps required: Visit the InfluxDB v1.8 installation guide at https://docs.influxdata.com/influxdb/v1.8/introduction/install/ Scroll down to the \"Installing InfluxDB OSS\" section Select the OS of the platform that you will be using to host your instance of InfluxDB Copy the commands provided for your server OS to add the required software repository SSH to the server that will be used to host InfluxDB Paste in the commands copied from the installation page on to the CLI of your server. These will ensure your server can find the required repository to pull the InfluxDB software To get the required commands to download & install the software, visit the following web page and select the v1.8 download option: https://portal.influxdata.com/downloads/ Copy the install commands provided for your OS Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start influxdb Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status influxdb (ensure the process is \"active (running)\" ) The next step is to create a database to drop our incoming data (from wiperf probes) into.","title":"Influx Installation"},{"location":"influx_platform/","text":"InfluxDB Platform InfluxDB is a time-series database that we use to store the network performance data that is collected by wiperf probes. It has many other uses and is used by many organizations as a backing store for use cases involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. InfluxDB does not show our network performance report data, but is used as a data repository source for Grafana in our use-case. Details about Grafana are provide later in this documentation. Note that for our use-case, we are using InfluxDB v1.8 (not v2.0). Influx can be installed on a wide variety of Linux-based platforms that can be viewed at : https://docs.influxdata.com/influxdb/v1.8/introduction/install/ . These include Ubuntu, Debian and macOS (no Windows) This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. To install InfluxDB and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software. Connectivity Planning One area to consider is connectivity between the wiperf probe and the InfluxDB instance. The wiperf probe needs to be able to access the InfluxDB server to send its data. If the wiperf probe probe is being deployed on a network, how is the performance data generated going to get back to the InfluxDB server? If the probe is being deployed on a customer network to perform temporary monitoring, it will obviously join the wireless network under test. But how is the wiperf probe going to send its data to the InfluxDB server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, hence (potentially) bridging wired and wireless networks. Therefore, in many instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier. Three topology deployment options are supported: - Results data over wireless - Results data over Ethernet - Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the InfluxDB server. The 3 connectivity options are discussed below. Results Data Over Wireless In this topology the wiperf probe is configured to join an SSID that has the InfluxDB server accessible via its WLAN interface. Typically, the InfluxDB server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the InfluxDB server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of InfluxDB server> Results data over Ethernet If the InfluxDB server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of InfluxDB server> Results data over Zerotier/wireless A very simple way of getting the wiperf probe talking with your InfluxDB server is to use the Zerotier service to create a virtual network. In summary, both the InfluxDB server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same VLAN in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the InfluxDB server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees, It\u2019s very easy to use and get going, plus your InfluxDB server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the InfluxDB server and wiperf probe when the client is installed. Seriously, give it a go...it's quicker to try it than me explaining it here: https://www.zerotier.com/ config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of InfluxDB server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"InfluxDB Platform"},{"location":"influx_platform/#influxdb-platform","text":"InfluxDB is a time-series database that we use to store the network performance data that is collected by wiperf probes. It has many other uses and is used by many organizations as a backing store for use cases involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. InfluxDB does not show our network performance report data, but is used as a data repository source for Grafana in our use-case. Details about Grafana are provide later in this documentation. Note that for our use-case, we are using InfluxDB v1.8 (not v2.0). Influx can be installed on a wide variety of Linux-based platforms that can be viewed at : https://docs.influxdata.com/influxdb/v1.8/introduction/install/ . These include Ubuntu, Debian and macOS (no Windows) This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. To install InfluxDB and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"InfluxDB Platform"},{"location":"influx_platform/#connectivity-planning","text":"One area to consider is connectivity between the wiperf probe and the InfluxDB instance. The wiperf probe needs to be able to access the InfluxDB server to send its data. If the wiperf probe probe is being deployed on a network, how is the performance data generated going to get back to the InfluxDB server? If the probe is being deployed on a customer network to perform temporary monitoring, it will obviously join the wireless network under test. But how is the wiperf probe going to send its data to the InfluxDB server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, hence (potentially) bridging wired and wireless networks. Therefore, in many instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier. Three topology deployment options are supported: - Results data over wireless - Results data over Ethernet - Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the InfluxDB server. The 3 connectivity options are discussed below.","title":"Connectivity Planning"},{"location":"influx_platform/#results-data-over-wireless","text":"In this topology the wiperf probe is configured to join an SSID that has the InfluxDB server accessible via its WLAN interface. Typically, the InfluxDB server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the InfluxDB server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of InfluxDB server>","title":"Results Data Over Wireless"},{"location":"influx_platform/#results-data-over-ethernet","text":"If the InfluxDB server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of InfluxDB server>","title":"Results data over Ethernet"},{"location":"influx_platform/#results-data-over-zerotierwireless","text":"A very simple way of getting the wiperf probe talking with your InfluxDB server is to use the Zerotier service to create a virtual network. In summary, both the InfluxDB server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same VLAN in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the InfluxDB server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees, It\u2019s very easy to use and get going, plus your InfluxDB server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the InfluxDB server and wiperf probe when the client is installed. Seriously, give it a go...it's quicker to try it than me explaining it here: https://www.zerotier.com/ config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of InfluxDB server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Results data over Zerotier/wireless"},{"location":"operation/","text":"Overview of Operation Wiperf is an open source utility that runs on a Raspberry Pi or a WLAN Pi hardware device. It provides network probe functionality to gather performance data to give an indication of how a network looks from an end user perspective. It runs a series of tests to gather metrics on network connectivity and performance through the execution of tests such ICMP pings, DNS lookups and iperf. These are fully configurable by editing a local configuration file on the probe device at the time of deployment. Configuration To configure the details of the tests to be run on a probe, a local configuration file on the probe needs to be updated. This will provide information to the probe about items such as the required network connectivity (e.g. wireless/ethernet), IP and credential information of the data server and test details. The configuration file can be updated by accessing the CLI of the probe (usually via SSH) and editing the file '/etc/wiperf/config.ini'. A default config template file is provided as a start point for the final configuration file ( '/etc/wiperf/config.default/ini'). It is best to take a copy of this file to create the final customised configuration file. When accessing the probe to create the configuration file, a Linux text editor such as 'nano' or 'vi' should be used. Here is a suggested workflow to create a probe configuration file: cd /etc/wiperf sudo cp ./config.default.ini ./config.ini sudo nano ./config.ini cron (Note: This operation is required for the Raspberry Pi only. The WLAN Pi will automatically setup the required cron job during the mode switch) In addition to the creating a customised configuration file for the probe, a mechanism is required to run the wiperf utility on a regular basis (e.g. every 5 minutes). Cron is a Linux utility that can be used to run wiperf periodically to gather data over time. The following CLI commands must be used to add a cron job to the probe to gather data on a regular basis: sudo crontab -e Add the following line to run the configured tests every 5 minutes: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 wpa_supplicant If the wiperf probe is to be connected to a wireless network, then details of the wireless network and the credentials to access the network need to be configured on the probe. This is achieved by configuring the following file: # on the RPi, edit the following file: sudo nano /etc/wpa_supplicant/wpa_supplicant.conf # on the WLAN Pi edit the following file: sudo nano /etc/wiperf/confi/etc/wpa_supplicant/wpa_supplicant.conf Logging Following the completion of the configuration described above, if all is configured correctly, then wiperf will run every 5 minutes, perform the configured tests, and then send the data back to the data server. A number of logs are generated to provide support information around the installation and operation of the wiperf process. Each of the generated log files are detailed below: # This log provides details of the installation and upgrade processes, so # can be useful in diagnosing installation issues /var/log/wiperf_install.log # This log file is updated by the main wiperf script each time # it is run. If the script appears to fail completely, this is # a good place to check /var/log/wiperg_cron.log # This log provides details of the tests performed each time # that wiperf runs. It is the main file to use for diagnosing # issues with wiperf /var/log/wiperf_agent.log Reporting Gathering the data with a probe is only half of the story when using wiperf. The gathered data must be sent to a data collection server to allow it to be visualized to allow analysis of network performance. The data server must be an instance of either: Splunk, or InfluxDB with Grafana Splunk The graphic above outlines the collection of network performance data and how this is sent to Splunk by the wiperf probe. The Splunk instance may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.) In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Splunk server for storage and later analysis The data is analyzed by accessing the dashboard (a web GUI) of the Splunk server (Note that the Splunk server acts as both the data repository and reporting platform for collected data) Grafana/Influx The graphic above outlines the collection of network performance data and how this is sent to Influx & Grafana by the wiperf probe. Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data source from which to pull its network performance data. To meet this requirement, the Influx database server is used. Like Grafana, InfluxDB is also an open-source package. Note this contrasts with using Splunk, which allows us to use the same package to provide both the data storage and analysis/visualization functions. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana & Influx may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.). Note that wiperf only needs to be able to send data to Influx - it requires no communication with the Grafana instance. In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Influx server for storage Grafana is configured to use Influx as its data source to allow visualization of performance data Data is analysed by accessing the dashboard (a web GUI) of the Grafana server, which pull the required dashboard data from Influx.","title":"Operation Overview"},{"location":"operation/#overview-of-operation","text":"Wiperf is an open source utility that runs on a Raspberry Pi or a WLAN Pi hardware device. It provides network probe functionality to gather performance data to give an indication of how a network looks from an end user perspective. It runs a series of tests to gather metrics on network connectivity and performance through the execution of tests such ICMP pings, DNS lookups and iperf. These are fully configurable by editing a local configuration file on the probe device at the time of deployment.","title":"Overview of Operation"},{"location":"operation/#configuration","text":"To configure the details of the tests to be run on a probe, a local configuration file on the probe needs to be updated. This will provide information to the probe about items such as the required network connectivity (e.g. wireless/ethernet), IP and credential information of the data server and test details. The configuration file can be updated by accessing the CLI of the probe (usually via SSH) and editing the file '/etc/wiperf/config.ini'. A default config template file is provided as a start point for the final configuration file ( '/etc/wiperf/config.default/ini'). It is best to take a copy of this file to create the final customised configuration file. When accessing the probe to create the configuration file, a Linux text editor such as 'nano' or 'vi' should be used. Here is a suggested workflow to create a probe configuration file: cd /etc/wiperf sudo cp ./config.default.ini ./config.ini sudo nano ./config.ini","title":"Configuration"},{"location":"operation/#cron","text":"(Note: This operation is required for the Raspberry Pi only. The WLAN Pi will automatically setup the required cron job during the mode switch) In addition to the creating a customised configuration file for the probe, a mechanism is required to run the wiperf utility on a regular basis (e.g. every 5 minutes). Cron is a Linux utility that can be used to run wiperf periodically to gather data over time. The following CLI commands must be used to add a cron job to the probe to gather data on a regular basis: sudo crontab -e Add the following line to run the configured tests every 5 minutes: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1","title":"cron"},{"location":"operation/#wpa_supplicant","text":"If the wiperf probe is to be connected to a wireless network, then details of the wireless network and the credentials to access the network need to be configured on the probe. This is achieved by configuring the following file: # on the RPi, edit the following file: sudo nano /etc/wpa_supplicant/wpa_supplicant.conf # on the WLAN Pi edit the following file: sudo nano /etc/wiperf/confi/etc/wpa_supplicant/wpa_supplicant.conf","title":"wpa_supplicant"},{"location":"operation/#logging","text":"Following the completion of the configuration described above, if all is configured correctly, then wiperf will run every 5 minutes, perform the configured tests, and then send the data back to the data server. A number of logs are generated to provide support information around the installation and operation of the wiperf process. Each of the generated log files are detailed below: # This log provides details of the installation and upgrade processes, so # can be useful in diagnosing installation issues /var/log/wiperf_install.log # This log file is updated by the main wiperf script each time # it is run. If the script appears to fail completely, this is # a good place to check /var/log/wiperg_cron.log # This log provides details of the tests performed each time # that wiperf runs. It is the main file to use for diagnosing # issues with wiperf /var/log/wiperf_agent.log","title":"Logging"},{"location":"operation/#reporting","text":"Gathering the data with a probe is only half of the story when using wiperf. The gathered data must be sent to a data collection server to allow it to be visualized to allow analysis of network performance. The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"Reporting"},{"location":"operation/#splunk","text":"The graphic above outlines the collection of network performance data and how this is sent to Splunk by the wiperf probe. The Splunk instance may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.) In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Splunk server for storage and later analysis The data is analyzed by accessing the dashboard (a web GUI) of the Splunk server (Note that the Splunk server acts as both the data repository and reporting platform for collected data)","title":"Splunk"},{"location":"operation/#grafanainflux","text":"The graphic above outlines the collection of network performance data and how this is sent to Influx & Grafana by the wiperf probe. Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data source from which to pull its network performance data. To meet this requirement, the Influx database server is used. Like Grafana, InfluxDB is also an open-source package. Note this contrasts with using Splunk, which allows us to use the same package to provide both the data storage and analysis/visualization functions. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana & Influx may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.). Note that wiperf only needs to be able to send data to Influx - it requires no communication with the Grafana instance. In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Influx server for storage Grafana is configured to use Influx as its data source to allow visualization of performance data Data is analysed by accessing the dashboard (a web GUI) of the Grafana server, which pull the required dashboard data from Influx.","title":"Grafana/Influx"},{"location":"probe_configure/","text":"Probe Configuration The final step in getting our probe ready to deploy is to configure the wiperf software to perform the tests we'd like to perform, and to tell wiperf where it can find the data server that will provide reporting. The configuration tasks break down as follows: Edit the probe config.ini file to configure tests and data server details Add a cron job on the probe to run wiperf every 5 mins to perform its tests Configuration File Note: the details in this section apply to both the WLAN Pi and RPi probe The operation of wiperf is configured using the file /etc/wiperf/config.ini This needs to be edited prior to running the wiperf software to perform tests. (Tests are initiated on the WLAN Pi by switching on to wiperf mode. On the RPi, tests are started by configuring a cron job - more on this later in this document) Prior to the first use of wiperf, the config.ini file does not exist. However, a default template config file ( /etc/wiperf/config.default.ini ) is supplied that can be used as the template to create the config.ini file. Here is the suggested workflow to create the config.ini file: Connect to the CLI of the probe (e.g. via SSH), create a copy of the config template file and edit the newly created config: cd /etc/wiperf cp ./config.default.ini ./config.ini sudo nano ./config.ini By default, the configuration file is set to run all tests (which may or may not suit your needs). However, there is a minimum configuration that must be applied to successfully run tests. The minimum configuration parameters you need to configure (just to get you going) are outlined in the subsections below. Once you've got your probe going, you're likely going to want to spend a little more time customising the file for your environment. In summary you need to: Configure the wiperf global mode of operation (wireless or Ethernet) and the interface parameters that determine how the probe is connected to its network Configure the management platform you'll be sending data to Configure the tests you'd like to run Mode/Interface Parameters The probe can be used to perform its tests over its wireless interface, or its ethernet interface. These are known as 'wireless' or 'ethernet' mode in the config.ini file. In addition, the probe needs to know which interface is used to send results data back to the data server. It is possible to perform tests and send results data over the same interface, or it may be preferable to have tests performed over the wireless interface and return results over the ethernet interface. The final choice is determined by the environment in to which the probe is deployed. (Note: if you choose to use Zerotier for management connectivity, the Zerotier interface is also an option available). The interfaces available in the probe for ethernet and wireless connectivity will generally be eth0 and wlan0 . However, these may vary in some platforms, there the option to change the actual names of the interfaces of the probe is available if required. The relevant section of the config.ini file is shown below for reference (note that lines that start with a semi-colon (;) are comments and are ignored. Blank lines are also ignored.): [General] ; global test mode: 'wireless' or 'ethernet' ; ; wireless mode: ; - test traffic runs over wireless interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; ethernet mode: ; - test traffic runs over ethernet interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; probe_mode: wireless ; ------------- ethernet mode parameters ------------ ; eth interface name - set this as per the output of an ifconfig command (usually eth0) eth_if: eth0 ; --------------------------------------------------- ; ------------- wireless mode parameters ------------ ; wlan interface name - set this as per the output of an iwconfig command (usually wlan0) wlan_if: wlan0 ; --------------------------------------------------- ; -------------mgt interface parameters ------------ ; interface name over which mgt traffic is sent (i.e. how we get to our management ; server) - options: wlan0, eth0, ztxxxxxx (ZeroTier), lo (local instance of Influx) mgt_if: wlan0 ; --------------------------------------------------- Data Server Parameters Wiperf can send results data to Splunk and InfluxDB (v1.x) data collectors through an exporter module for each collector type. The relevant authentication parameters need to be set for the collector in-use in the following sections (note these also need to be configured on the data collector platform also before sending results data - see here for more info: Splunk / InfluxDB ) In summary, the workflow to configure the data server parameters in the probe configuration file is to: Set the exporter type (splunk/influxdb) configure the server address of the target data server configure data server port details (if defaults changed) configure data server credential and database information The relevant section of the config.ini file is shown below: ; --------- Common Mgt Platform Params ------- ; set the data exporter type - current options: splunk, influxdb, influxdb2 exporter_type: splunk ; -------------------------------------------- ; -------------- Splunk Config --------------- ; IP address or hostname of Splunk host splunk_host: ; Splunk collector port (8088 by default) splunk_port: 8088 ; Splunk token to access Splunk server created by Splunk (example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d) splunk_token: ;--------------------------------------------- ; -------------- InFlux1 Config --------------- ; IP address or hostname of InfluxDB host influx_host: ; InfluxDb collector port (8086 by default) influx_port: 8086 influx_username: influx_password: influx_database: ;--------------------------------------------- Network Tests Note that all network tests are enabled by default. If there are some tests you'd like to disable (e.g. if you don't have an iperf3 server set up), then you'll need to open up the config.ini file and look through each section for the \"enabled\" parameter for that test and set it to \"no\". For example, to disable the iperf tcp test: [Iperf3_tcp_test] ; yes = enabled, no = disabled enabled: no For a full description of the configuration file parameters, please review the following page: config.ini reference guide . Running Regular Tests Once the wiperf software has been configured, the final job is to configure a 'cron job' on the probe to run the software every 5 minutes. Cron is a scheduler utility within Linux that will run a software task at configured intervals. ( Note: This step is not required on the WLAN Pi, as the cron job is added automatically when the WLAN Pi is switched in to wiperf mode ) To configure cron, on the CLI of the probe, open the cron editor: sudo crontab -e Next, with the editor open, add following line to the open file: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 This command will run the main wiperf script to run the tests configured within config.ini at an interval of 5 minutes. It will also dump all script output to the file /var/log/wiperf_cron.log (this is a good place to look if you hit any issues with wiperf not running as expected) Testing Once the cron job has been configured, the case of the RPi, or the WLAN Pi has been put in to wiperf mode, it's time to check if the probe is working as expected. To perform tests, the probe will need to be connected to a network and able to reach the data server. The easiest way to monitor the operation of the probe is to SSH in to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe: tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that look similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties. Once wiperf is running with no issues indicated in the logs, then it's time to check for results data on your data server. Hopefully, you'll see performance data being recorded over time as the probe runs its tests and sends the results to the data server.","title":"Probe Configuration"},{"location":"probe_configure/#probe-configuration","text":"The final step in getting our probe ready to deploy is to configure the wiperf software to perform the tests we'd like to perform, and to tell wiperf where it can find the data server that will provide reporting. The configuration tasks break down as follows: Edit the probe config.ini file to configure tests and data server details Add a cron job on the probe to run wiperf every 5 mins to perform its tests","title":"Probe Configuration"},{"location":"probe_configure/#configuration-file","text":"Note: the details in this section apply to both the WLAN Pi and RPi probe The operation of wiperf is configured using the file /etc/wiperf/config.ini This needs to be edited prior to running the wiperf software to perform tests. (Tests are initiated on the WLAN Pi by switching on to wiperf mode. On the RPi, tests are started by configuring a cron job - more on this later in this document) Prior to the first use of wiperf, the config.ini file does not exist. However, a default template config file ( /etc/wiperf/config.default.ini ) is supplied that can be used as the template to create the config.ini file. Here is the suggested workflow to create the config.ini file: Connect to the CLI of the probe (e.g. via SSH), create a copy of the config template file and edit the newly created config: cd /etc/wiperf cp ./config.default.ini ./config.ini sudo nano ./config.ini By default, the configuration file is set to run all tests (which may or may not suit your needs). However, there is a minimum configuration that must be applied to successfully run tests. The minimum configuration parameters you need to configure (just to get you going) are outlined in the subsections below. Once you've got your probe going, you're likely going to want to spend a little more time customising the file for your environment. In summary you need to: Configure the wiperf global mode of operation (wireless or Ethernet) and the interface parameters that determine how the probe is connected to its network Configure the management platform you'll be sending data to Configure the tests you'd like to run","title":"Configuration File"},{"location":"probe_configure/#modeinterface-parameters","text":"The probe can be used to perform its tests over its wireless interface, or its ethernet interface. These are known as 'wireless' or 'ethernet' mode in the config.ini file. In addition, the probe needs to know which interface is used to send results data back to the data server. It is possible to perform tests and send results data over the same interface, or it may be preferable to have tests performed over the wireless interface and return results over the ethernet interface. The final choice is determined by the environment in to which the probe is deployed. (Note: if you choose to use Zerotier for management connectivity, the Zerotier interface is also an option available). The interfaces available in the probe for ethernet and wireless connectivity will generally be eth0 and wlan0 . However, these may vary in some platforms, there the option to change the actual names of the interfaces of the probe is available if required. The relevant section of the config.ini file is shown below for reference (note that lines that start with a semi-colon (;) are comments and are ignored. Blank lines are also ignored.): [General] ; global test mode: 'wireless' or 'ethernet' ; ; wireless mode: ; - test traffic runs over wireless interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; ethernet mode: ; - test traffic runs over ethernet interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; probe_mode: wireless ; ------------- ethernet mode parameters ------------ ; eth interface name - set this as per the output of an ifconfig command (usually eth0) eth_if: eth0 ; --------------------------------------------------- ; ------------- wireless mode parameters ------------ ; wlan interface name - set this as per the output of an iwconfig command (usually wlan0) wlan_if: wlan0 ; --------------------------------------------------- ; -------------mgt interface parameters ------------ ; interface name over which mgt traffic is sent (i.e. how we get to our management ; server) - options: wlan0, eth0, ztxxxxxx (ZeroTier), lo (local instance of Influx) mgt_if: wlan0 ; ---------------------------------------------------","title":"Mode/Interface Parameters"},{"location":"probe_configure/#data-server-parameters","text":"Wiperf can send results data to Splunk and InfluxDB (v1.x) data collectors through an exporter module for each collector type. The relevant authentication parameters need to be set for the collector in-use in the following sections (note these also need to be configured on the data collector platform also before sending results data - see here for more info: Splunk / InfluxDB ) In summary, the workflow to configure the data server parameters in the probe configuration file is to: Set the exporter type (splunk/influxdb) configure the server address of the target data server configure data server port details (if defaults changed) configure data server credential and database information The relevant section of the config.ini file is shown below: ; --------- Common Mgt Platform Params ------- ; set the data exporter type - current options: splunk, influxdb, influxdb2 exporter_type: splunk ; -------------------------------------------- ; -------------- Splunk Config --------------- ; IP address or hostname of Splunk host splunk_host: ; Splunk collector port (8088 by default) splunk_port: 8088 ; Splunk token to access Splunk server created by Splunk (example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d) splunk_token: ;--------------------------------------------- ; -------------- InFlux1 Config --------------- ; IP address or hostname of InfluxDB host influx_host: ; InfluxDb collector port (8086 by default) influx_port: 8086 influx_username: influx_password: influx_database: ;---------------------------------------------","title":"Data Server Parameters"},{"location":"probe_configure/#network-tests","text":"Note that all network tests are enabled by default. If there are some tests you'd like to disable (e.g. if you don't have an iperf3 server set up), then you'll need to open up the config.ini file and look through each section for the \"enabled\" parameter for that test and set it to \"no\". For example, to disable the iperf tcp test: [Iperf3_tcp_test] ; yes = enabled, no = disabled enabled: no For a full description of the configuration file parameters, please review the following page: config.ini reference guide .","title":"Network Tests"},{"location":"probe_configure/#running-regular-tests","text":"Once the wiperf software has been configured, the final job is to configure a 'cron job' on the probe to run the software every 5 minutes. Cron is a scheduler utility within Linux that will run a software task at configured intervals. ( Note: This step is not required on the WLAN Pi, as the cron job is added automatically when the WLAN Pi is switched in to wiperf mode ) To configure cron, on the CLI of the probe, open the cron editor: sudo crontab -e Next, with the editor open, add following line to the open file: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 This command will run the main wiperf script to run the tests configured within config.ini at an interval of 5 minutes. It will also dump all script output to the file /var/log/wiperf_cron.log (this is a good place to look if you hit any issues with wiperf not running as expected)","title":"Running Regular Tests"},{"location":"probe_configure/#testing","text":"Once the cron job has been configured, the case of the RPi, or the WLAN Pi has been put in to wiperf mode, it's time to check if the probe is working as expected. To perform tests, the probe will need to be connected to a network and able to reach the data server. The easiest way to monitor the operation of the probe is to SSH in to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe: tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that look similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties. Once wiperf is running with no issues indicated in the logs, then it's time to check for results data on your data server. Hopefully, you'll see performance data being recorded over time as the probe runs its tests and sends the results to the data server.","title":"Testing"},{"location":"probe_deploy/","text":"Probe Deployment TBA DNS Lookup Order Issue By default in WLAN Pi image versions up to and including v1.9.1, the DNS server value 8.8.8.8 has been hard-coded in to the DNS server lookup list. This has been achieved by configuring the following line in to the file /etc/resolvconf/resolv.conf.d/head : nameserver 8.8.8.8 This ensures that the 8.8.8.8 address is always used as the first DNS lookup entry, even if the WLAN Pi receives a DNS server address during its DHCP process. This can be verified by performing a cat /etc/resolv.conf when the WLAN Pi is in wiperf mode - even though there may be two or more entries in the file, 8.8.8.8 will always be shown at the top of the file listing, and be used as the first lookup server. This may not be desirable behaviour, as 8.8.8.8 may not be available or DHCP assigned servers may be preferred. To ensure that the desired DNS environment is used, the following options exist: Remove the 8.8.8.8 entry completely: edit the file /etc/resolvconf/resolv.conf.d/head and remove the offending entry completely. This will ensure only the DHCP assigned DNS server(s) will be used. Replace the 8.8.8.8 entry by editing the /etc/resolvconf/resolv.conf.d/head and replacing it with a desired value Move the 8.8.8.8 entry from the /etc/resolvconf/resolv.conf.d/head file to the /etc/resolvconf/resolv.conf.d/tail file, so that 8.8.8.8 is still used, but is now the last option in the DNS server list (only used when other preceding servers do not return a result) top MCS & Rx Phy rates Missing From Reports top In several dashboard reports, the reported MCS values & Rx Phy rate may be blank. This is because these values are only reported by MediaTek wireless NICs. Therefore, the CF-912 will not show these values (as it is a Realtek NIC). Sorry, not much I can do about this. top Tests Fail To Start Due to DNS Failures In versions of wiperf before version v0.10, the wiperf probe performed a series of tests to verify the health of the wireless connection prior to tests running. One of these tests included a DNS lookup to \"bbc.co.uk\" to verify Internet connectivity. In some environments, this may not be a valid test. To fix this issue, a new configuring parameter was added to the config.ini file that allows a custom lookup target to be provided, if requried: connectivity_lookup: google.com top top Additional Features: Watchdog Wiperf has a watchdog feature that it uses to try to reset things when it is having connectivity related difficulties. There may be instances when tests are continually failing or wireless connectivity is intermittent due to perhaps being stuck on a remote AP that is sub-optimal from a connectivity perspective. If persistent issues are detected, then Wiperf will reboot the WLAN Pi to try to remediate the issue. This will provide the opportunity to the reset all network connectivity and internal processes. Note that this is a last ditch mechanism. Wiperf will try bouncing the WLAN interface to remediate any short-term connectivity issues, which will likely fixe many issues without the need for a full reboot. If you observe your WLAN Pi rebooting on a regular basis (e.g. a couple of times a hour), then check its logs as it is very unhappy about something. top Security Wiperf employs the following security mechanisms in an attempt to harden the WLAN Pi when deployed in Wiperf mode: No forwarding is allowed between interfaces The internal UFW firewall is configured to only allow incoming connectivity on port 22 on the wlan0 & eth0 interfaces top","title":"Probe Deployment"},{"location":"probe_deploy/#probe-deployment","text":"TBA","title":"Probe Deployment"},{"location":"probe_deploy/#dns-lookup-order-issue","text":"By default in WLAN Pi image versions up to and including v1.9.1, the DNS server value 8.8.8.8 has been hard-coded in to the DNS server lookup list. This has been achieved by configuring the following line in to the file /etc/resolvconf/resolv.conf.d/head : nameserver 8.8.8.8 This ensures that the 8.8.8.8 address is always used as the first DNS lookup entry, even if the WLAN Pi receives a DNS server address during its DHCP process. This can be verified by performing a cat /etc/resolv.conf when the WLAN Pi is in wiperf mode - even though there may be two or more entries in the file, 8.8.8.8 will always be shown at the top of the file listing, and be used as the first lookup server. This may not be desirable behaviour, as 8.8.8.8 may not be available or DHCP assigned servers may be preferred. To ensure that the desired DNS environment is used, the following options exist: Remove the 8.8.8.8 entry completely: edit the file /etc/resolvconf/resolv.conf.d/head and remove the offending entry completely. This will ensure only the DHCP assigned DNS server(s) will be used. Replace the 8.8.8.8 entry by editing the /etc/resolvconf/resolv.conf.d/head and replacing it with a desired value Move the 8.8.8.8 entry from the /etc/resolvconf/resolv.conf.d/head file to the /etc/resolvconf/resolv.conf.d/tail file, so that 8.8.8.8 is still used, but is now the last option in the DNS server list (only used when other preceding servers do not return a result) top","title":"DNS Lookup Order Issue"},{"location":"probe_deploy/#mcs-rx-phy-rates-missing-from-reports","text":"top In several dashboard reports, the reported MCS values & Rx Phy rate may be blank. This is because these values are only reported by MediaTek wireless NICs. Therefore, the CF-912 will not show these values (as it is a Realtek NIC). Sorry, not much I can do about this. top","title":"MCS &amp; Rx Phy rates Missing From Reports"},{"location":"probe_deploy/#tests-fail-to-start-due-to-dns-failures","text":"In versions of wiperf before version v0.10, the wiperf probe performed a series of tests to verify the health of the wireless connection prior to tests running. One of these tests included a DNS lookup to \"bbc.co.uk\" to verify Internet connectivity. In some environments, this may not be a valid test. To fix this issue, a new configuring parameter was added to the config.ini file that allows a custom lookup target to be provided, if requried: connectivity_lookup: google.com top top","title":"Tests Fail To Start Due to DNS Failures"},{"location":"probe_deploy/#additional-features","text":"","title":"Additional Features:"},{"location":"probe_deploy/#watchdog","text":"Wiperf has a watchdog feature that it uses to try to reset things when it is having connectivity related difficulties. There may be instances when tests are continually failing or wireless connectivity is intermittent due to perhaps being stuck on a remote AP that is sub-optimal from a connectivity perspective. If persistent issues are detected, then Wiperf will reboot the WLAN Pi to try to remediate the issue. This will provide the opportunity to the reset all network connectivity and internal processes. Note that this is a last ditch mechanism. Wiperf will try bouncing the WLAN interface to remediate any short-term connectivity issues, which will likely fixe many issues without the need for a full reboot. If you observe your WLAN Pi rebooting on a regular basis (e.g. a couple of times a hour), then check its logs as it is very unhappy about something. top","title":"Watchdog"},{"location":"probe_deploy/#security","text":"Wiperf employs the following security mechanisms in an attempt to harden the WLAN Pi when deployed in Wiperf mode: No forwarding is allowed between interfaces The internal UFW firewall is configured to only allow incoming connectivity on port 22 on the wlan0 & eth0 interfaces top","title":"Security"},{"location":"probe_install/","text":"Probe Software Installation This section takes a look at how we install various additional required software packages on to our probe. This includes any pre-requisite software packages and the wiperf software itself. WLAN Pi Good news! If you're using a WLAN Pi (v2.x image), you already have the software you require - it's part of the WLAN Pi software image. Go to the next section of this documentation site . Raspberry Pi The RPi requires a few pre-requisite Linux packages before we can install the wiperf software itself. Note that the probe must be connected to a network (via ethernet or wireless) that has access to the Internet to download the required code. You will need CLI access to the probe to perform the steps detailed below. Package Updates Before we start adding pre-requisite packages, it's always a good idea to update the existing Linux packages on our RPi to make sure we have the \"latest and greatest\". This may take a few minutes to complete as many files may be downloaded & updated, depending on when/if your RPi was last updated: sudo apt-get update && sudo apt-get upgrade -y sudo reboot Pre-requisite Packages Next, we need to install additional Linux packages that are not included as part of the standard RPi distribution: pip3, iperf3 and git. These are installed as follows: sudo apt-get update sudo apt-get install python3-pip iperf3 git -y sudo reboot wiperf Software Installation To install the wiperf code itself on to the RPi, execute the following command: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/setup.sh | sudo bash -s install rpi This will initiate the download and installation of a number of python packages, together with the wiperf code itself. This will take a few minutes to complete. Once installation is complete, our final step is to configure the wiperf probe to perform the tests we'd like to perform, and provide details of were the probe needs to send its data (i.e. our data server).","title":"Probe Software Installation"},{"location":"probe_install/#probe-software-installation","text":"This section takes a look at how we install various additional required software packages on to our probe. This includes any pre-requisite software packages and the wiperf software itself.","title":"Probe Software Installation"},{"location":"probe_install/#wlan-pi","text":"Good news! If you're using a WLAN Pi (v2.x image), you already have the software you require - it's part of the WLAN Pi software image. Go to the next section of this documentation site .","title":"WLAN Pi"},{"location":"probe_install/#raspberry-pi","text":"The RPi requires a few pre-requisite Linux packages before we can install the wiperf software itself. Note that the probe must be connected to a network (via ethernet or wireless) that has access to the Internet to download the required code. You will need CLI access to the probe to perform the steps detailed below.","title":"Raspberry Pi"},{"location":"probe_install/#package-updates","text":"Before we start adding pre-requisite packages, it's always a good idea to update the existing Linux packages on our RPi to make sure we have the \"latest and greatest\". This may take a few minutes to complete as many files may be downloaded & updated, depending on when/if your RPi was last updated: sudo apt-get update && sudo apt-get upgrade -y sudo reboot","title":"Package Updates"},{"location":"probe_install/#pre-requisite-packages","text":"Next, we need to install additional Linux packages that are not included as part of the standard RPi distribution: pip3, iperf3 and git. These are installed as follows: sudo apt-get update sudo apt-get install python3-pip iperf3 git -y sudo reboot","title":"Pre-requisite Packages"},{"location":"probe_install/#wiperf-software-installation","text":"To install the wiperf code itself on to the RPi, execute the following command: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/setup.sh | sudo bash -s install rpi This will initiate the download and installation of a number of python packages, together with the wiperf code itself. This will take a few minutes to complete. Once installation is complete, our final step is to configure the wiperf probe to perform the tests we'd like to perform, and provide details of were the probe needs to send its data (i.e. our data server).","title":"wiperf Software Installation"},{"location":"probe_platform/","text":"Probe Platform Wiperf has been primarily designed to work on the NEO2 version of the WLAN Pi platform and the Raspberry Pi. WLAN Pi Wiperf is baked in to the image of the WLAN Pi. It can be activated by switching in to wiperf mode on the WLAN Pi. Find out more details at the official documentation site for the WLAN Pi: https://wlan-pi.github.io/wlanpi-documentation/ Raspberry Pi Wiperf on the RPi has been tested on models that have an internal Wi-Fi NIC: 3b+, 3a+ and 4. It will likely work on most that have an internal NIC, but I don't have the resources or time to try them all. Earlier versions of the RPi that do not have a an internal NIC will need some type of USB wireless adapter, but as support for external wireless NICs is very poor and many tend to be 2.4GHz only, I've not explored this area in detail. Unfortunately, getting a 2 stream 802.11ac NIC going seems nigh-on impossible due to the lack of drivers available, so the internal, single stream NIC is the best we can generally do. Using a single stream NIC has its limitations as speed performance is very limited, but as the main aim of wiperf is to monitor user experience (particularly changes in that experience), then it's good enough for many use-cases where we are mainly interested in changes in relation to the usual baseline. Other Platforms In essence, wiperf is a series of python scripts & modules, together with a few supporting bash scripts to glue a few things together. It will likely work on other Debian-type systems, so it's worth giving it a go on other systems if you fancy tinkering around on another platform. When using the install script, install using the 'rpi' option. Let me know if you get it going on other platforms, as it will be interesting to share your experiences.","title":"Probe Platform"},{"location":"probe_platform/#probe-platform","text":"Wiperf has been primarily designed to work on the NEO2 version of the WLAN Pi platform and the Raspberry Pi.","title":"Probe Platform"},{"location":"probe_platform/#wlan-pi","text":"Wiperf is baked in to the image of the WLAN Pi. It can be activated by switching in to wiperf mode on the WLAN Pi. Find out more details at the official documentation site for the WLAN Pi: https://wlan-pi.github.io/wlanpi-documentation/","title":"WLAN Pi"},{"location":"probe_platform/#raspberry-pi","text":"Wiperf on the RPi has been tested on models that have an internal Wi-Fi NIC: 3b+, 3a+ and 4. It will likely work on most that have an internal NIC, but I don't have the resources or time to try them all. Earlier versions of the RPi that do not have a an internal NIC will need some type of USB wireless adapter, but as support for external wireless NICs is very poor and many tend to be 2.4GHz only, I've not explored this area in detail. Unfortunately, getting a 2 stream 802.11ac NIC going seems nigh-on impossible due to the lack of drivers available, so the internal, single stream NIC is the best we can generally do. Using a single stream NIC has its limitations as speed performance is very limited, but as the main aim of wiperf is to monitor user experience (particularly changes in that experience), then it's good enough for many use-cases where we are mainly interested in changes in relation to the usual baseline.","title":"Raspberry Pi"},{"location":"probe_platform/#other-platforms","text":"In essence, wiperf is a series of python scripts & modules, together with a few supporting bash scripts to glue a few things together. It will likely work on other Debian-type systems, so it's worth giving it a go on other systems if you fancy tinkering around on another platform. When using the install script, install using the 'rpi' option. Let me know if you get it going on other platforms, as it will be interesting to share your experiences.","title":"Other Platforms"},{"location":"probe_prepare/","text":"Probe Preparation The wiperf probe needs to have a few pre-requisite activities completed prior to the installation of the wiperf code. These vary slightly between the WLAN Pi and RPi platforms, but broadly break down as: Software image preparation CLI Access Configure the device hostname Configure network connectivity Add pre-requisite software packages. WLAN Pi Software Image There is little to do in terms of software image preparation for the WLAN Pi. Visit the WLAN Pi documentation site to find out how to obtain the WLAN Pi image: https://wlan-pi.github.io/wlanpi-documentation/ . If you install the a WLAN Pi image, wiperf will already be installed as part of the image. (Note: all information provided below assumes you are using a 2.0 or later version of the WLAN Pi image) Probe CLI Access To perform some of the configuration activities required, CLI access to the WLAN Pi is required. The easiest way to achieve this is to SSH to the probe over an OTG connection, or plug the WLAN Pi in to an ethernet network port and SSH to its DHCP assigned IP address (shown on the front panel). Visit the WLAN Pi documentation site for more details: https://wlan-pi.github.io/wlanpi-documentation/ Hostname Configuration By default, the hostname of your WLAN Pi is : wlanpi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the WLAN Pi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'wlanpi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Alternatively, you may also use the following CLI command to achieve the same result: sudo hostnamectl set-hostname <name> Whichever method is used to update the hostname file, next edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'wlanpi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your WLAN Pi: sudo reboot Network Connectivity Ethernet If the probe is to be connected by Ethernet only, then there is no additional configuration required. By default, if a switch port that can supply a DHCP address is used, then the probe will have the required network connection. Wireless Configuration (wpa_supplicant.conf) If wiperf is running in wireless mode, when the WLAN Pi is flipped in to wiperf mode, it will need to join the SSID under test to run the configured tests. We need to provide a configuration (that is only used in wiperf mode) to allow the WLAN Pi to join a WLAN. Edit the following file with the configuration and credentials that will be used by the WLAN Pi to join the SSID under test once it is switched in to wiperf mode: cd /etc/wiperf/conf/etc/wpa_supplicant sudo nano ./wpa_supplicant.conf There are a number of sample configurations included in the default file provided (PSK, PEAP & Open auth). Uncomment the required section and add in the correct SSID & authentication details. (For EAP-TLS, it's time to check-out Google as I've not had opportunity to figure that scenario out...) (Note: This configuration is only used when the WLAN Pi is flipped in to wiperf mode, not for standard (classic mode) connectivity) Raspberry Pi Software Image I would strongly recommend starting with a fresh image using the latest and greatest Raspberry Pi OS (previously called Raspbian): https://www.raspberrypi.org/downloads/raspberry-pi-os/ . For the development and testing of the wiperf code, version 10 (Buster) was used. You can check the version on your RPi using the cat /etc/os0-release command. Here is my sample output: pi@probe7:~$ cat /etc/os-release PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" NAME=\"Raspbian GNU/Linux\" VERSION_ID=\"10\" VERSION=\"10 (buster)\" VERSION_CODENAME=buster ID=raspbian ID_LIKE=debian HOME_URL=\"http://www.raspbian.org/\" SUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\" BUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\" Note that you will likely be able to use any recent version, so don't feel compelled to use this exact version. The download page provided above also has links to resources to guide you through burning the image on to your SD card. (You may also like to check out the 'Probe CLI Access' section below to setup SSH access to your headless RPI before booting from your new image) Once you have burned your image, I'd also recommend you apply all latest updates & give it a reboot for good measure: sudo apt-get update && sudo apt-get upgrade sudo reboot Probe CLI Access You will need CLI access to perform the required configuration steps for wiperf. There are a number of ways of gaining this access that are detailed in this document: https://www.raspberrypi.org/documentation/remote-access/ssh/ . My personal favourite is to enable SSH on a headless RPi by adding an 'ssh' file to the SD card prior to boot-up. Default Login Account Password If using a fresh RPI image (which is recommended), remember to either update the default 'pi' username with a new password so that your are not running with the default login of : pi/raspberry (user/pwd) Change password : sudo passwd pi Hostname Configuration By default, the hostname of your RPi is : pi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change this to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the RPi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'pi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Alternatively, you may also use the following CLI command to achieve the same result: sudo hostnamectl set-hostname <name> Whichever method is used to update the hostname file, next edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'pi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your RPi: sudo reboot Network Connectivity Ethernet If the probe is to be connected by Ethernet you will need to make some additions to the /etc/network/interfaces file to ensure you have network connectivity. Add the following lines to configure the Ethernet port for DHCP connectivity: # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp These lines may be added anywhere in the file, using a CLI editor such as nano: sudo nano /etc/network/interfaces Wireless Configuration The RPi needs to be configured to join the wireless network that you'd like to test. To join a network, we need to configure the wireless interface and provide the network credentials to join the network. To achieve this, we need to edit two files on the CLI of the RPI: sudo nano /etc/wpa_supplicant/wpa_supplicant.conf sudo nano /etc/network/interfaces Sample configurations for both files are provided below. /etc/network/interfaces # wiperf interface config file # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp # Wireless adapter #1 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf # wireless-power off # post-up iw dev wlan0 set power_save off # Local loopback auto lo iface lo inet loopback Note: The wireless power off commands are commented out in the file above. One of these needs to be uncommented to stop the wireless NC dropping in to power save mode. If you see huge drops in the wireless connection speed in the wireless connection graph, it is being caused by power save mode. Unfortunately, the command to use seems to vary between RPi model and operating system version. When you see the connection speed issue, try uncommenting one of the commands and reboot. If it doesn't fix the issue, try the other command. (see this article for more info ) /etc/wpa_supplicant/wpa_supplicant.conf ap_scan=1 # WPA2 PSK Network sample (highest priority - joined first) network={ ssid=\"enter SSID Name\" psk=\"enter key\" priority=10 } ####################################################################################### # NOTE: to use the templates below, remove the hash symbols at the start of each line ####################################################################################### # WPA2 PSK Network sample (next priority - joined if first priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # psk=\"enter key\" # priority=3 #} # WPA2 PEAP example (next priority - joined if second priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=WPA-EAP # eap=PEAP # anonymous_identity=\"anonymous\" # identity=\"enter your username\" # password=\"enter your password\" # phase2=\"autheap=MSCHAPV2\" # priority=2 #} # Open network example (lowest priority, only joined other 3 networks not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=NONE # priority=1 #} Note that the file includes several samples for a variety of security methods. You will need to uncomment the security mthod for your environment and comment out all other methods. By default, the PSK method is used, bit requires that you enter an SSID and shared key. Test Wireless Connection Once configuration is complete, reboot the RPI. To test if the wireless connection has come up OK, use the following commands to see if wireless interface has joined the wireless network and has an IP address: iwconfig ifconfig","title":"Probe Preparation"},{"location":"probe_prepare/#probe-preparation","text":"The wiperf probe needs to have a few pre-requisite activities completed prior to the installation of the wiperf code. These vary slightly between the WLAN Pi and RPi platforms, but broadly break down as: Software image preparation CLI Access Configure the device hostname Configure network connectivity Add pre-requisite software packages.","title":"Probe Preparation"},{"location":"probe_prepare/#wlan-pi","text":"","title":"WLAN Pi"},{"location":"probe_prepare/#software-image","text":"There is little to do in terms of software image preparation for the WLAN Pi. Visit the WLAN Pi documentation site to find out how to obtain the WLAN Pi image: https://wlan-pi.github.io/wlanpi-documentation/ . If you install the a WLAN Pi image, wiperf will already be installed as part of the image. (Note: all information provided below assumes you are using a 2.0 or later version of the WLAN Pi image)","title":"Software Image"},{"location":"probe_prepare/#probe-cli-access","text":"To perform some of the configuration activities required, CLI access to the WLAN Pi is required. The easiest way to achieve this is to SSH to the probe over an OTG connection, or plug the WLAN Pi in to an ethernet network port and SSH to its DHCP assigned IP address (shown on the front panel). Visit the WLAN Pi documentation site for more details: https://wlan-pi.github.io/wlanpi-documentation/","title":"Probe CLI Access"},{"location":"probe_prepare/#hostname-configuration","text":"By default, the hostname of your WLAN Pi is : wlanpi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the WLAN Pi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'wlanpi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Alternatively, you may also use the following CLI command to achieve the same result: sudo hostnamectl set-hostname <name> Whichever method is used to update the hostname file, next edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'wlanpi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your WLAN Pi: sudo reboot","title":"Hostname Configuration"},{"location":"probe_prepare/#network-connectivity","text":"","title":"Network Connectivity"},{"location":"probe_prepare/#ethernet","text":"If the probe is to be connected by Ethernet only, then there is no additional configuration required. By default, if a switch port that can supply a DHCP address is used, then the probe will have the required network connection.","title":"Ethernet"},{"location":"probe_prepare/#wireless-configuration-wpa_supplicantconf","text":"If wiperf is running in wireless mode, when the WLAN Pi is flipped in to wiperf mode, it will need to join the SSID under test to run the configured tests. We need to provide a configuration (that is only used in wiperf mode) to allow the WLAN Pi to join a WLAN. Edit the following file with the configuration and credentials that will be used by the WLAN Pi to join the SSID under test once it is switched in to wiperf mode: cd /etc/wiperf/conf/etc/wpa_supplicant sudo nano ./wpa_supplicant.conf There are a number of sample configurations included in the default file provided (PSK, PEAP & Open auth). Uncomment the required section and add in the correct SSID & authentication details. (For EAP-TLS, it's time to check-out Google as I've not had opportunity to figure that scenario out...) (Note: This configuration is only used when the WLAN Pi is flipped in to wiperf mode, not for standard (classic mode) connectivity)","title":"Wireless Configuration (wpa_supplicant.conf)"},{"location":"probe_prepare/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"probe_prepare/#software-image_1","text":"I would strongly recommend starting with a fresh image using the latest and greatest Raspberry Pi OS (previously called Raspbian): https://www.raspberrypi.org/downloads/raspberry-pi-os/ . For the development and testing of the wiperf code, version 10 (Buster) was used. You can check the version on your RPi using the cat /etc/os0-release command. Here is my sample output: pi@probe7:~$ cat /etc/os-release PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" NAME=\"Raspbian GNU/Linux\" VERSION_ID=\"10\" VERSION=\"10 (buster)\" VERSION_CODENAME=buster ID=raspbian ID_LIKE=debian HOME_URL=\"http://www.raspbian.org/\" SUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\" BUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\" Note that you will likely be able to use any recent version, so don't feel compelled to use this exact version. The download page provided above also has links to resources to guide you through burning the image on to your SD card. (You may also like to check out the 'Probe CLI Access' section below to setup SSH access to your headless RPI before booting from your new image) Once you have burned your image, I'd also recommend you apply all latest updates & give it a reboot for good measure: sudo apt-get update && sudo apt-get upgrade sudo reboot","title":"Software Image"},{"location":"probe_prepare/#probe-cli-access_1","text":"You will need CLI access to perform the required configuration steps for wiperf. There are a number of ways of gaining this access that are detailed in this document: https://www.raspberrypi.org/documentation/remote-access/ssh/ . My personal favourite is to enable SSH on a headless RPi by adding an 'ssh' file to the SD card prior to boot-up.","title":"Probe CLI Access"},{"location":"probe_prepare/#default-login-account-password","text":"If using a fresh RPI image (which is recommended), remember to either update the default 'pi' username with a new password so that your are not running with the default login of : pi/raspberry (user/pwd) Change password : sudo passwd pi","title":"Default Login Account Password"},{"location":"probe_prepare/#hostname-configuration_1","text":"By default, the hostname of your RPi is : pi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change this to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the RPi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'pi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Alternatively, you may also use the following CLI command to achieve the same result: sudo hostnamectl set-hostname <name> Whichever method is used to update the hostname file, next edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'pi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your RPi: sudo reboot","title":"Hostname Configuration"},{"location":"probe_prepare/#network-connectivity_1","text":"","title":"Network Connectivity"},{"location":"probe_prepare/#ethernet_1","text":"If the probe is to be connected by Ethernet you will need to make some additions to the /etc/network/interfaces file to ensure you have network connectivity. Add the following lines to configure the Ethernet port for DHCP connectivity: # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp These lines may be added anywhere in the file, using a CLI editor such as nano: sudo nano /etc/network/interfaces","title":"Ethernet"},{"location":"probe_prepare/#wireless-configuration","text":"The RPi needs to be configured to join the wireless network that you'd like to test. To join a network, we need to configure the wireless interface and provide the network credentials to join the network. To achieve this, we need to edit two files on the CLI of the RPI: sudo nano /etc/wpa_supplicant/wpa_supplicant.conf sudo nano /etc/network/interfaces Sample configurations for both files are provided below.","title":"Wireless Configuration"},{"location":"probe_prepare/#etcnetworkinterfaces","text":"# wiperf interface config file # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp # Wireless adapter #1 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf # wireless-power off # post-up iw dev wlan0 set power_save off # Local loopback auto lo iface lo inet loopback Note: The wireless power off commands are commented out in the file above. One of these needs to be uncommented to stop the wireless NC dropping in to power save mode. If you see huge drops in the wireless connection speed in the wireless connection graph, it is being caused by power save mode. Unfortunately, the command to use seems to vary between RPi model and operating system version. When you see the connection speed issue, try uncommenting one of the commands and reboot. If it doesn't fix the issue, try the other command. (see this article for more info )","title":"/etc/network/interfaces"},{"location":"probe_prepare/#etcwpa_supplicantwpa_supplicantconf","text":"ap_scan=1 # WPA2 PSK Network sample (highest priority - joined first) network={ ssid=\"enter SSID Name\" psk=\"enter key\" priority=10 } ####################################################################################### # NOTE: to use the templates below, remove the hash symbols at the start of each line ####################################################################################### # WPA2 PSK Network sample (next priority - joined if first priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # psk=\"enter key\" # priority=3 #} # WPA2 PEAP example (next priority - joined if second priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=WPA-EAP # eap=PEAP # anonymous_identity=\"anonymous\" # identity=\"enter your username\" # password=\"enter your password\" # phase2=\"autheap=MSCHAPV2\" # priority=2 #} # Open network example (lowest priority, only joined other 3 networks not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=NONE # priority=1 #} Note that the file includes several samples for a variety of security methods. You will need to uncomment the security mthod for your environment and comment out all other methods. By default, the PSK method is used, bit requires that you enter an SSID and shared key.","title":"/etc/wpa_supplicant/wpa_supplicant.conf"},{"location":"probe_prepare/#test-wireless-connection","text":"Once configuration is complete, reboot the RPI. To test if the wireless connection has come up OK, use the following commands to see if wireless interface has joined the wireless network and has an IP address: iwconfig ifconfig","title":"Test Wireless Connection"},{"location":"splunk_configure/","text":"Splunk Configuration Now that we have a Splunk server setup, we need to customize it to report our probe data. The steps required are all via the Splunk web GUI and are the same for all OS flavours. Configure Data Input To Splunk We need to tell Splunk how we\u2019ll be sending the data from our probe to Splunk. We need to configure a data input that will prepare SPlunk to receive the data and generate an authorization key to be used by the probe when sending data. Log In To Splunk The first step is to login to Splunk using the credentials created during the Splunk install. The URL to use is: http://<Splunk_server_IP>:8000 Configure HTTP Event Collector Global Options After login, the following page will be seen: Follow the \u201cSettings > Data > Data Inputs\u201d menu options : Click on the HTTP Event Collector link in the Data Inputs page shown: Click on the \u201cGlobal Settings\u201d button as indicated in the graphic above to reveal the global configuration panel for the HTTP Event Collector: Ensure the panel is configured to look like the example shown above. This should require the following steps: Make sure you hit the All Tokens > Enable button (disabled by default which stops everything working) Default Source Type: Structured > _json Hit: Save to take you back to the HTTP Event Collector page Create a HEC Token After returning to the HTTP Event Collector page, hit the New Token button. This will start a token creation wizard. Enter a name for the token (probe HEC Token) then hit Next > : In the next wizard panel select Source type: Select > Structured > _json : Scroll down to the indexes and make the following selections: Select Allowed Indexes > add all Default Index : main Next, hit the Review > button: The token review panel is now should and should look like the graphic below. Finally hit the Submit > button: A final confirmation message will be provided as shown below: If you return to Settings > Data Input > HTTPS Event Collector, you will now see the token your probe will need to communicate with the Splunk server: At this point, the Splunk server is ready to receive data from the probe. Ensure that your probe has been configured with the correct server IP address, port number and the token we have just created above (copy and paste the \u201cToken Value\u201d in to your probe config.ini file). If using the WLAN Pi, ensure that it is flipped in to wiperf mode. On the RPI, ensure that the required cron job has been configured to start polling. Perform a Test Search After a few minutes, when the probe has run a test cycle, data should start to appear in Splunk. The quickest way to check is to do a general search for data in Splunk and see what is being received. Go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and enter a \u201c*\u201d in the \u201cNew Search\u201d text box. Results data should be seen as shown below: If your search result looks like this (no results found message), then you need to wait a little longer for data to arrive, or there is likely a comms problem between your probe and Splunk: Create a Dashboard Now that we have data arriving at our Splunk server, we need to view the data in an interesting format. Splunk allows us to create a number of dashboards to visualize our data. We will now create a simple dashboard to demonstrate the visualization capabilities. In the WLANPi\u2019s /usr/share/wiperf/dashboards directory, a number of pre-canned dashboard files have been provided to allow a dashboard to be copied and pasted easily. These are also available on the GitHub page of the Wiperf project: https://github.com/wifinigel/wiperf/tree/master/dashboards Use an SFTP client to pull the \u201cprobe_summary.xml\u201d file from your probe or open the file on the GitHub page and select \u201cRaw\u201d to copy and paste the code in to a local file on your laptop. In the Splunk GUI, go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and hit the \u201cDashboards\u201d link: Hit the \"Create New Dashboard\" button: In the pop-up panel, enter a dashboard name and hit the \"Create Dashboard\" button: In the \u201cEdit Dashboard\u201d panel that opens, hit the \u201cSource\u201d button: By default, some basic XML configuration will exist in the dashboard definition: Open up the dashboard definition file previously downloaded from your probe or the GitHub site in a text editor. Then simply paste in the code as shown below (make sure the original code was all removed): After hitting the Save button, the dashboard will now be shown: Using the hostname and time period selector above the graphs, different probes and reporting periods may be viewed. The process above may be repeated using each of the xml files found in the dashboards folder to create a series of separate dashboards that focus on different aspects of data available using wiperf.","title":"Splunk Configuration"},{"location":"splunk_configure/#splunk-configuration","text":"Now that we have a Splunk server setup, we need to customize it to report our probe data. The steps required are all via the Splunk web GUI and are the same for all OS flavours.","title":"Splunk Configuration"},{"location":"splunk_configure/#configure-data-input-to-splunk","text":"We need to tell Splunk how we\u2019ll be sending the data from our probe to Splunk. We need to configure a data input that will prepare SPlunk to receive the data and generate an authorization key to be used by the probe when sending data.","title":"Configure Data Input To Splunk"},{"location":"splunk_configure/#log-in-to-splunk","text":"The first step is to login to Splunk using the credentials created during the Splunk install. The URL to use is: http://<Splunk_server_IP>:8000","title":"Log In To Splunk"},{"location":"splunk_configure/#configure-http-event-collector-global-options","text":"After login, the following page will be seen: Follow the \u201cSettings > Data > Data Inputs\u201d menu options : Click on the HTTP Event Collector link in the Data Inputs page shown: Click on the \u201cGlobal Settings\u201d button as indicated in the graphic above to reveal the global configuration panel for the HTTP Event Collector: Ensure the panel is configured to look like the example shown above. This should require the following steps: Make sure you hit the All Tokens > Enable button (disabled by default which stops everything working) Default Source Type: Structured > _json Hit: Save to take you back to the HTTP Event Collector page","title":"Configure  HTTP Event Collector Global Options"},{"location":"splunk_configure/#create-a-hec-token","text":"After returning to the HTTP Event Collector page, hit the New Token button. This will start a token creation wizard. Enter a name for the token (probe HEC Token) then hit Next > : In the next wizard panel select Source type: Select > Structured > _json : Scroll down to the indexes and make the following selections: Select Allowed Indexes > add all Default Index : main Next, hit the Review > button: The token review panel is now should and should look like the graphic below. Finally hit the Submit > button: A final confirmation message will be provided as shown below: If you return to Settings > Data Input > HTTPS Event Collector, you will now see the token your probe will need to communicate with the Splunk server: At this point, the Splunk server is ready to receive data from the probe. Ensure that your probe has been configured with the correct server IP address, port number and the token we have just created above (copy and paste the \u201cToken Value\u201d in to your probe config.ini file). If using the WLAN Pi, ensure that it is flipped in to wiperf mode. On the RPI, ensure that the required cron job has been configured to start polling.","title":"Create a HEC Token"},{"location":"splunk_configure/#perform-a-test-search","text":"After a few minutes, when the probe has run a test cycle, data should start to appear in Splunk. The quickest way to check is to do a general search for data in Splunk and see what is being received. Go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and enter a \u201c*\u201d in the \u201cNew Search\u201d text box. Results data should be seen as shown below: If your search result looks like this (no results found message), then you need to wait a little longer for data to arrive, or there is likely a comms problem between your probe and Splunk:","title":"Perform a Test Search"},{"location":"splunk_configure/#create-a-dashboard","text":"Now that we have data arriving at our Splunk server, we need to view the data in an interesting format. Splunk allows us to create a number of dashboards to visualize our data. We will now create a simple dashboard to demonstrate the visualization capabilities. In the WLANPi\u2019s /usr/share/wiperf/dashboards directory, a number of pre-canned dashboard files have been provided to allow a dashboard to be copied and pasted easily. These are also available on the GitHub page of the Wiperf project: https://github.com/wifinigel/wiperf/tree/master/dashboards Use an SFTP client to pull the \u201cprobe_summary.xml\u201d file from your probe or open the file on the GitHub page and select \u201cRaw\u201d to copy and paste the code in to a local file on your laptop. In the Splunk GUI, go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and hit the \u201cDashboards\u201d link: Hit the \"Create New Dashboard\" button: In the pop-up panel, enter a dashboard name and hit the \"Create Dashboard\" button: In the \u201cEdit Dashboard\u201d panel that opens, hit the \u201cSource\u201d button: By default, some basic XML configuration will exist in the dashboard definition: Open up the dashboard definition file previously downloaded from your probe or the GitHub site in a text editor. Then simply paste in the code as shown below (make sure the original code was all removed): After hitting the Save button, the dashboard will now be shown: Using the hostname and time period selector above the graphs, different probes and reporting periods may be viewed. The process above may be repeated using each of the xml files found in the dashboards folder to create a series of separate dashboards that focus on different aspects of data available using wiperf.","title":"Create a Dashboard"},{"location":"splunk_install/","text":"Splunk Installation Once the software is downloaded, follow the instructions that are appropriate for your OS in the Splunk installation manual (you can look for the latest guides via Google, but at the time of writing it was this manual): https://docs.splunk.com/Documentation/Splunk/latest/Installation/Chooseyourplatform The installation process for all platforms is very straightforward and is covered well in he official install guides, so will not be covered in detail here. Note When installing the Linux flavour of Splunk, make sure you do not miss the additional step required to ensure that Splunk starts after a server reboot. The following command needs to be executred after the software is installed (but please verify this isn the official installation documents): sudo /opt/splunk/bin/splunk enable boot-start Once installation has been completed, it should be possible to access the web dashboard of Splunk at the URL: http://<Splunk_server_IP>:8000","title":"Splunk Installation"},{"location":"splunk_install/#splunk-installation","text":"Once the software is downloaded, follow the instructions that are appropriate for your OS in the Splunk installation manual (you can look for the latest guides via Google, but at the time of writing it was this manual): https://docs.splunk.com/Documentation/Splunk/latest/Installation/Chooseyourplatform The installation process for all platforms is very straightforward and is covered well in he official install guides, so will not be covered in detail here. Note When installing the Linux flavour of Splunk, make sure you do not miss the additional step required to ensure that Splunk starts after a server reboot. The following command needs to be executred after the software is installed (but please verify this isn the official installation documents): sudo /opt/splunk/bin/splunk enable boot-start Once installation has been completed, it should be possible to access the web dashboard of Splunk at the URL: http://<Splunk_server_IP>:8000","title":"Splunk Installation"},{"location":"splunk_platform/","text":"Splunk Platform To collect and view the test results data, an instance of Splunk is required. Splunk is a very flexible data collection and reporting package that can take data sent by the wiperf probe and present it in a very nice report format. Splunk can be installed on a wide variety of platforms that can be viewed at : https://www.splunk.com/en_us/download/splunk-enterprise.html This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. Note that a free account sign-up is required when downloading the software from the link listed above. To install Splunk and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . The product being installed is Splunk Enterprise. This is a paid-for product, but it has a free-tier for low data volumes (500Mbytes per day). Install initially with all the licensing defaults and then drop back to the free-tier by selecting Settings > Licensing and selecting the free tier. The free tier is plenty for the low volume rates that the wiperf probe generates when deploying probes at small-scale. Attention If you forget to select the free tier and your trial license expires, you may become locked out of the GUI with a \u201clicense expired\u201d message. If this happens, from the CLI of your Splunk server, find the file \u201cserver.conf\u201d and add the following line to the bottom of the file: [license] active_group = Lite_Free Then, restart the Splunk server and the login issue should be fixed. (The file is /opt/splunk/etc/system/local/server.conf on Linux) Connectivity Planning One area to consider is connectivity between the wiperf probe and the Splunk instance. The wiperf probe needs to be able to access the Splunk server to send its data. If the wiperf probe probe is being deployed on a wireless network, how is the performance data generated going to get back to the Splunk server? If the probe is being deployed on a customer network to perform temporary monitoring, it will obviously join the wireless network under test. But how is the wiperf probe going to send its data to the Splunk server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, hence (potentially) bridging wired and wireless networks. Therefore, in many instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier. Three topology deployment options are supported: - Results data over wireless - Results data over Ethernet - Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the Splunk server. The 3 connectivity options are discussed below. Results Data Over Wireless In this topology the wiperf probe is configured to join an SSID that has the Splunk server accessible via its WLAN interface. Typically, the Splunk server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the Splunk server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: \\<public IP address of Splunk server\\> Results data over Ethernet If the Splunk server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of Splunk server> Results data over Zerotier/wireless A very simple way of getting the wiperf probe talking with your Splunk server is to use the Zerotier service to create a virtual network. In summary, both the Splunk server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same VLAN in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the Splunk server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees, It\u2019s very easy to use and get going, plus your Splunk server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the Splunk server and wiperf probe when the client is installed. Seriously, give it a go...it's quicker to try it than me explaining it here: https://www.zerotier.com/ config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of Splunk server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Splunk Platform"},{"location":"splunk_platform/#splunk-platform","text":"To collect and view the test results data, an instance of Splunk is required. Splunk is a very flexible data collection and reporting package that can take data sent by the wiperf probe and present it in a very nice report format. Splunk can be installed on a wide variety of platforms that can be viewed at : https://www.splunk.com/en_us/download/splunk-enterprise.html This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. Note that a free account sign-up is required when downloading the software from the link listed above. To install Splunk and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . The product being installed is Splunk Enterprise. This is a paid-for product, but it has a free-tier for low data volumes (500Mbytes per day). Install initially with all the licensing defaults and then drop back to the free-tier by selecting Settings > Licensing and selecting the free tier. The free tier is plenty for the low volume rates that the wiperf probe generates when deploying probes at small-scale. Attention If you forget to select the free tier and your trial license expires, you may become locked out of the GUI with a \u201clicense expired\u201d message. If this happens, from the CLI of your Splunk server, find the file \u201cserver.conf\u201d and add the following line to the bottom of the file: [license] active_group = Lite_Free Then, restart the Splunk server and the login issue should be fixed. (The file is /opt/splunk/etc/system/local/server.conf on Linux)","title":"Splunk Platform"},{"location":"splunk_platform/#connectivity-planning","text":"One area to consider is connectivity between the wiperf probe and the Splunk instance. The wiperf probe needs to be able to access the Splunk server to send its data. If the wiperf probe probe is being deployed on a wireless network, how is the performance data generated going to get back to the Splunk server? If the probe is being deployed on a customer network to perform temporary monitoring, it will obviously join the wireless network under test. But how is the wiperf probe going to send its data to the Splunk server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, hence (potentially) bridging wired and wireless networks. Therefore, in many instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier. Three topology deployment options are supported: - Results data over wireless - Results data over Ethernet - Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the Splunk server. The 3 connectivity options are discussed below.","title":"Connectivity Planning"},{"location":"splunk_platform/#results-data-over-wireless","text":"In this topology the wiperf probe is configured to join an SSID that has the Splunk server accessible via its WLAN interface. Typically, the Splunk server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the Splunk server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: \\<public IP address of Splunk server\\>","title":"Results Data Over Wireless"},{"location":"splunk_platform/#results-data-over-ethernet","text":"If the Splunk server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of Splunk server>","title":"Results data over Ethernet"},{"location":"splunk_platform/#results-data-over-zerotierwireless","text":"A very simple way of getting the wiperf probe talking with your Splunk server is to use the Zerotier service to create a virtual network. In summary, both the Splunk server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same VLAN in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the Splunk server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees, It\u2019s very easy to use and get going, plus your Splunk server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the Splunk server and wiperf probe when the client is installed. Seriously, give it a go...it's quicker to try it than me explaining it here: https://www.zerotier.com/ config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of Splunk server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Results data over Zerotier/wireless"},{"location":"splunk_software/","text":"Splunk Software To obtain the Splunk software for your data server, get along to the Splunk web site and sign up for an account if you don\u2019t already have one: https://www.splunk.com/en_us/download/splunk-enterprise.html Once you\u2019re logged in to the Splunk site, you\u2019ll have a number of OS options, so go ahead and choose your OS option ( supported platforms can be viewed here ). There are options for Windows, Linux & Mac OS: Once you've hit the download button, the Splunk Enterprise software chosen will start to download to your local machine, ready for installation. It\u2019s worth checking the download page to see if there are further download options. If you check the graphic below, you can see there is a \u201cDownload via Command Line (wget)\u201d option, which is a much easier way to get the code directly on to your server. The options you will see here will vary between OS selections:","title":"Splunk Software"},{"location":"splunk_software/#splunk-software","text":"To obtain the Splunk software for your data server, get along to the Splunk web site and sign up for an account if you don\u2019t already have one: https://www.splunk.com/en_us/download/splunk-enterprise.html Once you\u2019re logged in to the Splunk site, you\u2019ll have a number of OS options, so go ahead and choose your OS option ( supported platforms can be viewed here ). There are options for Windows, Linux & Mac OS: Once you've hit the download button, the Splunk Enterprise software chosen will start to download to your local machine, ready for installation. It\u2019s worth checking the download page to see if there are further download options. If you check the graphic below, you can see there is a \u201cDownload via Command Line (wget)\u201d option, which is a much easier way to get the code directly on to your server. The options you will see here will vary between OS selections:","title":"Splunk Software"},{"location":"troubleshooting/","text":"Troubleshooting: (In development - old version) If things seem to be going wrong, try the following: Connect to the WLAN Pi using the USB OTG connection to check log files: cat /home/wlanpi/wiperf/logs/agent.log cat /home/wlanpi/wiperf/wiperf.log SSH to the device & tail the agent log file in real-time, watching for errors and dumps of test results being performed: tail -f /home/wlanpi/wiperf/logs/agent.log Flip back in to classic mode and activate Wiperf mode from the CLI of the WLAN Pi, watching for errors: cd /home/wlanpi/wiperf sudo ./wiperf_switcher Try disabling tests & see if one specific test is causing an issue Make sure all pre-reqs have definitely been fulfilled Make sure your WLAN Pi and Splunk servers are NTP sync'ed Flip back to classic mode and re-check the edits made to the config.ini & wpa_supplicant.conf files If you have changed the WLAN Pi hostname from its default, make sure you have updated both the /etc/hosts AND the /etc/hostname file as per the instructions [here][hostname_faq] (this can cause some very weird issues!) Check the order of DNS servers being used by running the command cat /etc/resolv.conf on the CLI of the WLAN Pi when it is in wiperf mode and connected to the wireless network. If there are multiple servers shown and you see 8.8.8.8 at the top of the list, you may need to move the 8.8.8.8 entry from the file /etc/resolvconf/resolv.conf.d/head to /etc/resolvconf/resolv.conf.d/tail and then reboot. This should shift 8.8.8.8 to be bottom of the list in cat /etc/resolv.conf and may fix your name resolution issues","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting-in-development-old-version","text":"If things seem to be going wrong, try the following: Connect to the WLAN Pi using the USB OTG connection to check log files: cat /home/wlanpi/wiperf/logs/agent.log cat /home/wlanpi/wiperf/wiperf.log SSH to the device & tail the agent log file in real-time, watching for errors and dumps of test results being performed: tail -f /home/wlanpi/wiperf/logs/agent.log Flip back in to classic mode and activate Wiperf mode from the CLI of the WLAN Pi, watching for errors: cd /home/wlanpi/wiperf sudo ./wiperf_switcher Try disabling tests & see if one specific test is causing an issue Make sure all pre-reqs have definitely been fulfilled Make sure your WLAN Pi and Splunk servers are NTP sync'ed Flip back to classic mode and re-check the edits made to the config.ini & wpa_supplicant.conf files If you have changed the WLAN Pi hostname from its default, make sure you have updated both the /etc/hosts AND the /etc/hostname file as per the instructions [here][hostname_faq] (this can cause some very weird issues!) Check the order of DNS servers being used by running the command cat /etc/resolv.conf on the CLI of the WLAN Pi when it is in wiperf mode and connected to the wireless network. If there are multiple servers shown and you see 8.8.8.8 at the top of the list, you may need to move the 8.8.8.8 entry from the file /etc/resolvconf/resolv.conf.d/head to /etc/resolvconf/resolv.conf.d/tail and then reboot. This should shift 8.8.8.8 to be bottom of the list in cat /etc/resolv.conf and may fix your name resolution issues","title":"Troubleshooting:  (In development - old version)"}]}