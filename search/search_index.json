{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"wiperf: An Open Source UX Performance Probe Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana Data Server The core focus of this project is the probe platform that gathers the network performance data in which we are interested. However, the data server is a critical component that allows visualization of that performance data. High-level configuration details will be provided to \"get you going\", but detailed information about the operation of these platforms is beyond the scope f this project. Both of the data servers supported are \"NoSQL\" servers, which means that no data structures have to be pre-defined in database tables. This means we can send our data structures, that contain network performance data, to the server with very little set-up compared to traditional database servers. As long as we have a valid set of credentials for the data server, we can just send a data in JSON data structures over HTTPS in whatever structure we choose. A database query language on the data server allows us to retrieve and graph the data collected by the wiperf probe. Splunk Splunk is supported on all popular operating systems and is very easy to set up on your server of choice. It acts as both the data store and visualization platform. Splunk is a commercial, rather than open-source, product. The volume of data returned by the probe is very low, so the free tier of Splunk may be used to gather and report on data. For details on how to set up a Splunk server, visit this document: [Splunk build guide][splunk_build] (it's a lot easier than you might expect...honestly) Splunk product web site: https://www.splunk.com/ InfluxDB/Grafana Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data server from which to pull its network performance data. To meet this requirement, the Influx database server is used. Like Grafana, InfluxDB is also an open-source package. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana web site (v6.7): https://grafana.com/ Influx web site (v.1.8): https://www.influxdata.com Workflow to Setup Wiperf The workflow to get Wiperf fully operational consists if a number of steps that break down in to two main areas: Probe setup (the RPi or WLAN Pi) Data server setup (the Splunk or Influx/Grafana server) The Data server setup tends to be a task that needs completion only once (or at least very infrequently). Conversely, some or all of the probe setup will need to be completed each time a probe is deployed - this is mainly due to the fact that in each environment in which it is deployed, the connectivity for the probe will vary (e.g. different SSID, different network connection type). Here is an overview of the workflow, with links to documentation for each step: Data server setup: Prepare a server platform Obtain the data server application software Install the data server application Configure the data server application Probe setup: Obtain a probe platform (Raspberry Pi or WLAN Pi) Prepare the platform for the wiperf software Install the wiperf software Configure the wiperf software Deploy & test the wiperf probe In addition to the setup and deployment of the components, there may also be a requirement to troubleshoot the setup. This may involve: Troubleshooting steps Review known issues Further Documentation References Configuration file parameters Data points sent by the probe to the data server platform ![Speedtest Report][speedtest_image] Credits Thanks to Kristian Roberts for his invaluable input, testing and guidance on Splunk. He kicked this whole thing off and it definitely wouldn't have happened without him. A top bloke. Caveats This free software is provided for you to use at your own risk. There are no guarantees around its operation, suitability or the data that it provides. Please consult the [license file][license] shipped with this software. Developer Nigel Bowden (WifiNigel): https://twitter.com/wifinigel","title":"Home"},{"location":"#wiperf-an-open-source-ux-performance-probe","text":"Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"wiperf: An Open Source UX Performance Probe"},{"location":"#data-server","text":"The core focus of this project is the probe platform that gathers the network performance data in which we are interested. However, the data server is a critical component that allows visualization of that performance data. High-level configuration details will be provided to \"get you going\", but detailed information about the operation of these platforms is beyond the scope f this project. Both of the data servers supported are \"NoSQL\" servers, which means that no data structures have to be pre-defined in database tables. This means we can send our data structures, that contain network performance data, to the server with very little set-up compared to traditional database servers. As long as we have a valid set of credentials for the data server, we can just send a data in JSON data structures over HTTPS in whatever structure we choose. A database query language on the data server allows us to retrieve and graph the data collected by the wiperf probe.","title":"Data Server"},{"location":"#splunk","text":"Splunk is supported on all popular operating systems and is very easy to set up on your server of choice. It acts as both the data store and visualization platform. Splunk is a commercial, rather than open-source, product. The volume of data returned by the probe is very low, so the free tier of Splunk may be used to gather and report on data. For details on how to set up a Splunk server, visit this document: [Splunk build guide][splunk_build] (it's a lot easier than you might expect...honestly) Splunk product web site: https://www.splunk.com/","title":"Splunk"},{"location":"#influxdbgrafana","text":"Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data server from which to pull its network performance data. To meet this requirement, the Influx database server is used. Like Grafana, InfluxDB is also an open-source package. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana web site (v6.7): https://grafana.com/ Influx web site (v.1.8): https://www.influxdata.com","title":"InfluxDB/Grafana"},{"location":"#workflow-to-setup-wiperf","text":"The workflow to get Wiperf fully operational consists if a number of steps that break down in to two main areas: Probe setup (the RPi or WLAN Pi) Data server setup (the Splunk or Influx/Grafana server) The Data server setup tends to be a task that needs completion only once (or at least very infrequently). Conversely, some or all of the probe setup will need to be completed each time a probe is deployed - this is mainly due to the fact that in each environment in which it is deployed, the connectivity for the probe will vary (e.g. different SSID, different network connection type). Here is an overview of the workflow, with links to documentation for each step: Data server setup: Prepare a server platform Obtain the data server application software Install the data server application Configure the data server application Probe setup: Obtain a probe platform (Raspberry Pi or WLAN Pi) Prepare the platform for the wiperf software Install the wiperf software Configure the wiperf software Deploy & test the wiperf probe In addition to the setup and deployment of the components, there may also be a requirement to troubleshoot the setup. This may involve: Troubleshooting steps Review known issues","title":"Workflow to Setup Wiperf"},{"location":"#further-documentation-references","text":"Configuration file parameters Data points sent by the probe to the data server platform ![Speedtest Report][speedtest_image]","title":"Further Documentation References"},{"location":"#credits","text":"Thanks to Kristian Roberts for his invaluable input, testing and guidance on Splunk. He kicked this whole thing off and it definitely wouldn't have happened without him. A top bloke.","title":"Credits"},{"location":"#caveats","text":"This free software is provided for you to use at your own risk. There are no guarantees around its operation, suitability or the data that it provides. Please consult the [license file][license] shipped with this software.","title":"Caveats"},{"location":"#developer","text":"Nigel Bowden (WifiNigel): https://twitter.com/wifinigel","title":"Developer"},{"location":"about/","text":"About Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"About"},{"location":"about/#about","text":"Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"About"},{"location":"config.ini/","text":"Wiperf - config.ini reference guide Background The config.ini file controls the operation of the Wiperf utility. It has many options available for maximum flexiblity, but some may need some clarification. Many options will be fine using the defaults that are supplied with the installed package. However, some will definitely require configuration as they may require values such as IP addresses and port numbers which will vary in each instance where Wiperf is used. The config.ini file is located in the directory : /home/wlanpi/wiperf The file is organised in a number of sections that relate to different areas of operation. Each section begins with a name enclosed is square brackets, like this: [Speedtest] Within each section are a number of configurable parameters that are in the format: parameter: value You may also see some lines that begin with a semi-colon. These are comments and have no effect on the operation of Wiperf. You may add, remove or change these as you wish. Here is an example comment: ; wlan interface name set this as per the output of an iwconfig command (usually wlan0) Parameter Reference Guide We'll take a look at each section of the config file and provide some guidance on suitable parameter values: General Section wlan_if mgt_if platform exporter_type splunk_host splunk_port splunk_token influx_host influx_port influx_username influx_password influx_database influx2_host influx2_port influx2_token influx2_bucket influx2_org test_interval test_offset connectivity_lookup location data_format data_dir data_transport debug cfg_url cfg_username cfg_password cfg_token cfg_refresh_interval unit_bouncer Network_Test Section network_data_file Speetest Section enabled server_id http_proxy https_proxy no_proxy speedtest_data_file Ping_Test Section enabled ping_host1 ping_host2 ping_host3 ping_host4 ping_host5 ping_count ping_data_file Iperf3_tcp_test Section enabled server_hostname port duration iperf3_tcp_data_file Iperf3_udp_test Section enabled server_hostname port duration bandwidth iperf3_udp_data_file DNS_test Section enabled dns_target1 dns_target2 dns_target3 dns_target4 dns_target5 dns_data_file HTTP_test Section enabled http_target1 http_target2 http_target3 http_target4 http_target5 http_data_file DHCP_test Section enabled mode dhcp_data_file [General] Section Note: any changes to this section on the WLANPi should only be made when it is running in classic mode (not while in Wiperf mode). wlan_if This parameter contains the name of the WLAN interface on the Pi. This will almost always be 'wlan0', but is provided in case of new use-cases in the future. You can see the WLAN interface name by running the 'ifconfig' command from the CLI of the Pi Default setting: wlan_if: wlan0 top mgt_if When performance tests have been completed, the results data needs to be sent to a reporting server (e.g. Splunk/InfluxDb). This parameter configures the interface over which this management traffic needs to be sent. Getting this parameter correct for your environment is very important to ensure that test results data makes it back to your reporting server. The available options are: wlan0 (the first available WLAN port - usually a USB dongle plugged in to the WLANPi) eth0 (the internal Ethernet port of the WLANPi) zt (Zerotier (the virtual network service) is installed and used to connect to the reporting server) The WANPi is configured to assign a higher cost default route to eth0 by default so that all traffic (tests & test results) will choose the default route provided by wlan0. If eth0 is used as the path to return test results to the reporting server, then a static route is injected in to the WLANPi route table on start-up to ensure correct routing. If this parameter is not correctly set, then results data may not make it back to the reporting server. Default setting: mgt_if: wlan0 top platform Wiperf is supported on both the WLANPi and Raspberry Pi platforms. The available options are: wlanpi rpi Default setting: platform: wlanpi top exporter_type Wiperf supports a number of remote data repositories that can be used as targets to store test result data. The available options are: splunk influxdb influxdb2 Default setting: exporter_type: splunk top splunk_host This is the hostname or IP address of the Splunk platform where test result data is sent to. If the hostname of the Splunk server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): splunk_host: top splunk_port The network port used to send updates to the Splunk server. By default this is 8088, but this may be changed within the Splunk application if an alternative port is required for your environment Default setting: splunk_port: 8088 top splunk_token Splunk will only receive HEC updates from devices that are authorised to send it data. Splunk uses tokens to decide if an update is from a valid device. To view available (or create) tokens within Splunk, view the menu option: \"Settings > Data > Data Inputs > HTTP Event Collector\" Here is example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d Default setting (none): splunk_token: top influx_host This is the hostname or IP address of the Influx (v1.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): influx_host: top influx_port The network port used to send updates to the Influx (v1.x) server. By default this is 8086, but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx_port: 8086 top influx_username The username that will be used to access the Influx (v1.x) server DB to post results data. This username must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_username: top influx_password The password that will be used to access the Influx (v1.x) server DB to post results data. This password must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_password: top influx_database The name of the database on the Influx (v1.x) server DB where wiperf will post results data. This database must have been created on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_database: top influx2_host This is the hostname or IP address of the Influx (v2.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your InfluxDb2 server in the Zerotier dashboard for your network) Default setting (none): influx2_host: top influx2_port The network port used to send updates to the Influx (v2.x) server. By default this is 443 (this assumes the cloud service is used), but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx2_port: 443 top influx2_token InfluxDB2 allows the use of authentication tokens when sending results data to the InfluxDB2 server. This provides an easier authentication methods than using a username and password. Once a token has been created on InfluxDB server, it can be used by wiperf to authenticate the results data sent to the InfluxDB2 server Default setting (none): influx2_token: top influx2_bucket Data sent to the InfluxDB2 server from wiperf is stored in a \"bucket\" in the data store. This field is used to configure the bucket to which wiperf should send it's data. Default setting (none): influx2_bucket: top influx2_org The InfluxDB2 server can be partitioned in to a number of organizations, which contain the buckets where data will be stored. Use this field to configure wiperf to send data to the correct organisation on InfluxDB2. Default setting (none): influx2_org: top test_interval (WLANPi only) This is the interval (in minutes) at which we would like to run the performance tests. The recommened minimum is 5, which is also the default. (Note: if this setting is too low, scheduled tests may try to run before the previous test sequence has completed, which could cause gaps in your data) Default setting: test_interval: 5 top test_offset (WLANPi only) By default test run at the interval specified by the test_interval parameter, which is referenced to the to of the hours (e.g. 5 mins interval will run at 5, 10, 15, 20, 25...etc. mins past the hour). If multiple proes are running, it mau be useful to stagger their start times. By setting test_offset to a value of one, this will offset all test start times by 1 minutes (i.e. 6,11,16,21,26...etc. mins past the hour) The default value is zero which means that the default 5,10,15,20... run pattern will be used. Default setting: test_offset: 0 top connectivity_lookup At the start of each test cycle, a DNS lookup is performed to ensure that DNS is working. By default this is 'google.com' (this was 'bbc.co.uk' on older versions of wiperf). This may be set to any required hostname lookup in instances when the default site may not be available for some reason (e.g. DNS restrictions due to filtering or lack of Internet access) Default setting: connectivity_lookup: google.com top location This is a string that can be added to assist with report filtering, if required. Its default value in an empty string. It could be be used in an expression within your reports to filter units based on a location field (for instance) Default setting: location: top data_format (Not currently operational) Wiperf has the capability to output data in a number of formats. The current options are: csv or json However this field is not currently used, as selecting the 'hec' transport mode (the only supported transport currently) over-rides this field. The value in this filed is currently irrelevant, but it s recommended to leave it at the default setting of json Default setting: data_format: json top data_dir This is the directory on the WLANPi/RPi where test result data is dumped. Do not change this value from the default . This field is provided for future configuration options if required. Default setting: data_dir: /home/wlanpi/wiperf/data top data_transport The currently supported data transport mode is hec . This is the HTTP Event Collector supported natively within the Splunk server. Other transport modes will be suported in the future, but currently this should be left at the default setting of `hec . (Note: the transport method forwarder is also a valid transport method which provides support for very early versions of this code which used the Splunk Univeral Forwarder. Use of this method is deprecated and will be removed in the near future. Anyone still using the UF should move to using hec ASAP) Default setting: data_transport: hec top debug To enable enhanced logging in the agent.log file, change this setting to \"on\" Default setting: debug: off top cfg_url If using centralized configuration file retrieval, this field specifies the full URL of the config file on the remote repo. (Note that on GitHub this is the URL of the raw file itself) If this field is not set, then centralized configuration retrieval is disabled Default setting (none): cfg_url: top cfg_username If username/pasword credentials are used to retrieve the centralized config, this field specifies the usename to be used. Default setting (none): cfg_username: top cfg_password If username/pasword credentials are used to retrieve the centralized config, this field specifies the password to be used. Default setting (none): cfg_password: top cfg_token If a GitHub authentication token is used to retrieve the centralized config, this field specifies the token to be used. (Note: this is used instead of a username/pwd) Default setting (none): cfg_token: top cfg_refresh_interval This field specifies how often (in seconds) the centralized config file should be retrieved . Recommended value: 900 (i.e. 15 mins) Default setting (none): cfg_refresh_interval: top unit_bouncer If you need to bounce (reboot) the unit for some reason on a regular basis, this field can be used to signal to the WLAN Pi each hour at which it must reboot. The field is a comma separated string that lists the hours at which the unit must reboot (in 24-hour format). The number-format and comma separation are important to get right! Note that the reboot is not exactly on the hour, but will occur at the end of the next test cycle that it is within the hour where a reboot is required. It will only happen once per hour. Example: the following config will reboot at midnight, 04:00, 08:00, 12:00, 16:00: unit_bouncer: 00, 06, 12, 18 This parameter is commented out by default as it is obviously not something you necessarilly want to switch on accidentally. Default setting: ; unit_bouncer: 00, 06, 12, 18 top [Network_Test] Section network_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for network tests in remote data repositories (e.g. Splunk, InfluxDB) Default setting: network_data_file: wiperf-network top [Speedtest] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no Speedtest is run. When enabled, a speedtest to the Ookla speedtest service is run each test cycle. Default setting: enabled: yes top server_id If you wish to specify a particular Ookla speedtest server that the test needs to be run against, you can enter its ID here. This must be the (numeric) server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If no value is specified, best server is used (default) Default setting: server_id: top http_proxy https_proxy no_proxy If proxy server access is required to run a speedtest, enter the proxy server details here for https & https e.g. https_proxy: http://10.1.1.1:8080 For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) e.g. no_proxy: \"mail.local, intranet.local\" Default settings: http_proxy: https_proxy: no_proxy: top speedtest_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for Speedtests in the reporting server (e.g. Splunk/InfluxDB) Default setting: speedtest_data_file: wiperf-speedtest top [Ping_Test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no ping tests are run. When enabled, up to 5 entries will be targetted with an ICMP ping and the RRT times recorded Default setting: enabled: yes top ping_host1 IP address or hostname of first ping target. No target details = no test run Default setting: ping_host1: bbc.co.uk top ping_host2 IP address or hostname of second ping target. No target details = no test run Default setting: ping_host2: cisco.com top ping_host3 IP address or hostname of third ping target. No target details = no test run Default setting: ping_host3: google.com top ping_host4 IP address or hostname of fourth ping target. No target details = no test run Default setting: ping_host4: top ping_host5 IP address or hostname of fifth ping target. No target details = no test run Default setting: ping_host2: top ping_count The number of pings to send for each ping target Default setting: ping_count: 10 top ping_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for ping tests in the reporting server (e.g. Splunk.InfuxDB) Default setting: ping_data_file: wiperf-ping top [Iperf3_tcp_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no tcp iperf3 test is run. When enabled, a tcp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs) Default setting: enabled: yes top server_hostname The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: 192.168.0.14 top port The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top duration The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 20 top iperf3_tcp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for tcp iperf3 tests in Splunk Default setting: iperf3_tcp_data_file: wiperf-iperf3-tcp top [Iperf3_udp_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no udp iperf3 test is run. When enabled, a udp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs), attempting to achieve a data transfer rate of bandwidth bps. Default setting: enabled: yes top server_hostname The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: 192.168.0.14 top port The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top duration The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 20 top bandwidth The data rate that will be attempted for the UDP iperf3 test in bps Default setting: bandwidth: 20000000 top iperf3_udp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for udp iperf3 tests in Splunk Default setting: iperf3_udp_data_file: wiperf-iperf3-udp top [DNS_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no DNS tests are run. When enabled, DNS tests are run for each of the dns_target paramters defined in this section. Any targets that have no value entered will be ignored. Default setting: enabled: yes top dns_target1 Hostname of first DNS target. No target details = no test run Default setting: dns_target1: bbc.co.uk top dns_target2 Hostname of second DNS target. No target details = no test run Default setting: dns_target2: cisco.com top dns_target3 Hostname of third DNS target. No target details = no test run Default setting: dns_target3: google.com top dns_target4 Hostname of fourth DNS target. No target details = no test run Default setting: dns_target4: top dns_target5 Hostname of fifth DNS target. No target details = no test run Default setting: dns_target5: top dns_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DNS tests in Splunk Default setting: dns_data_file: wiperf-dns top [HTTP_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no HTTP tests are run. When enabled, HTTP tests are run for each of the http_target paramters defined in this section. Any targets that have no value entered will be ignored. Targets must include the full url of each site to be queried (including http:// or https:// element). Valid site address examples: http://bbc.co.uk https://ebay.com A http get will be performed for each target and the result code returned. Default setting: enabled: yes top http_target1 Hostname of first HTTP target. No target details = no test run Default setting: http_target1: https://ebay.co.uk top http_target2 Hostname of second HTTP target. No target details = no test run Default setting: http_target2: http://twitter.com top http_target3 Hostname of third HTTP target. No target details = no test run Default setting: http_target3: https://facebook.com top http_target4 Hostname of fourth HTTP target. No target details = no test run Default setting: http_target4: https://instagram.com top http_target5 Hostname of fifth HTTP target. No target details = no test run Default setting: https://amazon.com http_target5: top http_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for HTTP tests in Splunk Default setting: http_data_file: wiperf-http top [DHCP_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi) enabled Options: yes or no. If set to no, entire section is ignored and no DHCP test is run. Note that the DHCP test has 2 modes : passive: only a renewal request is sent (no release of IP) active: a release and renew request is performed. Note that the active setting has shown varying degrees of usefulness in esting. In some scenarios (e.g. when connected via ZeroTier), it has caused connectivity issues, hence the passive setting is a better choice. Obviously, the passive setting does not perform such a rigorous DHCP test and is completed much quicker than the active mode. However, it still provides a useful comparative measure of the reponsivemess of DHCP servers. Default setting: enabled: yes top mode Available options: passive active The active settings performs a full release/request and may be disruptve to connectivity - use with caution. The passive setting is the recommended option for most situations. Default setting: mode: passive top dhcp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DHCP tests in Splunk Default setting: dhcp_data_file: wiperf-dhcp top","title":"Wiperf - config.ini reference guide"},{"location":"config.ini/#wiperf-configini-reference-guide","text":"","title":"Wiperf - config.ini reference guide"},{"location":"config.ini/#background","text":"The config.ini file controls the operation of the Wiperf utility. It has many options available for maximum flexiblity, but some may need some clarification. Many options will be fine using the defaults that are supplied with the installed package. However, some will definitely require configuration as they may require values such as IP addresses and port numbers which will vary in each instance where Wiperf is used. The config.ini file is located in the directory : /home/wlanpi/wiperf The file is organised in a number of sections that relate to different areas of operation. Each section begins with a name enclosed is square brackets, like this: [Speedtest] Within each section are a number of configurable parameters that are in the format: parameter: value You may also see some lines that begin with a semi-colon. These are comments and have no effect on the operation of Wiperf. You may add, remove or change these as you wish. Here is an example comment: ; wlan interface name set this as per the output of an iwconfig command (usually wlan0)","title":"Background"},{"location":"config.ini/#parameter-reference-guide","text":"We'll take a look at each section of the config file and provide some guidance on suitable parameter values: General Section wlan_if mgt_if platform exporter_type splunk_host splunk_port splunk_token influx_host influx_port influx_username influx_password influx_database influx2_host influx2_port influx2_token influx2_bucket influx2_org test_interval test_offset connectivity_lookup location data_format data_dir data_transport debug cfg_url cfg_username cfg_password cfg_token cfg_refresh_interval unit_bouncer Network_Test Section network_data_file Speetest Section enabled server_id http_proxy https_proxy no_proxy speedtest_data_file Ping_Test Section enabled ping_host1 ping_host2 ping_host3 ping_host4 ping_host5 ping_count ping_data_file Iperf3_tcp_test Section enabled server_hostname port duration iperf3_tcp_data_file Iperf3_udp_test Section enabled server_hostname port duration bandwidth iperf3_udp_data_file DNS_test Section enabled dns_target1 dns_target2 dns_target3 dns_target4 dns_target5 dns_data_file HTTP_test Section enabled http_target1 http_target2 http_target3 http_target4 http_target5 http_data_file DHCP_test Section enabled mode dhcp_data_file","title":"Parameter Reference Guide"},{"location":"config.ini/#general-section","text":"Note: any changes to this section on the WLANPi should only be made when it is running in classic mode (not while in Wiperf mode).","title":"[General] Section"},{"location":"config.ini/#wlan_if","text":"This parameter contains the name of the WLAN interface on the Pi. This will almost always be 'wlan0', but is provided in case of new use-cases in the future. You can see the WLAN interface name by running the 'ifconfig' command from the CLI of the Pi Default setting: wlan_if: wlan0 top","title":"wlan_if"},{"location":"config.ini/#mgt_if","text":"When performance tests have been completed, the results data needs to be sent to a reporting server (e.g. Splunk/InfluxDb). This parameter configures the interface over which this management traffic needs to be sent. Getting this parameter correct for your environment is very important to ensure that test results data makes it back to your reporting server. The available options are: wlan0 (the first available WLAN port - usually a USB dongle plugged in to the WLANPi) eth0 (the internal Ethernet port of the WLANPi) zt (Zerotier (the virtual network service) is installed and used to connect to the reporting server) The WANPi is configured to assign a higher cost default route to eth0 by default so that all traffic (tests & test results) will choose the default route provided by wlan0. If eth0 is used as the path to return test results to the reporting server, then a static route is injected in to the WLANPi route table on start-up to ensure correct routing. If this parameter is not correctly set, then results data may not make it back to the reporting server. Default setting: mgt_if: wlan0 top","title":"mgt_if"},{"location":"config.ini/#platform","text":"Wiperf is supported on both the WLANPi and Raspberry Pi platforms. The available options are: wlanpi rpi Default setting: platform: wlanpi top","title":"platform"},{"location":"config.ini/#exporter_type","text":"Wiperf supports a number of remote data repositories that can be used as targets to store test result data. The available options are: splunk influxdb influxdb2 Default setting: exporter_type: splunk top","title":"exporter_type"},{"location":"config.ini/#splunk_host","text":"This is the hostname or IP address of the Splunk platform where test result data is sent to. If the hostname of the Splunk server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): splunk_host: top","title":"splunk_host"},{"location":"config.ini/#splunk_port","text":"The network port used to send updates to the Splunk server. By default this is 8088, but this may be changed within the Splunk application if an alternative port is required for your environment Default setting: splunk_port: 8088 top","title":"splunk_port"},{"location":"config.ini/#splunk_token","text":"Splunk will only receive HEC updates from devices that are authorised to send it data. Splunk uses tokens to decide if an update is from a valid device. To view available (or create) tokens within Splunk, view the menu option: \"Settings > Data > Data Inputs > HTTP Event Collector\" Here is example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d Default setting (none): splunk_token: top","title":"splunk_token"},{"location":"config.ini/#influx_host","text":"This is the hostname or IP address of the Influx (v1.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): influx_host: top","title":"influx_host"},{"location":"config.ini/#influx_port","text":"The network port used to send updates to the Influx (v1.x) server. By default this is 8086, but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx_port: 8086 top","title":"influx_port"},{"location":"config.ini/#influx_username","text":"The username that will be used to access the Influx (v1.x) server DB to post results data. This username must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_username: top","title":"influx_username"},{"location":"config.ini/#influx_password","text":"The password that will be used to access the Influx (v1.x) server DB to post results data. This password must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_password: top","title":"influx_password"},{"location":"config.ini/#influx_database","text":"The name of the database on the Influx (v1.x) server DB where wiperf will post results data. This database must have been created on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_database: top","title":"influx_database"},{"location":"config.ini/#influx2_host","text":"This is the hostname or IP address of the Influx (v2.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the WLANPi. (Note: If using Zerotier, make sure this is the address of the IP assigned to your InfluxDb2 server in the Zerotier dashboard for your network) Default setting (none): influx2_host: top","title":"influx2_host"},{"location":"config.ini/#influx2_port","text":"The network port used to send updates to the Influx (v2.x) server. By default this is 443 (this assumes the cloud service is used), but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx2_port: 443 top","title":"influx2_port"},{"location":"config.ini/#influx2_token","text":"InfluxDB2 allows the use of authentication tokens when sending results data to the InfluxDB2 server. This provides an easier authentication methods than using a username and password. Once a token has been created on InfluxDB server, it can be used by wiperf to authenticate the results data sent to the InfluxDB2 server Default setting (none): influx2_token: top","title":"influx2_token"},{"location":"config.ini/#influx2_bucket","text":"Data sent to the InfluxDB2 server from wiperf is stored in a \"bucket\" in the data store. This field is used to configure the bucket to which wiperf should send it's data. Default setting (none): influx2_bucket: top","title":"influx2_bucket"},{"location":"config.ini/#influx2_org","text":"The InfluxDB2 server can be partitioned in to a number of organizations, which contain the buckets where data will be stored. Use this field to configure wiperf to send data to the correct organisation on InfluxDB2. Default setting (none): influx2_org: top","title":"influx2_org"},{"location":"config.ini/#test_interval","text":"(WLANPi only) This is the interval (in minutes) at which we would like to run the performance tests. The recommened minimum is 5, which is also the default. (Note: if this setting is too low, scheduled tests may try to run before the previous test sequence has completed, which could cause gaps in your data) Default setting: test_interval: 5 top","title":"test_interval"},{"location":"config.ini/#test_offset","text":"(WLANPi only) By default test run at the interval specified by the test_interval parameter, which is referenced to the to of the hours (e.g. 5 mins interval will run at 5, 10, 15, 20, 25...etc. mins past the hour). If multiple proes are running, it mau be useful to stagger their start times. By setting test_offset to a value of one, this will offset all test start times by 1 minutes (i.e. 6,11,16,21,26...etc. mins past the hour) The default value is zero which means that the default 5,10,15,20... run pattern will be used. Default setting: test_offset: 0 top","title":"test_offset"},{"location":"config.ini/#connectivity_lookup","text":"At the start of each test cycle, a DNS lookup is performed to ensure that DNS is working. By default this is 'google.com' (this was 'bbc.co.uk' on older versions of wiperf). This may be set to any required hostname lookup in instances when the default site may not be available for some reason (e.g. DNS restrictions due to filtering or lack of Internet access) Default setting: connectivity_lookup: google.com top","title":"connectivity_lookup"},{"location":"config.ini/#location","text":"This is a string that can be added to assist with report filtering, if required. Its default value in an empty string. It could be be used in an expression within your reports to filter units based on a location field (for instance) Default setting: location: top","title":"location"},{"location":"config.ini/#data_format","text":"(Not currently operational) Wiperf has the capability to output data in a number of formats. The current options are: csv or json However this field is not currently used, as selecting the 'hec' transport mode (the only supported transport currently) over-rides this field. The value in this filed is currently irrelevant, but it s recommended to leave it at the default setting of json Default setting: data_format: json top","title":"data_format"},{"location":"config.ini/#data_dir","text":"This is the directory on the WLANPi/RPi where test result data is dumped. Do not change this value from the default . This field is provided for future configuration options if required. Default setting: data_dir: /home/wlanpi/wiperf/data top","title":"data_dir"},{"location":"config.ini/#data_transport","text":"The currently supported data transport mode is hec . This is the HTTP Event Collector supported natively within the Splunk server. Other transport modes will be suported in the future, but currently this should be left at the default setting of `hec . (Note: the transport method forwarder is also a valid transport method which provides support for very early versions of this code which used the Splunk Univeral Forwarder. Use of this method is deprecated and will be removed in the near future. Anyone still using the UF should move to using hec ASAP) Default setting: data_transport: hec top","title":"data_transport"},{"location":"config.ini/#debug","text":"To enable enhanced logging in the agent.log file, change this setting to \"on\" Default setting: debug: off top","title":"debug"},{"location":"config.ini/#cfg_url","text":"If using centralized configuration file retrieval, this field specifies the full URL of the config file on the remote repo. (Note that on GitHub this is the URL of the raw file itself) If this field is not set, then centralized configuration retrieval is disabled Default setting (none): cfg_url: top","title":"cfg_url"},{"location":"config.ini/#cfg_username","text":"If username/pasword credentials are used to retrieve the centralized config, this field specifies the usename to be used. Default setting (none): cfg_username: top","title":"cfg_username"},{"location":"config.ini/#cfg_password","text":"If username/pasword credentials are used to retrieve the centralized config, this field specifies the password to be used. Default setting (none): cfg_password: top","title":"cfg_password"},{"location":"config.ini/#cfg_token","text":"If a GitHub authentication token is used to retrieve the centralized config, this field specifies the token to be used. (Note: this is used instead of a username/pwd) Default setting (none): cfg_token: top","title":"cfg_token"},{"location":"config.ini/#cfg_refresh_interval","text":"This field specifies how often (in seconds) the centralized config file should be retrieved . Recommended value: 900 (i.e. 15 mins) Default setting (none): cfg_refresh_interval: top","title":"cfg_refresh_interval"},{"location":"config.ini/#unit_bouncer","text":"If you need to bounce (reboot) the unit for some reason on a regular basis, this field can be used to signal to the WLAN Pi each hour at which it must reboot. The field is a comma separated string that lists the hours at which the unit must reboot (in 24-hour format). The number-format and comma separation are important to get right! Note that the reboot is not exactly on the hour, but will occur at the end of the next test cycle that it is within the hour where a reboot is required. It will only happen once per hour. Example: the following config will reboot at midnight, 04:00, 08:00, 12:00, 16:00: unit_bouncer: 00, 06, 12, 18 This parameter is commented out by default as it is obviously not something you necessarilly want to switch on accidentally. Default setting: ; unit_bouncer: 00, 06, 12, 18 top","title":"unit_bouncer"},{"location":"config.ini/#network_test-section","text":"","title":"[Network_Test] Section"},{"location":"config.ini/#network_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for network tests in remote data repositories (e.g. Splunk, InfluxDB) Default setting: network_data_file: wiperf-network top","title":"network_data_file"},{"location":"config.ini/#speedtest-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[Speedtest] Section"},{"location":"config.ini/#enabled","text":"Options: yes or no. If set to no, entire section is ignored and no Speedtest is run. When enabled, a speedtest to the Ookla speedtest service is run each test cycle. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_id","text":"If you wish to specify a particular Ookla speedtest server that the test needs to be run against, you can enter its ID here. This must be the (numeric) server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If no value is specified, best server is used (default) Default setting: server_id: top","title":"server_id"},{"location":"config.ini/#http_proxy","text":"","title":"http_proxy"},{"location":"config.ini/#https_proxy","text":"","title":"https_proxy"},{"location":"config.ini/#no_proxy","text":"If proxy server access is required to run a speedtest, enter the proxy server details here for https & https e.g. https_proxy: http://10.1.1.1:8080 For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) e.g. no_proxy: \"mail.local, intranet.local\" Default settings: http_proxy: https_proxy: no_proxy: top","title":"no_proxy"},{"location":"config.ini/#speedtest_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for Speedtests in the reporting server (e.g. Splunk/InfluxDB) Default setting: speedtest_data_file: wiperf-speedtest top","title":"speedtest_data_file"},{"location":"config.ini/#ping_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[Ping_Test] Section"},{"location":"config.ini/#enabled_1","text":"Options: yes or no. If set to no, entire section is ignored and no ping tests are run. When enabled, up to 5 entries will be targetted with an ICMP ping and the RRT times recorded Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#ping_host1","text":"IP address or hostname of first ping target. No target details = no test run Default setting: ping_host1: bbc.co.uk top","title":"ping_host1"},{"location":"config.ini/#ping_host2","text":"IP address or hostname of second ping target. No target details = no test run Default setting: ping_host2: cisco.com top","title":"ping_host2"},{"location":"config.ini/#ping_host3","text":"IP address or hostname of third ping target. No target details = no test run Default setting: ping_host3: google.com top","title":"ping_host3"},{"location":"config.ini/#ping_host4","text":"IP address or hostname of fourth ping target. No target details = no test run Default setting: ping_host4: top","title":"ping_host4"},{"location":"config.ini/#ping_host5","text":"IP address or hostname of fifth ping target. No target details = no test run Default setting: ping_host2: top","title":"ping_host5"},{"location":"config.ini/#ping_count","text":"The number of pings to send for each ping target Default setting: ping_count: 10 top","title":"ping_count"},{"location":"config.ini/#ping_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for ping tests in the reporting server (e.g. Splunk.InfuxDB) Default setting: ping_data_file: wiperf-ping top","title":"ping_data_file"},{"location":"config.ini/#iperf3_tcp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[Iperf3_tcp_test] Section"},{"location":"config.ini/#enabled_2","text":"Options: yes or no. If set to no, entire section is ignored and no tcp iperf3 test is run. When enabled, a tcp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs) Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_hostname","text":"The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: 192.168.0.14 top","title":"server_hostname"},{"location":"config.ini/#port","text":"The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top","title":"port"},{"location":"config.ini/#duration","text":"The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 20 top","title":"duration"},{"location":"config.ini/#iperf3_tcp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for tcp iperf3 tests in Splunk Default setting: iperf3_tcp_data_file: wiperf-iperf3-tcp top","title":"iperf3_tcp_data_file"},{"location":"config.ini/#iperf3_udp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[Iperf3_udp_test] Section"},{"location":"config.ini/#enabled_3","text":"Options: yes or no. If set to no, entire section is ignored and no udp iperf3 test is run. When enabled, a udp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs), attempting to achieve a data transfer rate of bandwidth bps. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_hostname_1","text":"The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: 192.168.0.14 top","title":"server_hostname"},{"location":"config.ini/#port_1","text":"The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top","title":"port"},{"location":"config.ini/#duration_1","text":"The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 20 top","title":"duration"},{"location":"config.ini/#bandwidth","text":"The data rate that will be attempted for the UDP iperf3 test in bps Default setting: bandwidth: 20000000 top","title":"bandwidth"},{"location":"config.ini/#iperf3_udp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for udp iperf3 tests in Splunk Default setting: iperf3_udp_data_file: wiperf-iperf3-udp top","title":"iperf3_udp_data_file"},{"location":"config.ini/#dns_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[DNS_test] Section"},{"location":"config.ini/#enabled_4","text":"Options: yes or no. If set to no, entire section is ignored and no DNS tests are run. When enabled, DNS tests are run for each of the dns_target paramters defined in this section. Any targets that have no value entered will be ignored. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#dns_target1","text":"Hostname of first DNS target. No target details = no test run Default setting: dns_target1: bbc.co.uk top","title":"dns_target1"},{"location":"config.ini/#dns_target2","text":"Hostname of second DNS target. No target details = no test run Default setting: dns_target2: cisco.com top","title":"dns_target2"},{"location":"config.ini/#dns_target3","text":"Hostname of third DNS target. No target details = no test run Default setting: dns_target3: google.com top","title":"dns_target3"},{"location":"config.ini/#dns_target4","text":"Hostname of fourth DNS target. No target details = no test run Default setting: dns_target4: top","title":"dns_target4"},{"location":"config.ini/#dns_target5","text":"Hostname of fifth DNS target. No target details = no test run Default setting: dns_target5: top","title":"dns_target5"},{"location":"config.ini/#dns_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DNS tests in Splunk Default setting: dns_data_file: wiperf-dns top","title":"dns_data_file"},{"location":"config.ini/#http_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[HTTP_test] Section"},{"location":"config.ini/#enabled_5","text":"Options: yes or no. If set to no, entire section is ignored and no HTTP tests are run. When enabled, HTTP tests are run for each of the http_target paramters defined in this section. Any targets that have no value entered will be ignored. Targets must include the full url of each site to be queried (including http:// or https:// element). Valid site address examples: http://bbc.co.uk https://ebay.com A http get will be performed for each target and the result code returned. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#http_target1","text":"Hostname of first HTTP target. No target details = no test run Default setting: http_target1: https://ebay.co.uk top","title":"http_target1"},{"location":"config.ini/#http_target2","text":"Hostname of second HTTP target. No target details = no test run Default setting: http_target2: http://twitter.com top","title":"http_target2"},{"location":"config.ini/#http_target3","text":"Hostname of third HTTP target. No target details = no test run Default setting: http_target3: https://facebook.com top","title":"http_target3"},{"location":"config.ini/#http_target4","text":"Hostname of fourth HTTP target. No target details = no test run Default setting: http_target4: https://instagram.com top","title":"http_target4"},{"location":"config.ini/#http_target5","text":"Hostname of fifth HTTP target. No target details = no test run Default setting: https://amazon.com http_target5: top","title":"http_target5"},{"location":"config.ini/#http_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for HTTP tests in Splunk Default setting: http_data_file: wiperf-http top","title":"http_data_file"},{"location":"config.ini/#dhcp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in Wiperf mode on the WLANPi)","title":"[DHCP_test] Section"},{"location":"config.ini/#enabled_6","text":"Options: yes or no. If set to no, entire section is ignored and no DHCP test is run. Note that the DHCP test has 2 modes : passive: only a renewal request is sent (no release of IP) active: a release and renew request is performed. Note that the active setting has shown varying degrees of usefulness in esting. In some scenarios (e.g. when connected via ZeroTier), it has caused connectivity issues, hence the passive setting is a better choice. Obviously, the passive setting does not perform such a rigorous DHCP test and is completed much quicker than the active mode. However, it still provides a useful comparative measure of the reponsivemess of DHCP servers. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#mode","text":"Available options: passive active The active settings performs a full release/request and may be disruptve to connectivity - use with caution. The passive setting is the recommended option for most situations. Default setting: mode: passive top","title":"mode"},{"location":"config.ini/#dhcp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DHCP tests in Splunk Default setting: dhcp_data_file: wiperf-dhcp top","title":"dhcp_data_file"},{"location":"data_points/","text":"Wiperf - Data Points Reference Guide Background The wiperf probe collects a variety of data points about various aspects of network connectivity and performance. It then makes those data points available to a number of databases via their standard API (e.g. Splunk, InfluxDB etc.). The data collected in all instances is the same, but the format of the data presented to each type of database varies depending on the their API and formatting rules and syntax. This document details the data points collected by the probe. These field names and the data values should be the same no matter which database is used. The probe may collect data for the following network tests, depending upon its configuration: Wireless network connectivity details Speedtest testing results data ICMP ping tests to various destinations DNS lookup tests to various destinations ] HTTP (web) tests to various destinations iperf3 TCP test to a nominated iperf3 server iperf3 UDP test to a nominated iperf3 server DHCP renewal test to test DHCP performance on network to which the WLAN Pi is connected The tests are run each time the wiperf process is triggered (usually every 5 minutes from a local cron job). The tests that are run, together with test configuration parameters are configured in the config.ini file. Here are the data points that may be collected, displayed by test type: Data Points Details Wireless Network Connectivity (Data source: wiperf-network) time : Unix timestamp of time test was performed ssid : The network name of the wireless network to which the wiperf probe is currently connected bssid : The basic service set identifier (i.e. MAC address) of the radio to which the wiperf probe is currently connected freq_ghz : The centre frequency of the channel on which the probe is operating (note this may be different to the primary channel centre freq if a bonded channel is in use) center_freq_ghz : The centre frequency of the primary channel on which the probe is operating channel : The channel number on which the probe is operating channel_width: The channel width (e.g. 20MHz, 40MHz, 80MHz) of the channel on which the probe is operating tx_rate_mbps : The PHY rate at which data is being sent from the probe to the AP (note this is not a throughput rate, just a physical connection rate) rx_rate_mbps : The PHY rate at which data is being sent from the AP to the probe (note this is not a throughput rate, just a physical connection rate) tx_mcs : For HT & VHT connections, this is the the MCS value used by the probe to the AP rx_mcs : For HT & VHT connections, this is the the MCS value used by the AP to the probe signal_level_dbm : The power level of the AP radio signal as observed by the probe (in dBm) tx_retries : The number of transmitted frames that have had to be sent gain (retried) ip_address : The IP address assigned to the probe WLAN NIC Speedtest Results (Data source: wiperf-speedtest) time : Unix timestamp of time test was performed ping_time : The RTT of a ping test to the speedtest server download_rate_mbps : The throughput rate achieved when receiving data from the speedtest server in megabits per second upload_rate_mbps : The throughput rate achieved when sending data to the speedtest server in megabits per second server_name : The name of the speedtest server used for this test Ping Results (Data source: wiperf-ping) time : Unix timestamp of time test was performed ping_index : wiperf runs up to 5 instances of ping test via its configuration file. This index uniquely identifies each instance. ping_host : The IP address or hostname of the target host/site being pinged pkts_tx : The number of ping request packets sent during the ping test pkts_rx : The number of ping response packets received back during the ping test percent_loss : The percentage (%) of packets lost during the test (i.e. how many responses were received compared to requests sent) test_time_ms : How long the ping test took in total rtt_min_ms : The minimum round trip time of all ping tests to this test instance in milliseconds rtt_avg_ms : The average round trip time of all ping tests to this test instance in milliseconds rtt_max_ms : The maximum round trip time of all ping tests to this test instance in milliseconds rtt_mdev_ms : Standard deviation of all ping tests to this test instance (...no, I don't know either...but you'll look cool at dinner parties if you mention it.) DNS Results (Data source: wiperf-ping) time : Unix timestamp of time test was performed dns_index : wiperf runs up to 5 instances of DNS test via its configuration file. This index uniquely identifies each instance. dns_target : The domain name of the target host/site which is the subject of the DNS lookup test lookup_time_ms : The time taken to perform the DNS lookup in milliseconds HTTP Results (Data source: wiperf-http) time : Unix timestamp of time test was performed http_index : wiperf runs up to 5 instances of HTTP test via its configuration file. This index uniquely identifies each instance. http_target : The domain name (or IP address) of the target site which is the subject of the HTTP test lookup_time_ms : The time taken to retrieve the html page from the target site in milliseconds http_status_code : The HTTP status code returned from the target site in this test instance (200 is good, other values have varying meanings: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes ) iperf3 TCP Results (Data source: wiperf-iperf3-tcp) time : Unix timestamp of time test was performed sent_mbps : The transmit throughput achieved (in megabits per seconds) during the TCP iperf test received_mbps : The receive throughput achieved (in megabits per seconds) during the TCP iperf test sent_bytes : The number of bytes (i.e. data volume) sent from the probe to the iperf server during the test received_bytes : The number of bytes (i.e. data volume) received by the probe from the iperf server during the test retransmits : The number of times frames had to be re=transmitted during the test iperf3 UDP Results (Data source: wiperf-iperf3-udp) time : Unix timestamp of time test was performed bytes : The number of bytes transferred from the probe to the iperf server during the test mbps : The throughput achieved (in megabits per second) during the iperf test when sending data to the iperf server jitter_ms : The level of jitter measured (in milliseconds) during the test packets : The number of packets sent from the probe to the iperf server during the test lost_packets : The number of transmitted packets lost during the test lost_percent : The percentage of transmitted packets lost during the test DHCP Test Results (Data source: wiperf-dhcp) time : Unix timestamp of time test was performed renewal_time_ms : The time taken for the probe to renew it's IP address in milliseconds","title":"Wiperf - Data Points Reference Guide"},{"location":"data_points/#wiperf-data-points-reference-guide","text":"","title":"Wiperf - Data Points Reference Guide"},{"location":"data_points/#background","text":"The wiperf probe collects a variety of data points about various aspects of network connectivity and performance. It then makes those data points available to a number of databases via their standard API (e.g. Splunk, InfluxDB etc.). The data collected in all instances is the same, but the format of the data presented to each type of database varies depending on the their API and formatting rules and syntax. This document details the data points collected by the probe. These field names and the data values should be the same no matter which database is used. The probe may collect data for the following network tests, depending upon its configuration: Wireless network connectivity details Speedtest testing results data ICMP ping tests to various destinations DNS lookup tests to various destinations ] HTTP (web) tests to various destinations iperf3 TCP test to a nominated iperf3 server iperf3 UDP test to a nominated iperf3 server DHCP renewal test to test DHCP performance on network to which the WLAN Pi is connected The tests are run each time the wiperf process is triggered (usually every 5 minutes from a local cron job). The tests that are run, together with test configuration parameters are configured in the config.ini file. Here are the data points that may be collected, displayed by test type:","title":"Background"},{"location":"data_points/#data-points-details","text":"","title":"Data Points Details"},{"location":"data_points/#wireless-network-connectivity","text":"(Data source: wiperf-network) time : Unix timestamp of time test was performed ssid : The network name of the wireless network to which the wiperf probe is currently connected bssid : The basic service set identifier (i.e. MAC address) of the radio to which the wiperf probe is currently connected freq_ghz : The centre frequency of the channel on which the probe is operating (note this may be different to the primary channel centre freq if a bonded channel is in use) center_freq_ghz : The centre frequency of the primary channel on which the probe is operating channel : The channel number on which the probe is operating channel_width: The channel width (e.g. 20MHz, 40MHz, 80MHz) of the channel on which the probe is operating tx_rate_mbps : The PHY rate at which data is being sent from the probe to the AP (note this is not a throughput rate, just a physical connection rate) rx_rate_mbps : The PHY rate at which data is being sent from the AP to the probe (note this is not a throughput rate, just a physical connection rate) tx_mcs : For HT & VHT connections, this is the the MCS value used by the probe to the AP rx_mcs : For HT & VHT connections, this is the the MCS value used by the AP to the probe signal_level_dbm : The power level of the AP radio signal as observed by the probe (in dBm) tx_retries : The number of transmitted frames that have had to be sent gain (retried) ip_address : The IP address assigned to the probe WLAN NIC","title":"Wireless Network Connectivity"},{"location":"data_points/#speedtest-results","text":"(Data source: wiperf-speedtest) time : Unix timestamp of time test was performed ping_time : The RTT of a ping test to the speedtest server download_rate_mbps : The throughput rate achieved when receiving data from the speedtest server in megabits per second upload_rate_mbps : The throughput rate achieved when sending data to the speedtest server in megabits per second server_name : The name of the speedtest server used for this test","title":"Speedtest Results"},{"location":"data_points/#ping-results","text":"(Data source: wiperf-ping) time : Unix timestamp of time test was performed ping_index : wiperf runs up to 5 instances of ping test via its configuration file. This index uniquely identifies each instance. ping_host : The IP address or hostname of the target host/site being pinged pkts_tx : The number of ping request packets sent during the ping test pkts_rx : The number of ping response packets received back during the ping test percent_loss : The percentage (%) of packets lost during the test (i.e. how many responses were received compared to requests sent) test_time_ms : How long the ping test took in total rtt_min_ms : The minimum round trip time of all ping tests to this test instance in milliseconds rtt_avg_ms : The average round trip time of all ping tests to this test instance in milliseconds rtt_max_ms : The maximum round trip time of all ping tests to this test instance in milliseconds rtt_mdev_ms : Standard deviation of all ping tests to this test instance (...no, I don't know either...but you'll look cool at dinner parties if you mention it.)","title":"Ping Results"},{"location":"data_points/#dns-results","text":"(Data source: wiperf-ping) time : Unix timestamp of time test was performed dns_index : wiperf runs up to 5 instances of DNS test via its configuration file. This index uniquely identifies each instance. dns_target : The domain name of the target host/site which is the subject of the DNS lookup test lookup_time_ms : The time taken to perform the DNS lookup in milliseconds","title":"DNS Results"},{"location":"data_points/#http-results","text":"(Data source: wiperf-http) time : Unix timestamp of time test was performed http_index : wiperf runs up to 5 instances of HTTP test via its configuration file. This index uniquely identifies each instance. http_target : The domain name (or IP address) of the target site which is the subject of the HTTP test lookup_time_ms : The time taken to retrieve the html page from the target site in milliseconds http_status_code : The HTTP status code returned from the target site in this test instance (200 is good, other values have varying meanings: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes )","title":"HTTP Results"},{"location":"data_points/#iperf3-tcp-results","text":"(Data source: wiperf-iperf3-tcp) time : Unix timestamp of time test was performed sent_mbps : The transmit throughput achieved (in megabits per seconds) during the TCP iperf test received_mbps : The receive throughput achieved (in megabits per seconds) during the TCP iperf test sent_bytes : The number of bytes (i.e. data volume) sent from the probe to the iperf server during the test received_bytes : The number of bytes (i.e. data volume) received by the probe from the iperf server during the test retransmits : The number of times frames had to be re=transmitted during the test","title":"iperf3 TCP Results"},{"location":"data_points/#iperf3-udp-results","text":"(Data source: wiperf-iperf3-udp) time : Unix timestamp of time test was performed bytes : The number of bytes transferred from the probe to the iperf server during the test mbps : The throughput achieved (in megabits per second) during the iperf test when sending data to the iperf server jitter_ms : The level of jitter measured (in milliseconds) during the test packets : The number of packets sent from the probe to the iperf server during the test lost_packets : The number of transmitted packets lost during the test lost_percent : The percentage of transmitted packets lost during the test","title":"iperf3 UDP Results"},{"location":"data_points/#dhcp-test-results","text":"(Data source: wiperf-dhcp) time : Unix timestamp of time test was performed renewal_time_ms : The time taken for the probe to renew it's IP address in milliseconds","title":"DHCP Test Results"},{"location":"faq/","text":"FAQ I see bounce command error messages in the agent.log file, what is going on? The following error messages may be seen in the agent.log file of wiperf if using the WLAN Pi v1.9.1 (or earlier) image: ERROR - if bounce command appears to have failed. Error: sudo: no tty present and no askpass program specified This occurs when connectivity issues are experienced and wiperf attempt to bounce the wireless interface to recover the wireless connection. To fix this issue, add the following entries to the /etc/sudoers.d/wlanpidump file: /sbin/ifdown /sbin/ifup The modified file content should be as follows: wlanpi ALL = (root) NOPASSWD: /sbin/iwconfig, /usr/sbin/iw, /sbin/dhclient, /sbin/ifconfig, /sbin/reboot, /bin/kill, /bin/date, /sbin/ifdown, /sbin/ifup This will ensure that the wireless interface may be correctly bounced by wiperf if required. Where do I get the dashboard reports for Splunk? Use SFTP/SCP and pull the xml files in /home/wlanpi/wiperf/dashboards from your WLAN Pi. See the [Splunk build guide][splunk_build] for details of how to add them to Splunk. The dashboard reports show no MCS data and RX PHY rate data - why not? Various WLAN NICs that use both Realtek and Mediatek WLAN chips are now supported by the WLAN Pi. Unfortunately, the Realtek chipsets (e.g. our old favourite the CF-912) do not report as much data as the Mediatek chips, so this data is missing. As I am not aware of any way of making the dashboard reports show data conditional on the chipset used, some graphs are shown but not fully populate - sorry. How do I get more reports or customize the supplied Splunk reports? Sorry, you'll have to roll up your sleeves and have a look at this for yourself: https://docs.splunk.com/Documentation/Splunk/8.0.1/SearchTutorial/Createnewdashboard Can I make a feature suggestion? Yes, get along to the GitHub site and post your suggestion in the 'Issues' section: https://github.com/wifinigel/wiperf/issues. It will be added to my \"todo\" list. Can I run tests over the Ethernet interface of the WLAN Pi? No, not at present. It was originally designed as a WLAN test device, so I need to do a bit of code re-writing to get tests going over Ethernet. Stay tuned. I'm running the v1.9 WLAN Pi image and the iperf tests don't work....what's going on? There was an issue with the code distributed with image v1.9. Try the following: ssh to the WLAN Pi Run the following commands (assuming the WLAN Pi has Internet connectivity): cd ~/wiperf git pull https://github.com/wifinigel/wiperf.git (It's best to do this is classic mode and redo you Wiperf configuration again after this operation - note that the config.default.ini file has new options you will probably like to use. Don't forget to check /home/wlanpi/wiperf/config/etc/wpa_supplicant/wpa_supplicant.conf too.)","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#i-see-bounce-command-error-messages-in-the-agentlog-file-what-is-going-on","text":"The following error messages may be seen in the agent.log file of wiperf if using the WLAN Pi v1.9.1 (or earlier) image: ERROR - if bounce command appears to have failed. Error: sudo: no tty present and no askpass program specified This occurs when connectivity issues are experienced and wiperf attempt to bounce the wireless interface to recover the wireless connection. To fix this issue, add the following entries to the /etc/sudoers.d/wlanpidump file: /sbin/ifdown /sbin/ifup The modified file content should be as follows: wlanpi ALL = (root) NOPASSWD: /sbin/iwconfig, /usr/sbin/iw, /sbin/dhclient, /sbin/ifconfig, /sbin/reboot, /bin/kill, /bin/date, /sbin/ifdown, /sbin/ifup This will ensure that the wireless interface may be correctly bounced by wiperf if required.","title":"I see bounce command error messages in the agent.log file, what is going on?"},{"location":"faq/#where-do-i-get-the-dashboard-reports-for-splunk","text":"Use SFTP/SCP and pull the xml files in /home/wlanpi/wiperf/dashboards from your WLAN Pi. See the [Splunk build guide][splunk_build] for details of how to add them to Splunk.","title":"Where do I get the dashboard reports for Splunk?"},{"location":"faq/#the-dashboard-reports-show-no-mcs-data-and-rx-phy-rate-data-why-not","text":"Various WLAN NICs that use both Realtek and Mediatek WLAN chips are now supported by the WLAN Pi. Unfortunately, the Realtek chipsets (e.g. our old favourite the CF-912) do not report as much data as the Mediatek chips, so this data is missing. As I am not aware of any way of making the dashboard reports show data conditional on the chipset used, some graphs are shown but not fully populate - sorry.","title":"The dashboard reports show no MCS data and RX PHY rate data - why not?"},{"location":"faq/#how-do-i-get-more-reports-or-customize-the-supplied-splunk-reports","text":"Sorry, you'll have to roll up your sleeves and have a look at this for yourself: https://docs.splunk.com/Documentation/Splunk/8.0.1/SearchTutorial/Createnewdashboard","title":"How do I get more reports or customize the supplied Splunk reports?"},{"location":"faq/#can-i-make-a-feature-suggestion","text":"Yes, get along to the GitHub site and post your suggestion in the 'Issues' section: https://github.com/wifinigel/wiperf/issues. It will be added to my \"todo\" list.","title":"Can I make a feature suggestion?"},{"location":"faq/#can-i-run-tests-over-the-ethernet-interface-of-the-wlan-pi","text":"No, not at present. It was originally designed as a WLAN test device, so I need to do a bit of code re-writing to get tests going over Ethernet. Stay tuned.","title":"Can I run tests over the Ethernet interface of the WLAN Pi?"},{"location":"faq/#im-running-the-v19-wlan-pi-image-and-the-iperf-tests-dont-workwhats-going-on","text":"There was an issue with the code distributed with image v1.9. Try the following: ssh to the WLAN Pi Run the following commands (assuming the WLAN Pi has Internet connectivity): cd ~/wiperf git pull https://github.com/wifinigel/wiperf.git (It's best to do this is classic mode and redo you Wiperf configuration again after this operation - note that the config.default.ini file has new options you will probably like to use. Don't forget to check /home/wlanpi/wiperf/config/etc/wpa_supplicant/wpa_supplicant.conf too.)","title":"I'm running the v1.9 WLAN Pi image and the iperf tests don't work....what's going on?"},{"location":"grafana_configure/","text":"Grafana Configuration Once the Granafa installation is complete, there are two main tasks remaining: Integration of Grafana with InfluxDB Addition of the wiperf dashboards Integration of Grafana with InfluxDB Integration with InfluxDB is required to allow Grafana to send data queries to the InfluxDB database and turn the data in to graphical reports. To do this, Grafana needs to know: the data source is an InfluxDB database where it is (IP address & port) the name of the database within Influx DB (as we previously configured) the access credentials to be used to pull the data out of InfluxDB The screen-shots below show the required steps: Configuration > Data Sources > Added Data Source: Select InfluxDB: Enter the name to be referenced for the connection, the URL, database name, username & password (all highlighted below) - note the InfluxDB values use those configure previously when we set up InfluxDB (these settings must match those used in the InfluxDB setup): Once completed, if you hit 'Save and Test', the database connection test should indicate success if all information has been correctly entered. Adding Wiperf Dashboards Dashboards can be obtained from the '/usr/share/wiperf/dashboards' folder of the probe. These may be downloaded to your local laptop using a utlity such as SCP or SFTP. Alternatively, the dashboard files may be accessed in the main wiperf repo at : folder Once the dashboard files have been downloaded, they may be imported to Grafana using the following steps: Use the menu option Dashboards > Manage > Import: Hit the 'Upload Json' button and select the dashboard file on your local laptop that you'd like to upload The name of the imported report will be shown as indicated. If you'd like to create or select a folder, this can be done in the 'Folder' field. The database connection previously created as the data source must also be selected to ensure the InfluxDB database can be queried: Once 'Import' is hit, the dashboard will be available in the main Grafana GUI.","title":"Grafana Configuration"},{"location":"grafana_configure/#grafana-configuration","text":"Once the Granafa installation is complete, there are two main tasks remaining: Integration of Grafana with InfluxDB Addition of the wiperf dashboards","title":"Grafana Configuration"},{"location":"grafana_configure/#integration-of-grafana-with-influxdb","text":"Integration with InfluxDB is required to allow Grafana to send data queries to the InfluxDB database and turn the data in to graphical reports. To do this, Grafana needs to know: the data source is an InfluxDB database where it is (IP address & port) the name of the database within Influx DB (as we previously configured) the access credentials to be used to pull the data out of InfluxDB The screen-shots below show the required steps: Configuration > Data Sources > Added Data Source: Select InfluxDB: Enter the name to be referenced for the connection, the URL, database name, username & password (all highlighted below) - note the InfluxDB values use those configure previously when we set up InfluxDB (these settings must match those used in the InfluxDB setup): Once completed, if you hit 'Save and Test', the database connection test should indicate success if all information has been correctly entered.","title":"Integration of Grafana with InfluxDB"},{"location":"grafana_configure/#adding-wiperf-dashboards","text":"Dashboards can be obtained from the '/usr/share/wiperf/dashboards' folder of the probe. These may be downloaded to your local laptop using a utlity such as SCP or SFTP. Alternatively, the dashboard files may be accessed in the main wiperf repo at : folder Once the dashboard files have been downloaded, they may be imported to Grafana using the following steps: Use the menu option Dashboards > Manage > Import: Hit the 'Upload Json' button and select the dashboard file on your local laptop that you'd like to upload The name of the imported report will be shown as indicated. If you'd like to create or select a folder, this can be done in the 'Folder' field. The database connection previously created as the data source must also be selected to ensure the InfluxDB database can be queried: Once 'Import' is hit, the dashboard will be available in the main Grafana GUI.","title":"Adding Wiperf Dashboards"},{"location":"grafana_install/","text":"Grafana Installation Obtaining and installing the Grafana software is very straightforward. The following notes provide a high level overview of the steps required. Note that these instructions are for Grafana version 6.7 (other versions may work, but have not been tested): Visit the Grafana 6.7 installation guide at https://grafana.com/docs/grafana/v6.7/ . This provides acces to a wide variety of information about Grafanam including supported OS'es and platform concepts To download the code and to see the commands required for installation on the server CLI, visit the following download page: https://grafana.com/grafana/download Select the required version, Enterprise Edition and your OS (most likely a Linux variant) Make a copy of the CLI commands provided to download and install the software for your OS SSH to the server that will be used to host Grafana Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start grafana Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status grafana (ensure the process is \"active (running)\" ) As a final check, ensure that the Grafana web GUI is available using the URL: http:// :3000/","title":"Grafana Installation"},{"location":"grafana_install/#grafana-installation","text":"Obtaining and installing the Grafana software is very straightforward. The following notes provide a high level overview of the steps required. Note that these instructions are for Grafana version 6.7 (other versions may work, but have not been tested): Visit the Grafana 6.7 installation guide at https://grafana.com/docs/grafana/v6.7/ . This provides acces to a wide variety of information about Grafanam including supported OS'es and platform concepts To download the code and to see the commands required for installation on the server CLI, visit the following download page: https://grafana.com/grafana/download Select the required version, Enterprise Edition and your OS (most likely a Linux variant) Make a copy of the CLI commands provided to download and install the software for your OS SSH to the server that will be used to host Grafana Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start grafana Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status grafana (ensure the process is \"active (running)\" ) As a final check, ensure that the Grafana web GUI is available using the URL: http:// :3000/","title":"Grafana Installation"},{"location":"grafana_platform/","text":"Grafana Platform Grafana is an open-source visualization tool that allows us to create reports around the data sent from wiperf probes to InfluxDB. It integrates with a number of data sources to query raw data and provides a wide variety of graphical report options. This guide does not cover all installation details of the software package, these may be obtained from the official Grafana web site: https://grafana.com/docs/grafana/latest/ . Installation instructions are available for all major operating systems. Note that although Windows is supported, if you intend to install Grafana on the same platform as InfuxDB, Windows is not an option as InfluxDB v1.8 does not support Windows. To install Grafana and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"Grafana Platform"},{"location":"grafana_platform/#grafana-platform","text":"Grafana is an open-source visualization tool that allows us to create reports around the data sent from wiperf probes to InfluxDB. It integrates with a number of data sources to query raw data and provides a wide variety of graphical report options. This guide does not cover all installation details of the software package, these may be obtained from the official Grafana web site: https://grafana.com/docs/grafana/latest/ . Installation instructions are available for all major operating systems. Note that although Windows is supported, if you intend to install Grafana on the same platform as InfuxDB, Windows is not an option as InfluxDB v1.8 does not support Windows. To install Grafana and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"Grafana Platform"},{"location":"influx_configure/","text":"Influx Configuration Now that we have the InfluxDB software installed, the next step is to create a database into which data from our wiperf probes will be stored. To create the database, we need to execute a series of commands on the CLI of the Influx DB server. Follow the following steps to create the required database: Check the InfluxDB service is running before starting sudo systemctl status influxdb (ensure the process is \"active (running)\" ) Enter the InfluxDB shell using the following command: sudo influx (indicated by the new \">\" prompt) Create an admin user to administer Influx: CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES Exit the InfluxDB shell with the command exit Edit the InfluxDB configuration file (/etc/influxdb/influxdb.conf): sudo nano /etc/influxdb/influxdb.conf uncomment the line # auth-enabled = false in the [http] section and change to auth-enabled = true to enable authentication of access to the database restart the InfluxDB process for the change to take effect: sudo systemctl restart influxdb Enter the InfluxDB shell using the following command: sudo influx -username admin -password letmein (now using authentication) Create a new database with the following commands: CREATE DATABASE wiperf Check the new database exists using: SHOW DATABASES (the database \"wiperf\" should be shown in the list) Create and assign a user who can write to the wiperf database (i.e. a probe) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" Create and assign a user who can read fro the wiperf database (i.e. the Grafana program) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" Exit the InfluxDB shell with the command exit At this point, the InfluxDB service is ready to receive data from a probe. If you have any probes ready to go, make sure they used the \"wiperf_probe\" user credentials in their configuration file so that they can add their data to the database. If you believe you have a probe that has successfully sent data, you can check the database contents using the following commands in the InfluxDB shell: USE wiperf SHOW SERIES SELECT * FROM \"wiperf-speedtest\" SHOW FIELD KEYS ON \"wiperf\" FROM \"wiperf-speedtest\" To find out more details, please checkout the official getting started guide: https://docs.influxdata.com/influxdb/v1.8/introduction/get-started/ For more information about adding users, check out: https://docs.influxdata.com/influxdb/v1.8/administration/authentication_and_authorization/","title":"Influx Configuration"},{"location":"influx_configure/#influx-configuration","text":"Now that we have the InfluxDB software installed, the next step is to create a database into which data from our wiperf probes will be stored. To create the database, we need to execute a series of commands on the CLI of the Influx DB server. Follow the following steps to create the required database: Check the InfluxDB service is running before starting sudo systemctl status influxdb (ensure the process is \"active (running)\" ) Enter the InfluxDB shell using the following command: sudo influx (indicated by the new \">\" prompt) Create an admin user to administer Influx: CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES Exit the InfluxDB shell with the command exit Edit the InfluxDB configuration file (/etc/influxdb/influxdb.conf): sudo nano /etc/influxdb/influxdb.conf uncomment the line # auth-enabled = false in the [http] section and change to auth-enabled = true to enable authentication of access to the database restart the InfluxDB process for the change to take effect: sudo systemctl restart influxdb Enter the InfluxDB shell using the following command: sudo influx -username admin -password letmein (now using authentication) Create a new database with the following commands: CREATE DATABASE wiperf Check the new database exists using: SHOW DATABASES (the database \"wiperf\" should be shown in the list) Create and assign a user who can write to the wiperf database (i.e. a probe) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" Create and assign a user who can read fro the wiperf database (i.e. the Grafana program) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" Exit the InfluxDB shell with the command exit At this point, the InfluxDB service is ready to receive data from a probe. If you have any probes ready to go, make sure they used the \"wiperf_probe\" user credentials in their configuration file so that they can add their data to the database. If you believe you have a probe that has successfully sent data, you can check the database contents using the following commands in the InfluxDB shell: USE wiperf SHOW SERIES SELECT * FROM \"wiperf-speedtest\" SHOW FIELD KEYS ON \"wiperf\" FROM \"wiperf-speedtest\" To find out more details, please checkout the official getting started guide: https://docs.influxdata.com/influxdb/v1.8/introduction/get-started/ For more information about adding users, check out: https://docs.influxdata.com/influxdb/v1.8/administration/authentication_and_authorization/","title":"Influx Configuration"},{"location":"influx_install/","text":"Influx Installation Obtaining and installing the InfluxDB software is very straightforward. The following steps provide a high level over view of the steps required: Visit the InfluxDB v1.8 installation guide at https://docs.influxdata.com/influxdb/v1.8/introduction/install/ Scroll down to the \"Installing InfluxDB OSS\" section Select the OS of the platform that you will be using to host your instance of InfluxDB Copy the commands provided for your server OS to add the required software repository SSH to the server that will be used to host InfluxDB Paste in the commands copied from the installation page on to the CLI of your server. These will ensure your server can find the required repository to pull the InfluxDB software To get the required commands to download & install the software, visit the following web page and select the v1.8 download option: https://portal.influxdata.com/downloads/ Copy the install commands provided for your OS Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start influxdb Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status influxdb (ensure the process is \"active (running)\" ) The next step is to create a database to drop our incoming data (from wiperf probes) into.","title":"Influx Installation"},{"location":"influx_install/#influx-installation","text":"Obtaining and installing the InfluxDB software is very straightforward. The following steps provide a high level over view of the steps required: Visit the InfluxDB v1.8 installation guide at https://docs.influxdata.com/influxdb/v1.8/introduction/install/ Scroll down to the \"Installing InfluxDB OSS\" section Select the OS of the platform that you will be using to host your instance of InfluxDB Copy the commands provided for your server OS to add the required software repository SSH to the server that will be used to host InfluxDB Paste in the commands copied from the installation page on to the CLI of your server. These will ensure your server can find the required repository to pull the InfluxDB software To get the required commands to download & install the software, visit the following web page and select the v1.8 download option: https://portal.influxdata.com/downloads/ Copy the install commands provided for your OS Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start influxdb Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status influxdb (ensure the process is \"active (running)\" ) The next step is to create a database to drop our incoming data (from wiperf probes) into.","title":"Influx Installation"},{"location":"influx_platform/","text":"InfluxDB Platform InfluxDB is a time-series database that we use to store the network performance data that is collected by wiperf probes. It has many other uses and is used by many organizations as a backing store for use cases involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. InfluxDB does not show our network performance report data, but is used as a data repository source for Grafana in our use-case. Details about Grafana are provide later in this documentation. Note that for our use-case, we are using InfluxDB v1.8 (not v2.0). Influx can be installed on a wide variety of Linux-based platforms that can be viewed at : https://docs.influxdata.com/influxdb/v1.8/introduction/install/ . These include Ubuntu, Debian and macOS (no Windows) This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. To install InfluxDB and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software. Connectivity Planning One area to consider is connectivity between the wiperf probe and the InfluxDB instance. The wiperf probe needs to be able to access the InfluxDB server to send its data. If the wiperf probe probe is being deployed on a network, how is the performance data generated going to get back to the InfluxDB server? If the probe is being deployed on a customer network to perform temporary monitoring, it will obviously join the wireless network under test. But how is the wiperf probe going to send its data to the InfluxDB server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, hence (potentially) bridging wired and wireless networks. Therefore, in many instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier. Three topology deployment options are supported: - Results data over wireless - Results data over Ethernet - Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the InfluxDB server. The 3 connectivity options are discussed below. Results Data Over Wireless In this topology the wiperf probe is configured to join an SSID that has the InfluxDB server accessible via its WLAN interface. Typically, the InfluxDB server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the InfluxDB server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of InfluxDB server> Results data over Ethernet If the InfluxDB server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of InfluxDB server> Results data over Zerotier/wireless A very simple way of getting the wiperf probe talking with your InfluxDB server is to use the Zerotier service to create a virtual network. In summary, both the InfluxDB server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same VLAN in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the InfluxDB server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees, It\u2019s very easy to use and get going, plus your InfluxDB server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the InfluxDB server and wiperf probe when the client is installed. Seriously, give it a go...it's quicker to try it than me explaining it here: https://www.zerotier.com/ config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of InfluxDB server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"InfluxDB Platform"},{"location":"influx_platform/#influxdb-platform","text":"InfluxDB is a time-series database that we use to store the network performance data that is collected by wiperf probes. It has many other uses and is used by many organizations as a backing store for use cases involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. InfluxDB does not show our network performance report data, but is used as a data repository source for Grafana in our use-case. Details about Grafana are provide later in this documentation. Note that for our use-case, we are using InfluxDB v1.8 (not v2.0). Influx can be installed on a wide variety of Linux-based platforms that can be viewed at : https://docs.influxdata.com/influxdb/v1.8/introduction/install/ . These include Ubuntu, Debian and macOS (no Windows) This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. To install InfluxDB and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"InfluxDB Platform"},{"location":"influx_platform/#connectivity-planning","text":"One area to consider is connectivity between the wiperf probe and the InfluxDB instance. The wiperf probe needs to be able to access the InfluxDB server to send its data. If the wiperf probe probe is being deployed on a network, how is the performance data generated going to get back to the InfluxDB server? If the probe is being deployed on a customer network to perform temporary monitoring, it will obviously join the wireless network under test. But how is the wiperf probe going to send its data to the InfluxDB server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, hence (potentially) bridging wired and wireless networks. Therefore, in many instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier. Three topology deployment options are supported: - Results data over wireless - Results data over Ethernet - Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the InfluxDB server. The 3 connectivity options are discussed below.","title":"Connectivity Planning"},{"location":"influx_platform/#results-data-over-wireless","text":"In this topology the wiperf probe is configured to join an SSID that has the InfluxDB server accessible via its WLAN interface. Typically, the InfluxDB server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the InfluxDB server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of InfluxDB server>","title":"Results Data Over Wireless"},{"location":"influx_platform/#results-data-over-ethernet","text":"If the InfluxDB server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of InfluxDB server>","title":"Results data over Ethernet"},{"location":"influx_platform/#results-data-over-zerotierwireless","text":"A very simple way of getting the wiperf probe talking with your InfluxDB server is to use the Zerotier service to create a virtual network. In summary, both the InfluxDB server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same VLAN in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the InfluxDB server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees, It\u2019s very easy to use and get going, plus your InfluxDB server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the InfluxDB server and wiperf probe when the client is installed. Seriously, give it a go...it's quicker to try it than me explaining it here: https://www.zerotier.com/ config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of InfluxDB server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Results data over Zerotier/wireless"},{"location":"operation/","text":"Overview of Operation Wiperf is an open source utility that runs on a Raspberry Pi or a WLAN Pi hardware device. It provides network probe functionality to gather performance data to give an indication of how a network looks from an end user perspective. It runs a series of tests to gather metrics on network connectivity and performance through the execution of tests such ICMP pings, DNS lookups and iperf. These are fully configurable by editing a local configuration file on the probe device at the time of deployment. Configuration To configure the details of the tests to be run on a probe, a local configuration file on the probe needs to be updated. This will provide information to the probe about items such as the required network connectivity (e.g. wireless/ethernet), IP and credential information of the data server and test details. The configuration file can be updated by accessing the CLI of the probe (usually via SSH) and editing the file '/etc/wiperf/config.ini'. A default config template file is provided as a start point for the final configuration file ( '/etc/wiperf/config.default/ini'). It is best to take a copy of this file to create the final customised configuration file. When accessing the probe to create the configuration file, a Linux text editor such as 'nano' or 'vi' should be used. Here is a suggested workflow to create a probe configuration file: cd /etc/wiperf sudo cp ./config.default.ini ./config.ini sudo nano ./config.ini cron (Note: This operation is required for the Raspberry Pi only. The WLAN Pi will automatically setup the required cron job during the mode switch) In addition to the creating a customised configuration file for the probe, a mechanism is required to run the wiperf utility on a regular basis (e.g. every 5 minutes). Cron is a Linux utility that can be used to run wiperf periodically to gather data over time. The following CLI commands must be used to add a cron job to the probe to gather data on a regular basis: sudo crontab -e Add the following line to run the configured tests every 5 minutes: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 wpa_supplicant If the wiperf probe is to be connected to a wireless network, then details of the wireless network and the credentials to access the network need to be configured on the probe. This is achieved by configuring the following file: # on the RPi, edit the following file: sudo nano /etc/wpa_supplicant/wpa_supplicant.conf # on the WLAN Pi edit the following file: sudo nano /etc/wiperf/confi/etc/wpa_supplicant/wpa_supplicant.conf Logging Following the completion of the configuration described above, if all is configured correctly, then wiperf will run every 5 minutes, perform the configured tests, and then send the data back to the data server. A number of logs are generated to provide support information around the installation and operation of the wiperf process. Each of the generated log files are detailed below: # This log provides details of the installation and upgrade processes, so # can be useful in diagnosing installation issues /var/log/wiperf_install.log # This log file is updated by the main wiperf script each time # it is run. If the script appears to fail completely, this is # a good place to check /var/log/wiperg_cron.log # This log provides details of the tests performed each time # that wiperf runs. It is the main file to use for diagnosing # issues with wiperf /var/log/wiperf_agent.log Reporting Gathering the data with a probe is only half of the story when using wiperf. The gathered data must be sent to a data collection server to allow it to be visualized to allow analysis of network performance. The data server must be an instance of either: Splunk, or InfluxDB with Grafana Splunk The graphic above outlines the collection of network performance data and how this is sent to Splunk by the wiperf probe. The Splunk instance may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.) In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Splunk server for storage and later analysis The data is analyzed by accessing the dashboard (a web GUI) of the Splunk server (Note that the Splunk server acts as both the data repository and reporting platform for collected data) Grafana/Influx The graphic above outlines the collection of network performance data and how this is sent to Influx & Grafana by the wiperf probe. Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data source from which to pull its network performance data. To meet this requirement, the Influx database server is used. Like Grafana, InfluxDB is also an open-source package. Note this contrasts with using Splunk, which allows us to use the same package to provide both the data storage and analysis/visualization functions. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana & Influx may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.). Note that wiperf only needs to be able to send data to Influx - it requires no communication with the Grafana instance. In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Influx server for storage Grafana is configured to use Influx as its data source to allow visualization of performance data Data is analysed by accessing the dashboard (a web GUI) of the Grafana server, which pull the required dashboard data from Influx.","title":"Operation Overview"},{"location":"operation/#overview-of-operation","text":"Wiperf is an open source utility that runs on a Raspberry Pi or a WLAN Pi hardware device. It provides network probe functionality to gather performance data to give an indication of how a network looks from an end user perspective. It runs a series of tests to gather metrics on network connectivity and performance through the execution of tests such ICMP pings, DNS lookups and iperf. These are fully configurable by editing a local configuration file on the probe device at the time of deployment.","title":"Overview of Operation"},{"location":"operation/#configuration","text":"To configure the details of the tests to be run on a probe, a local configuration file on the probe needs to be updated. This will provide information to the probe about items such as the required network connectivity (e.g. wireless/ethernet), IP and credential information of the data server and test details. The configuration file can be updated by accessing the CLI of the probe (usually via SSH) and editing the file '/etc/wiperf/config.ini'. A default config template file is provided as a start point for the final configuration file ( '/etc/wiperf/config.default/ini'). It is best to take a copy of this file to create the final customised configuration file. When accessing the probe to create the configuration file, a Linux text editor such as 'nano' or 'vi' should be used. Here is a suggested workflow to create a probe configuration file: cd /etc/wiperf sudo cp ./config.default.ini ./config.ini sudo nano ./config.ini","title":"Configuration"},{"location":"operation/#cron","text":"(Note: This operation is required for the Raspberry Pi only. The WLAN Pi will automatically setup the required cron job during the mode switch) In addition to the creating a customised configuration file for the probe, a mechanism is required to run the wiperf utility on a regular basis (e.g. every 5 minutes). Cron is a Linux utility that can be used to run wiperf periodically to gather data over time. The following CLI commands must be used to add a cron job to the probe to gather data on a regular basis: sudo crontab -e Add the following line to run the configured tests every 5 minutes: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1","title":"cron"},{"location":"operation/#wpa_supplicant","text":"If the wiperf probe is to be connected to a wireless network, then details of the wireless network and the credentials to access the network need to be configured on the probe. This is achieved by configuring the following file: # on the RPi, edit the following file: sudo nano /etc/wpa_supplicant/wpa_supplicant.conf # on the WLAN Pi edit the following file: sudo nano /etc/wiperf/confi/etc/wpa_supplicant/wpa_supplicant.conf","title":"wpa_supplicant"},{"location":"operation/#logging","text":"Following the completion of the configuration described above, if all is configured correctly, then wiperf will run every 5 minutes, perform the configured tests, and then send the data back to the data server. A number of logs are generated to provide support information around the installation and operation of the wiperf process. Each of the generated log files are detailed below: # This log provides details of the installation and upgrade processes, so # can be useful in diagnosing installation issues /var/log/wiperf_install.log # This log file is updated by the main wiperf script each time # it is run. If the script appears to fail completely, this is # a good place to check /var/log/wiperg_cron.log # This log provides details of the tests performed each time # that wiperf runs. It is the main file to use for diagnosing # issues with wiperf /var/log/wiperf_agent.log","title":"Logging"},{"location":"operation/#reporting","text":"Gathering the data with a probe is only half of the story when using wiperf. The gathered data must be sent to a data collection server to allow it to be visualized to allow analysis of network performance. The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"Reporting"},{"location":"operation/#splunk","text":"The graphic above outlines the collection of network performance data and how this is sent to Splunk by the wiperf probe. The Splunk instance may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.) In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Splunk server for storage and later analysis The data is analyzed by accessing the dashboard (a web GUI) of the Splunk server (Note that the Splunk server acts as both the data repository and reporting platform for collected data)","title":"Splunk"},{"location":"operation/#grafanainflux","text":"The graphic above outlines the collection of network performance data and how this is sent to Influx & Grafana by the wiperf probe. Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data source from which to pull its network performance data. To meet this requirement, the Influx database server is used. Like Grafana, InfluxDB is also an open-source package. Note this contrasts with using Splunk, which allows us to use the same package to provide both the data storage and analysis/visualization functions. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana & Influx may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.). Note that wiperf only needs to be able to send data to Influx - it requires no communication with the Grafana instance. In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Influx server for storage Grafana is configured to use Influx as its data source to allow visualization of performance data Data is analysed by accessing the dashboard (a web GUI) of the Grafana server, which pull the required dashboard data from Influx.","title":"Grafana/Influx"},{"location":"probe_configure/","text":"Probe Configuration Contents Wiperf - Configuration on the WLAN Pi Instructions for InfluxDB Testers Hostname Configuration File (config.ini) Wireless Client Configuration (wpa_supplicant.conf) Ethernet Client Configuration Testing Updating Known Issues iperf Test Fail Interface Bounce Failures DNS Lookup Order Issue Hostname Change Related Issues Comfast CF-912 80MHz Width Channels MCS & Rx Phy rates Missing From Reports Tests Fail To Start Due to DNS Failures Troubleshooting Additional Features Watchdog Security CLI Mode Switch Instructions for InfluxDB Testers (conf_pull branch) Burn a new SD card with the V1.9.1 image Make sure WLAN Pi is connected to the Internet SSH to the WLAN Pi and execute the following command to install a missing required Python InfluxDB module: sudo /usr/bin/python3 -m pip install influxdb Install the latest version of wiperf with Influx support (in the same SSH session): sudo pkg_admin -i wiperf -b v0.12-a1 Then, proceed with the instructions in the remainder of this document. Wiperf - Configuration on the WLAN Pi This instruction paper assumes you are running Wiperf on a WLAN Pi on an image version of v1.9 or later (which has Wiperf installed and available as part of the image.) ( Special note for WLAN Pi image v1.9.0: please see the Known Issues ) section of this document) The Wiperf probe is activated via the front panel menu system (FPMS) of the WLAN Pi. But, before flipping in to the Wiperf mode, a few configuration steps need to be completed: top Hostname It is strongly advised that you configure the hostname of your WLAN Pi before following the steps detailed below. The data sent to and stored in your reporting database (e.g. Splunk, Influx etc.) will be associated with the WLAN Pi hostname that is used when the data is forwarded. If you decide to subsequently change the hostname, then historical data from the unit will not be associated with the data sent with the new hostname. If you are running multiple WLAN Pi units, then you MUST change their hostnames, as you will not be able to differentiate their data within Splunk. All data, from all units, will be shown under the default 'wlanpi' hostname. To change the WLAN Pi hostname, please check out the following FAQ page and complete all suggested steps: link top Configuration File (config.ini) The operation of Wiperf is configured using the file '/home/wanpi/wiperf/config.ini' This needs to be edited prior to entering Wiperf mode. Prior to the first use of Wiperf, the config.ini file does not exist in the required WLAN Pi directory. However, a default template config file ( config.default.ini ) is supplied that can be used to create the config.ini file. Here is the suggested workflow: Connect to the WLAN Pi, create a copy of the config template file and edit the newly created config (as the wlanpi user): cd /home/wlanpi/wiperf cp ./config.default.ini ./config.ini nano ./config.ini By default, the configuration file is set to run all tests. However, there is a minimum configuration that must be applied for Wiperf mode to run out-of-the-box. The minimum configuration parameters you need to configure (just to get you going...) are outlined in the subsections below. In summary you need to: Configure the Wiperf global mode of operation (wireless or Ethernet) and the interface parameters that determine how the WLAN Pi is connected Configure the management platform you'll be sending data to Configure the tests you'd like to run Interface Parameters As the WLAN Pi can now test over the ethernet or WLAN interfaces, we need to tell the software which mode is in use and which interfaces the test traffic and results data will be sent over. (Note that \"ethernet\" mode with mgt traffic sent over the wireless interface is not supported) [General] ; global test mode: wireless or ethernet ; ; wireless mode: ; - test traffic runs over wireless interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; ethernet mode: ; - all test and management traffic sent over ethernet port (mgt_if parameter not used/ignored) ; probe_mode: wireless ; ------------- ethernet mode parameters ------------ ; eth interface name set this as per the output of an ifconfig command (usually eth0) ; (no management interface required, as tests & management traffic over same i/f) eth_if: eth0 ; --------------------------------------------------- ; ------------- wireless mode parameters ------------ ; wlan interface name set this as per the output of an iwconfig command (usually wlan0) wlan_if: wlan0 ; interface name over which mgt traffic is sent (i.e. how we get to our management ; server) - options: wlan0, eth0, zt mgt_if: wlan0 ; --------------------------------------------------- Database Parameters Wiperf can send results data to Splunk, InfluxDB (v1.x) and InfluxDB2 data collectors through an exporter module for each collector type. The relevant authentication parameters need to be set for the collector in-use in the following sections (note these need to be configured on the data collector platform also before sending results data): [General] ; --------- Common Mgt Platform Params ------- ; set the data exporter type - current options: splunk, influxdb, influxdb2 exporter_type: splunk ; -------------------------------------------- ; -------------- Splunk Config --------------- ; IP address or hostname of Splunk host splunk_host: 192.168.0.99 ; Splunk collector port (8088 by default) splunk_port: 8088 ; Splunk token to access Splunk server created by Splunk (example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d) splunk_token: <token_here> ;--------------------------------------------- ; -------------- InFlux1 Config --------------- ; IP address or hostname of InfluxDB host influx_host: ; InfluxDb collector port (8086 by default) influx_port: 8086 influx_username: influx_password: influx_database: ;--------------------------------------------- ; -------------- InFlux2 Config --------------- ; IP address or hostname of InfluxDB2 host influx2_host: ; InfluxDB2 collector port (443 by default) influx2_port: 443 influx2_token: influx2_bucket: influx2_org: ;--------------------------------------------- Tests Note that all network tests are enabled by default. If there are some tests you'd like to disable (e.g. if you don't have an iperf3 server set up), then you'll need to open up the config.ini file and look through each section for the \"enabled\" parameter for that test and set it to \"no\". For example, to disable the iperf tcp test: [Iperf3_tcp_test] ; yes = enabled, no = disabled enabled: no For a full description of the configuration file parameters, please review the following page: config.ini reference guide . The Splunk token is obtained from your Splunk server (see Splunk build guide ). top Wireless Client Configuration (wpa_supplicant.conf) If Wiperf is running in wireless mode, then WLAN Pi is flipped in to Wiperf mode, it will need to join the SSID under test to run the configured tests. We need to provide a configuration (that is only used in Wiperf mode) to allow the WLAN Pi to join a WLAN. Edit the following file with the configuration and credentials that will be used by the WLAN Pi to join the SSID under test once it is switched in to Wiperf mode (make edits logged in as the wlanpi user): cd /home/wlanpi/wiperf/conf/etc/wpa_supplicant nano ./wpa_supplicant.conf top Ethernet Client Configuration Note that no specific configuration is required for the Ethernet interface when running Wiperf in Ethernet mode. As long as the Ethernet port is connected to a switch port tht supplies an IP address via DHCP, then you're good to go. top Testing Once the required edits have been made to configure Wiperf mode, flip the WLAN Pi in to Wiperf mode using the following FPMS options: Actions > Wiperf > Confirm If no errors are observed on the FPMS during flip-over, inspect the following files to double-check for errors & verify that test data is generated (as indicated in the log messages): cat /home/wlanpi/wiperf/logs/agent.log cat /home/wlanpi/wiperf/wiperf.log Note that by default the tests are run every 5 mins unless the interval has been changed in the config.ini file. Wait at least this interval before determining that there is an issue - the test cycle will NOT begin immediately upon entering Wiperf mode. Check your database platform and verify that data is being received. top Updating To get the latest updates from the GitHub repo (when available), use the following commands when logged in as the wlanpi user: cd ~/wiperf git pull https://github.com/wifinigel/wiperf.git (note that this will update config.default.ini but not config.ini, so remember to re-edit it after a pull if the config file format changes) top Known Issues: iperf Tests Fail There is an issue with the v1.9.0 WLAN Pi image that means that the iperf tests fail when running in wiperf mode. To get the fixed version, follow the update process detailed in the Updating section of this document (3rd Jan 2020) top Interface Bounce Failures There is an issue in WLAN Pi image versions v1.9.0 & v1.9.1 with some required commands missing from the /etc/sudoers.d/wlanpidump file. This issue manifests itself with errors about WLAN interface bounce operations failing in the agent.log file. To fix this issue, please apply the following fix: Add the following entries to the /etc/sudoers.d/wlanpidump file: /sbin/ifdown /sbin/ifup The modified file content should be as follows: wlanpi ALL = (root) NOPASSWD: /sbin/iwconfig, /usr/sbin/iw, /sbin/dhclient, /sbin/ifconfig, /sbin/reboot, /bin/kill, /bin/date, /sbin/ifdown, /sbin/ifup Reboot the WLAN Pi after applying this file update top DNS Lookup Order Issue By default in WLAN Pi image versions up to and including v1.9.1, the DNS server value 8.8.8.8 has been hard-coded in to the DNS server lookup list. This has been achieved by configuring the following line in to the file /etc/resolvconf/resolv.conf.d/head : nameserver 8.8.8.8 This ensures that the 8.8.8.8 address is always used as the first DNS lookup entry, even if the WLAN Pi receives a DNS server address during its DHCP process. This can be verified by performing a cat /etc/resolv.conf when the WLAN Pi is in wiperf mode - even though there may be two or more entries in the file, 8.8.8.8 will always be shown at the top of the file listing, and be used as the first lookup server. This may not be desirable behaviour, as 8.8.8.8 may not be available or DHCP assigned servers may be preferred. To ensure that the desired DNS environment is used, the following options exist: Remove the 8.8.8.8 entry completely: edit the file /etc/resolvconf/resolv.conf.d/head and remove the offending entry completely. This will ensure only the DHCP assigned DNS server(s) will be used. Replace the 8.8.8.8 entry by editing the /etc/resolvconf/resolv.conf.d/head and replacing it with a desired value Move the 8.8.8.8 entry from the /etc/resolvconf/resolv.conf.d/head file to the /etc/resolvconf/resolv.conf.d/tail file, so that 8.8.8.8 is still used, but is now the last option in the DNS server list (only used when other preceding servers do not return a result) top Hostname Change Related Issues There have been a number of issues reported that have been reported that are due to the WLAN Pi hostname being changed from the default, but it has not been updated in both the /etc/hostname AND /etc/hosts file. Please ensure you have followed this process : Link The issue tends to manifest itself as various \"weird\" issues such as \"sudo\" commands failing for no apparent reason. top Comfast CF-912 80MHz Width Channels There seems to be an issue with the Comfast CF-912 adapter when using it with the WLAN Pi and associating as a client to SSIDs that use 80MHz width channels. If you hit an issue where the WLAN Pi seems to lock up or does not boot correctly, try a different adapter or a network that does not use 80Mhz channels. top MCS & Rx Phy rates Missing From Reports top In several dashboard reports, the reported MCS values & Rx Phy rate may be blank. This is because these values are only reported by MediaTek wireless NICs. Therefore, the CF-912 will not show these values (as it is a Realtek NIC). Sorry, not much I can do about this. top Tests Fail To Start Due to DNS Failures In versions of wiperf before version v0.10, the wiperf probe performed a series of tests to verify the health of the wireless connection prior to tests running. One of these tests included a DNS lookup to \"bbc.co.uk\" to verify Internet connectivity. In some environments, this may not be a valid test. To fix this issue, a new configuring parameter was added to the config.ini file that allows a custom lookup target to be provided, if requried: connectivity_lookup: google.com top top Additional Features: Watchdog Wiperf has a watchdog feature that it uses to try to reset things when it is having connectivity related difficulties. There may be instances when tests are continually failing or wireless connectivity is intermittent due to perhaps being stuck on a remote AP that is sub-optimal from a connectivity perspective. If persistent issues are detected, then Wiperf will reboot the WLAN Pi to try to remediate the issue. This will provide the opportunity to the reset all network connectivity and internal processes. Note that this is a last ditch mechanism. Wiperf will try bouncing the WLAN interface to remediate any short-term connectivity issues, which will likely fixe many issues without the need for a full reboot. If you observe your WLAN Pi rebooting on a regular basis (e.g. a couple of times a hour), then check its logs as it is very unhappy about something. top Security Wiperf employs the following security mechanisms in an attempt to harden the WLAN Pi when deployed in Wiperf mode: No forwarding is allowed between interfaces The internal UFW firewall is configured to only allow incoming connectivity on port 22 on the wlan0 & eth0 interfaces top CLI Mode Switch If you are remote from your WLAN Pi you may not be able to flip it in to Wiperf mode using the front panel buttons. However, it is possible to flip it in to Wiperf mode using the CLI (via an SSH session). To flip in to Wiperf mode using the CLI, SSH to your WLAN Pi and execute the following on the CLI ( CAVEAT: make sure you have correctly configured the /home/wlanpi/wiperf/conf/etc/wpa_suplicant/wpa_supplicant.conf file before do this...otherwise you may lose comms with the WLAN Pi after it reboots if you rely on the WLAN connection for access): # performed as the wlanpi user cd ~/wiperf sudo ./wiperf_switcher on ``` After executing this command, the WLAN Pi will reboot in to the Wiperf mode. If you'd like to flip back from Wiperf mode, SSH to the WLAN Pi and execute: # performed as the wlanpi user cd ~/wiperf sudo ./wiperf_switcher off ``` top","title":"Probe Configuration"},{"location":"probe_configure/#probe-configuration","text":"","title":"Probe Configuration"},{"location":"probe_configure/#contents","text":"Wiperf - Configuration on the WLAN Pi Instructions for InfluxDB Testers Hostname Configuration File (config.ini) Wireless Client Configuration (wpa_supplicant.conf) Ethernet Client Configuration Testing Updating Known Issues iperf Test Fail Interface Bounce Failures DNS Lookup Order Issue Hostname Change Related Issues Comfast CF-912 80MHz Width Channels MCS & Rx Phy rates Missing From Reports Tests Fail To Start Due to DNS Failures Troubleshooting Additional Features Watchdog Security CLI Mode Switch","title":"Contents"},{"location":"probe_configure/#instructions-for-influxdb-testers-conf_pull-branch","text":"Burn a new SD card with the V1.9.1 image Make sure WLAN Pi is connected to the Internet SSH to the WLAN Pi and execute the following command to install a missing required Python InfluxDB module: sudo /usr/bin/python3 -m pip install influxdb Install the latest version of wiperf with Influx support (in the same SSH session): sudo pkg_admin -i wiperf -b v0.12-a1 Then, proceed with the instructions in the remainder of this document.","title":"Instructions for InfluxDB Testers (conf_pull branch)"},{"location":"probe_configure/#wiperf-configuration-on-the-wlan-pi","text":"This instruction paper assumes you are running Wiperf on a WLAN Pi on an image version of v1.9 or later (which has Wiperf installed and available as part of the image.) ( Special note for WLAN Pi image v1.9.0: please see the Known Issues ) section of this document) The Wiperf probe is activated via the front panel menu system (FPMS) of the WLAN Pi. But, before flipping in to the Wiperf mode, a few configuration steps need to be completed: top","title":"Wiperf - Configuration on the WLAN Pi"},{"location":"probe_configure/#hostname","text":"It is strongly advised that you configure the hostname of your WLAN Pi before following the steps detailed below. The data sent to and stored in your reporting database (e.g. Splunk, Influx etc.) will be associated with the WLAN Pi hostname that is used when the data is forwarded. If you decide to subsequently change the hostname, then historical data from the unit will not be associated with the data sent with the new hostname. If you are running multiple WLAN Pi units, then you MUST change their hostnames, as you will not be able to differentiate their data within Splunk. All data, from all units, will be shown under the default 'wlanpi' hostname. To change the WLAN Pi hostname, please check out the following FAQ page and complete all suggested steps: link top","title":"Hostname"},{"location":"probe_configure/#configuration-file-configini","text":"The operation of Wiperf is configured using the file '/home/wanpi/wiperf/config.ini' This needs to be edited prior to entering Wiperf mode. Prior to the first use of Wiperf, the config.ini file does not exist in the required WLAN Pi directory. However, a default template config file ( config.default.ini ) is supplied that can be used to create the config.ini file. Here is the suggested workflow: Connect to the WLAN Pi, create a copy of the config template file and edit the newly created config (as the wlanpi user): cd /home/wlanpi/wiperf cp ./config.default.ini ./config.ini nano ./config.ini By default, the configuration file is set to run all tests. However, there is a minimum configuration that must be applied for Wiperf mode to run out-of-the-box. The minimum configuration parameters you need to configure (just to get you going...) are outlined in the subsections below. In summary you need to: Configure the Wiperf global mode of operation (wireless or Ethernet) and the interface parameters that determine how the WLAN Pi is connected Configure the management platform you'll be sending data to Configure the tests you'd like to run","title":"Configuration File (config.ini)"},{"location":"probe_configure/#interface-parameters","text":"As the WLAN Pi can now test over the ethernet or WLAN interfaces, we need to tell the software which mode is in use and which interfaces the test traffic and results data will be sent over. (Note that \"ethernet\" mode with mgt traffic sent over the wireless interface is not supported) [General] ; global test mode: wireless or ethernet ; ; wireless mode: ; - test traffic runs over wireless interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; ethernet mode: ; - all test and management traffic sent over ethernet port (mgt_if parameter not used/ignored) ; probe_mode: wireless ; ------------- ethernet mode parameters ------------ ; eth interface name set this as per the output of an ifconfig command (usually eth0) ; (no management interface required, as tests & management traffic over same i/f) eth_if: eth0 ; --------------------------------------------------- ; ------------- wireless mode parameters ------------ ; wlan interface name set this as per the output of an iwconfig command (usually wlan0) wlan_if: wlan0 ; interface name over which mgt traffic is sent (i.e. how we get to our management ; server) - options: wlan0, eth0, zt mgt_if: wlan0 ; ---------------------------------------------------","title":"Interface Parameters"},{"location":"probe_configure/#database-parameters","text":"Wiperf can send results data to Splunk, InfluxDB (v1.x) and InfluxDB2 data collectors through an exporter module for each collector type. The relevant authentication parameters need to be set for the collector in-use in the following sections (note these need to be configured on the data collector platform also before sending results data): [General] ; --------- Common Mgt Platform Params ------- ; set the data exporter type - current options: splunk, influxdb, influxdb2 exporter_type: splunk ; -------------------------------------------- ; -------------- Splunk Config --------------- ; IP address or hostname of Splunk host splunk_host: 192.168.0.99 ; Splunk collector port (8088 by default) splunk_port: 8088 ; Splunk token to access Splunk server created by Splunk (example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d) splunk_token: <token_here> ;--------------------------------------------- ; -------------- InFlux1 Config --------------- ; IP address or hostname of InfluxDB host influx_host: ; InfluxDb collector port (8086 by default) influx_port: 8086 influx_username: influx_password: influx_database: ;--------------------------------------------- ; -------------- InFlux2 Config --------------- ; IP address or hostname of InfluxDB2 host influx2_host: ; InfluxDB2 collector port (443 by default) influx2_port: 443 influx2_token: influx2_bucket: influx2_org: ;---------------------------------------------","title":"Database Parameters"},{"location":"probe_configure/#tests","text":"Note that all network tests are enabled by default. If there are some tests you'd like to disable (e.g. if you don't have an iperf3 server set up), then you'll need to open up the config.ini file and look through each section for the \"enabled\" parameter for that test and set it to \"no\". For example, to disable the iperf tcp test: [Iperf3_tcp_test] ; yes = enabled, no = disabled enabled: no For a full description of the configuration file parameters, please review the following page: config.ini reference guide . The Splunk token is obtained from your Splunk server (see Splunk build guide ). top","title":"Tests"},{"location":"probe_configure/#wireless-client-configuration-wpa_supplicantconf","text":"If Wiperf is running in wireless mode, then WLAN Pi is flipped in to Wiperf mode, it will need to join the SSID under test to run the configured tests. We need to provide a configuration (that is only used in Wiperf mode) to allow the WLAN Pi to join a WLAN. Edit the following file with the configuration and credentials that will be used by the WLAN Pi to join the SSID under test once it is switched in to Wiperf mode (make edits logged in as the wlanpi user): cd /home/wlanpi/wiperf/conf/etc/wpa_supplicant nano ./wpa_supplicant.conf top","title":"Wireless Client Configuration (wpa_supplicant.conf)"},{"location":"probe_configure/#ethernet-client-configuration","text":"Note that no specific configuration is required for the Ethernet interface when running Wiperf in Ethernet mode. As long as the Ethernet port is connected to a switch port tht supplies an IP address via DHCP, then you're good to go. top","title":"Ethernet Client Configuration"},{"location":"probe_configure/#testing","text":"Once the required edits have been made to configure Wiperf mode, flip the WLAN Pi in to Wiperf mode using the following FPMS options: Actions > Wiperf > Confirm If no errors are observed on the FPMS during flip-over, inspect the following files to double-check for errors & verify that test data is generated (as indicated in the log messages): cat /home/wlanpi/wiperf/logs/agent.log cat /home/wlanpi/wiperf/wiperf.log Note that by default the tests are run every 5 mins unless the interval has been changed in the config.ini file. Wait at least this interval before determining that there is an issue - the test cycle will NOT begin immediately upon entering Wiperf mode. Check your database platform and verify that data is being received. top","title":"Testing"},{"location":"probe_configure/#updating","text":"To get the latest updates from the GitHub repo (when available), use the following commands when logged in as the wlanpi user: cd ~/wiperf git pull https://github.com/wifinigel/wiperf.git (note that this will update config.default.ini but not config.ini, so remember to re-edit it after a pull if the config file format changes) top","title":"Updating"},{"location":"probe_configure/#known-issues","text":"","title":"Known Issues:"},{"location":"probe_configure/#iperf-tests-fail","text":"There is an issue with the v1.9.0 WLAN Pi image that means that the iperf tests fail when running in wiperf mode. To get the fixed version, follow the update process detailed in the Updating section of this document (3rd Jan 2020) top","title":"iperf Tests Fail"},{"location":"probe_configure/#interface-bounce-failures","text":"There is an issue in WLAN Pi image versions v1.9.0 & v1.9.1 with some required commands missing from the /etc/sudoers.d/wlanpidump file. This issue manifests itself with errors about WLAN interface bounce operations failing in the agent.log file. To fix this issue, please apply the following fix: Add the following entries to the /etc/sudoers.d/wlanpidump file: /sbin/ifdown /sbin/ifup The modified file content should be as follows: wlanpi ALL = (root) NOPASSWD: /sbin/iwconfig, /usr/sbin/iw, /sbin/dhclient, /sbin/ifconfig, /sbin/reboot, /bin/kill, /bin/date, /sbin/ifdown, /sbin/ifup Reboot the WLAN Pi after applying this file update top","title":"Interface Bounce Failures"},{"location":"probe_configure/#dns-lookup-order-issue","text":"By default in WLAN Pi image versions up to and including v1.9.1, the DNS server value 8.8.8.8 has been hard-coded in to the DNS server lookup list. This has been achieved by configuring the following line in to the file /etc/resolvconf/resolv.conf.d/head : nameserver 8.8.8.8 This ensures that the 8.8.8.8 address is always used as the first DNS lookup entry, even if the WLAN Pi receives a DNS server address during its DHCP process. This can be verified by performing a cat /etc/resolv.conf when the WLAN Pi is in wiperf mode - even though there may be two or more entries in the file, 8.8.8.8 will always be shown at the top of the file listing, and be used as the first lookup server. This may not be desirable behaviour, as 8.8.8.8 may not be available or DHCP assigned servers may be preferred. To ensure that the desired DNS environment is used, the following options exist: Remove the 8.8.8.8 entry completely: edit the file /etc/resolvconf/resolv.conf.d/head and remove the offending entry completely. This will ensure only the DHCP assigned DNS server(s) will be used. Replace the 8.8.8.8 entry by editing the /etc/resolvconf/resolv.conf.d/head and replacing it with a desired value Move the 8.8.8.8 entry from the /etc/resolvconf/resolv.conf.d/head file to the /etc/resolvconf/resolv.conf.d/tail file, so that 8.8.8.8 is still used, but is now the last option in the DNS server list (only used when other preceding servers do not return a result) top","title":"DNS Lookup Order Issue"},{"location":"probe_configure/#hostname-change-related-issues","text":"There have been a number of issues reported that have been reported that are due to the WLAN Pi hostname being changed from the default, but it has not been updated in both the /etc/hostname AND /etc/hosts file. Please ensure you have followed this process : Link The issue tends to manifest itself as various \"weird\" issues such as \"sudo\" commands failing for no apparent reason. top","title":"Hostname Change Related Issues"},{"location":"probe_configure/#comfast-cf-912-80mhz-width-channels","text":"There seems to be an issue with the Comfast CF-912 adapter when using it with the WLAN Pi and associating as a client to SSIDs that use 80MHz width channels. If you hit an issue where the WLAN Pi seems to lock up or does not boot correctly, try a different adapter or a network that does not use 80Mhz channels. top","title":"Comfast CF-912 80MHz Width Channels"},{"location":"probe_configure/#mcs-rx-phy-rates-missing-from-reports","text":"top In several dashboard reports, the reported MCS values & Rx Phy rate may be blank. This is because these values are only reported by MediaTek wireless NICs. Therefore, the CF-912 will not show these values (as it is a Realtek NIC). Sorry, not much I can do about this. top","title":"MCS &amp; Rx Phy rates Missing From Reports"},{"location":"probe_configure/#tests-fail-to-start-due-to-dns-failures","text":"In versions of wiperf before version v0.10, the wiperf probe performed a series of tests to verify the health of the wireless connection prior to tests running. One of these tests included a DNS lookup to \"bbc.co.uk\" to verify Internet connectivity. In some environments, this may not be a valid test. To fix this issue, a new configuring parameter was added to the config.ini file that allows a custom lookup target to be provided, if requried: connectivity_lookup: google.com top top","title":"Tests Fail To Start Due to DNS Failures"},{"location":"probe_configure/#additional-features","text":"","title":"Additional Features:"},{"location":"probe_configure/#watchdog","text":"Wiperf has a watchdog feature that it uses to try to reset things when it is having connectivity related difficulties. There may be instances when tests are continually failing or wireless connectivity is intermittent due to perhaps being stuck on a remote AP that is sub-optimal from a connectivity perspective. If persistent issues are detected, then Wiperf will reboot the WLAN Pi to try to remediate the issue. This will provide the opportunity to the reset all network connectivity and internal processes. Note that this is a last ditch mechanism. Wiperf will try bouncing the WLAN interface to remediate any short-term connectivity issues, which will likely fixe many issues without the need for a full reboot. If you observe your WLAN Pi rebooting on a regular basis (e.g. a couple of times a hour), then check its logs as it is very unhappy about something. top","title":"Watchdog"},{"location":"probe_configure/#security","text":"Wiperf employs the following security mechanisms in an attempt to harden the WLAN Pi when deployed in Wiperf mode: No forwarding is allowed between interfaces The internal UFW firewall is configured to only allow incoming connectivity on port 22 on the wlan0 & eth0 interfaces top","title":"Security"},{"location":"probe_configure/#cli-mode-switch","text":"If you are remote from your WLAN Pi you may not be able to flip it in to Wiperf mode using the front panel buttons. However, it is possible to flip it in to Wiperf mode using the CLI (via an SSH session). To flip in to Wiperf mode using the CLI, SSH to your WLAN Pi and execute the following on the CLI ( CAVEAT: make sure you have correctly configured the /home/wlanpi/wiperf/conf/etc/wpa_suplicant/wpa_supplicant.conf file before do this...otherwise you may lose comms with the WLAN Pi after it reboots if you rely on the WLAN connection for access): # performed as the wlanpi user cd ~/wiperf sudo ./wiperf_switcher on ``` After executing this command, the WLAN Pi will reboot in to the Wiperf mode. If you'd like to flip back from Wiperf mode, SSH to the WLAN Pi and execute: # performed as the wlanpi user cd ~/wiperf sudo ./wiperf_switcher off ``` top","title":"CLI Mode Switch"},{"location":"probe_deploy/","text":"Probe Deployment","title":"Probe Deployment"},{"location":"probe_deploy/#probe-deployment","text":"","title":"Probe Deployment"},{"location":"probe_install/","text":"Probe Installation Raspberry Pi Pre-requisites Package Updates Update existing Linux packages: sudo apt-get update && sudo apt-get upgrade -y sudo reboot Install required Linux packages: sudo apt-get update sudo apt-get install python3-pip iperf3 git -y sudo reboot Install required python3 modules sudo pip3 install iperf3 speedtest-cli configparser sudo pip3 install git+git://github.com/georgestarcher/Splunk-Class-httpevent.git # I've had the Splunk-Class-httpevent install fail a couple of times, you can # try this as a workaround if you hit a similar issue, but this is a bit of a kludge: sudo wget -P /usr/local/lib/python3.7/dist-packages/ https://raw.githubusercontent.com/georgestarcher/Splunk-Class-httpevent/master/splunk_http_event_collector.py) User Account Create the wlanpi user: sudo adduser wlanpi Edit the sudoers file to enable the wlanpi user to run some commands that require elevated privilege: sudo visudo Add following line to bottom of file: wlanpi ALL=(ALL) NOPASSWD: ALL Reboot and log back in with the wlanpi user: sudo reboot Wireless Configuration Configure RPi to join a wireless network. Edit files 'sudo nano /etc/wpa_supplicant/wpa_supplicant.conf' and 'sudo nano /etc/network/interfaces'. The eth0 port is configured as static IP below, but can be left as dhcp if wlan0 & eth0 are on different networks (otherwise Speedtest traffic goes out of eth0 port) (note: wpa_supplicant.conf must have root:root ownership - 'chown root:root /etc/wpa_supplicant/wpa_supplicant.conf' if required) *** Sample '/etc/wpa_supplicant/wpa_supplicant.conf': ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 ap_scan=1 network={ ssid=\"My_SSID\" psk=\"My_SSID_Key\" } *** Sample '/etc/network/interfaces': # interfaces(5) file used by ifup(8) and ifdown(8) # Please note that this file is written to be used with dhcpcd # For static IP, consult /etc/dhcpcd.conf and 'man dhcpcd.conf' auto wlan0 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf # RPi 3a+ # post-up iw dev wlan0 set power_save off # RPi 3b+ # wireless-power off # Note eth0 has been set to a static address to avoid routing issues # when both eth0 and wlan0 are on same network (traffic goes out of # eth0 rather than wlan0). If they are on different networks, it's # OK to set eth0 to DHCP, but may still need a static route to force # Internet-bound traffic to use wlan0 rather than eth0 auto eth0 allow-hotplug eth0 # iface eth0 inet dhcp iface eth0 inet static address 192.168.254.1 netmask 255.255.255.0 # Local loopback auto lo iface lo inet loopback # Include files from /etc/network/interfaces.d: source-directory /etc/network/interfaces.d/* Reboot the RPi & verify the it has joined the wireless network with iwconfig/ifconfig Installation With the RPi connected to the Internet, login using the wlanpi user and clone this project: cd ~ git clone https://github.com/wifinigel/wiperf.git --depth 1 Edit the config file to customize the operation of the script: cd /home/wlanpi/wiperf cp ./config.default.ini ./config.ini nano ./config.ini Testing Test the script by running the following command as the wlanpi user (takes around 2 minutes to complete, depending on tests enabled): /usr/bin/env python3 /home/wlanpi/wiperf/wi-perf.py If no errors are observed when running it then check the following files to check for no errors & that data is generated: cat /home/wlanpi/wiperf/logs/agent.log # (Note: none of the files below are created when using the HEC forwarder ) cat /home/wlanpi/wiperf/data/wiperf-speedtest-splunk.json cat /home/wlanpi/wiperf/data/wiperf-ping-splunk.json cat /home/wlanpi/wiperf/data/wiperf-iperf3-udp-splunk.json cat /home/wlanpi/wiperf/data/wiperf-iperf3-tcp-splunk.json . . etc... Running: Schedule Regular Job Create a cronjob to run the script very 5 mins: crontab -e add line: */5 * * * * /usr/bin/env python3 /home/wlanpi/wiperf/wi-perf.py > /home/wlanpi/wiperf/wiperf.log 2>&1 Account Tidy-up If this has been built using a new RPI image, remember to either update the default 'pi' username with a new password, or remove the account. Make sure you have successfully logged in with the 'wlanpi' user and are using it to perform the operations shown below. Change password : sudo passwd pi Remove account: sudo userdel pi Updating To get the latest updates from the GitHub repo , use the following commands when logged in as the wlanpi user: cd ~/wiperf git pull https://github.com/wifinigel/wiperf.git (note that this will update config.default.ini but not config.ini, or remember to re-edit it after a pull if the format changes) Troubleshooting: If things seem to be going wrong, try the following: Run the script from the command line and watch for errors (/usr/bin/python3 /home/wlanpi/wiperf/wi-perf.py) SSH to the device & tail the log files in real-time: tail -f /home/wlanpi/wiperf/logs/agent.log Try disabling tests & see if one specific test is causing an issue Make sure all pre-reqs have definitely been fulfilled WLAN Pi If you have a WLAN Pi with image version v1.9 or later, the good news is you're good to go! If not, you will need to update your WLAN Pi image (see this video on [YouTube][wlanpi_reimage]). Note: if you have the v1.9 image, you need to check this out and update the wiperf code before you start configuring wiperf: FAQ note Assuming your image is up to date, you need to complete the following steps: Data Server: Splunk: Setup your Splunk server and obtain the HEC token from your Splunk instance (see this doc: [Splunk build guide][splunk_build]) InfluxDB: Setup your InfluxDB server and a Grafana server to graph the performance data received. SSH to your WLAN Pi and configure your WLAN Pi as detailed in this guide: [WLANPi initial config & test guide][wlanpi_config] Flip your WLAN Pi in to wiperf mode and wait for your data to appear (Front panel option : Menu > Mode > Wiperf)","title":"Probe Installation"},{"location":"probe_install/#probe-installation","text":"","title":"Probe Installation"},{"location":"probe_install/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"probe_install/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"probe_install/#package-updates","text":"Update existing Linux packages: sudo apt-get update && sudo apt-get upgrade -y sudo reboot Install required Linux packages: sudo apt-get update sudo apt-get install python3-pip iperf3 git -y sudo reboot Install required python3 modules sudo pip3 install iperf3 speedtest-cli configparser sudo pip3 install git+git://github.com/georgestarcher/Splunk-Class-httpevent.git # I've had the Splunk-Class-httpevent install fail a couple of times, you can # try this as a workaround if you hit a similar issue, but this is a bit of a kludge: sudo wget -P /usr/local/lib/python3.7/dist-packages/ https://raw.githubusercontent.com/georgestarcher/Splunk-Class-httpevent/master/splunk_http_event_collector.py)","title":"Package Updates"},{"location":"probe_install/#user-account","text":"Create the wlanpi user: sudo adduser wlanpi Edit the sudoers file to enable the wlanpi user to run some commands that require elevated privilege: sudo visudo Add following line to bottom of file: wlanpi ALL=(ALL) NOPASSWD: ALL Reboot and log back in with the wlanpi user: sudo reboot","title":"User Account"},{"location":"probe_install/#wireless-configuration","text":"Configure RPi to join a wireless network. Edit files 'sudo nano /etc/wpa_supplicant/wpa_supplicant.conf' and 'sudo nano /etc/network/interfaces'. The eth0 port is configured as static IP below, but can be left as dhcp if wlan0 & eth0 are on different networks (otherwise Speedtest traffic goes out of eth0 port) (note: wpa_supplicant.conf must have root:root ownership - 'chown root:root /etc/wpa_supplicant/wpa_supplicant.conf' if required) *** Sample '/etc/wpa_supplicant/wpa_supplicant.conf': ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 ap_scan=1 network={ ssid=\"My_SSID\" psk=\"My_SSID_Key\" } *** Sample '/etc/network/interfaces': # interfaces(5) file used by ifup(8) and ifdown(8) # Please note that this file is written to be used with dhcpcd # For static IP, consult /etc/dhcpcd.conf and 'man dhcpcd.conf' auto wlan0 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf # RPi 3a+ # post-up iw dev wlan0 set power_save off # RPi 3b+ # wireless-power off # Note eth0 has been set to a static address to avoid routing issues # when both eth0 and wlan0 are on same network (traffic goes out of # eth0 rather than wlan0). If they are on different networks, it's # OK to set eth0 to DHCP, but may still need a static route to force # Internet-bound traffic to use wlan0 rather than eth0 auto eth0 allow-hotplug eth0 # iface eth0 inet dhcp iface eth0 inet static address 192.168.254.1 netmask 255.255.255.0 # Local loopback auto lo iface lo inet loopback # Include files from /etc/network/interfaces.d: source-directory /etc/network/interfaces.d/* Reboot the RPi & verify the it has joined the wireless network with iwconfig/ifconfig","title":"Wireless Configuration"},{"location":"probe_install/#installation","text":"With the RPi connected to the Internet, login using the wlanpi user and clone this project: cd ~ git clone https://github.com/wifinigel/wiperf.git --depth 1 Edit the config file to customize the operation of the script: cd /home/wlanpi/wiperf cp ./config.default.ini ./config.ini nano ./config.ini","title":"Installation"},{"location":"probe_install/#testing","text":"Test the script by running the following command as the wlanpi user (takes around 2 minutes to complete, depending on tests enabled): /usr/bin/env python3 /home/wlanpi/wiperf/wi-perf.py If no errors are observed when running it then check the following files to check for no errors & that data is generated: cat /home/wlanpi/wiperf/logs/agent.log # (Note: none of the files below are created when using the HEC forwarder ) cat /home/wlanpi/wiperf/data/wiperf-speedtest-splunk.json cat /home/wlanpi/wiperf/data/wiperf-ping-splunk.json cat /home/wlanpi/wiperf/data/wiperf-iperf3-udp-splunk.json cat /home/wlanpi/wiperf/data/wiperf-iperf3-tcp-splunk.json . . etc...","title":"Testing"},{"location":"probe_install/#running-schedule-regular-job","text":"Create a cronjob to run the script very 5 mins: crontab -e add line: */5 * * * * /usr/bin/env python3 /home/wlanpi/wiperf/wi-perf.py > /home/wlanpi/wiperf/wiperf.log 2>&1","title":"Running: Schedule Regular Job"},{"location":"probe_install/#account-tidy-up","text":"If this has been built using a new RPI image, remember to either update the default 'pi' username with a new password, or remove the account. Make sure you have successfully logged in with the 'wlanpi' user and are using it to perform the operations shown below. Change password : sudo passwd pi Remove account: sudo userdel pi","title":"Account Tidy-up"},{"location":"probe_install/#updating","text":"To get the latest updates from the GitHub repo , use the following commands when logged in as the wlanpi user: cd ~/wiperf git pull https://github.com/wifinigel/wiperf.git (note that this will update config.default.ini but not config.ini, or remember to re-edit it after a pull if the format changes)","title":"Updating"},{"location":"probe_install/#troubleshooting","text":"If things seem to be going wrong, try the following: Run the script from the command line and watch for errors (/usr/bin/python3 /home/wlanpi/wiperf/wi-perf.py) SSH to the device & tail the log files in real-time: tail -f /home/wlanpi/wiperf/logs/agent.log Try disabling tests & see if one specific test is causing an issue Make sure all pre-reqs have definitely been fulfilled","title":"Troubleshooting:"},{"location":"probe_install/#wlan-pi","text":"If you have a WLAN Pi with image version v1.9 or later, the good news is you're good to go! If not, you will need to update your WLAN Pi image (see this video on [YouTube][wlanpi_reimage]). Note: if you have the v1.9 image, you need to check this out and update the wiperf code before you start configuring wiperf: FAQ note Assuming your image is up to date, you need to complete the following steps: Data Server: Splunk: Setup your Splunk server and obtain the HEC token from your Splunk instance (see this doc: [Splunk build guide][splunk_build]) InfluxDB: Setup your InfluxDB server and a Grafana server to graph the performance data received. SSH to your WLAN Pi and configure your WLAN Pi as detailed in this guide: [WLANPi initial config & test guide][wlanpi_config] Flip your WLAN Pi in to wiperf mode and wait for your data to appear (Front panel option : Menu > Mode > Wiperf)","title":"WLAN Pi"},{"location":"probe_platform/","text":"Probe Platform","title":"Probe Platform"},{"location":"probe_platform/#probe-platform","text":"","title":"Probe Platform"},{"location":"probe_prepare/","text":"Probe Preparation","title":"Probe Preparation"},{"location":"probe_prepare/#probe-preparation","text":"","title":"Probe Preparation"},{"location":"splunk_configure/","text":"Splunk Configuration Now that we have a Splunk server setup, we need to customize it to report our probe data. The steps required are all via the Splunk web GUI and are the same for all OS flavours. Configure Data Input To Splunk We need to tell Splunk how we\u2019ll be sending the data from our probe to Splunk. We need to configure a data input that will prepare SPlunk to receive the data and generate an authorization key to be used by the probe when sending data. Log In To Splunk The first step is to login to Splunk using the credentials created during the Splunk install. The URL to use is: http://<Splunk_server_IP>:8000 Configure HTTP Event Collector Global Options After login, the following page will be seen: Follow the \u201cSettings > Data > Data Inputs\u201d menu options : Click on the HTTP Event Collector link in the Data Inputs page shown: Click on the \u201cGlobal Settings\u201d button as indicated in the graphic above to reveal the global configuration panel for the HTTP Event Collector: Ensure the panel is configured to look like the example shown above. This should require the following steps: Make sure you hit the All Tokens > Enable button (disabled by default which stops everything working) Default Source Type: Structured > _json Hit: Save to take you back to the HTTP Event Collector page Create a HEC Token After returning to the HTTP Event Collector page, hit the New Token button. This will start a token creation wizard. Enter a name for the token (probe HEC Token) then hit Next > : In the next wizard panel select Source type: Select > Structured > _json : Scroll down to the indexes and make the following selections: Select Allowed Indexes > add all Default Index : main Next, hit the Review > button: The token review panel is now should and should look like the graphic below. Finally hit the Submit > button: A final confirmation message will be provided as shown below: If you return to Settings > Data Input > HTTPS Event Collector, you will now see the token your probe will need to communicate with the Splunk server: At this point, the Splunk server is ready to receive data from the probe. Ensure that your probe has been configured with the correct server IP address, port number and the token we have just created above (copy and paste the \u201cToken Value\u201d in to your probe config.ini file). If using the WLAN Pi, ensure that it is flipped in to wiperf mode. On the RPI, ensure that the required cron job has been configured to start polling. Perform a Test Search After a few minutes, when the probe has run a test cycle, data should start to appear in Splunk. The quickest way to check is to do a general search for data in Splunk and see what is being received. Go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and enter a \u201c*\u201d in the \u201cNew Search\u201d text box. Results data should be seen as shown below: If your search result looks like this (no results found message), then you need to wait a little longer for data to arrive, or there is likely a comms problem between your probe and Splunk: Create a Dashboard Now that we have data arriving at our Splunk server, we need to view the data in an interesting format. Splunk allows us to create a number of dashboards to visualize our data. We will now create a simple dashboard to demonstrate the visualization capabilities. In the WLANPi\u2019s /usr/share/wiperf/dashboards directory, a number of pre-canned dashboard files have been provided to allow a dashboard to be copied and pasted easily. These are also available on the GitHub page of the Wiperf project: https://github.com/wifinigel/wiperf/tree/master/dashboards Use an SFTP client to pull the \u201cprobe_summary.xml\u201d file from your probe or open the file on the GitHub page and select \u201cRaw\u201d to copy and paste the code in to a local file on your laptop. In the Splunk GUI, go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and hit the \u201cDashboards\u201d link: Hit the \"Create New Dashboard\" button: In the pop-up panel, enter a dashboard name and hit the \"Create Dashboard\" button: In the \u201cEdit Dashboard\u201d panel that opens, hit the \u201cSource\u201d button: By default, some basic XML configuration will exist in the dashboard definition: Open up the dashboard definition file previously downloaded from your probe or the GitHub site in a text editor. Then simply paste in the code as shown below (make sure the original code was all removed): After hitting the Save button, the dashboard will now be shown: Using the hostname and time period selector above the graphs, different probes and reporting periods may be viewed. The process above may be repeated using each of the xml files found in the dashboards folder to create a series of separate dashboards that focus on different aspects of data available using wiperf.","title":"Splunk Configuration"},{"location":"splunk_configure/#splunk-configuration","text":"Now that we have a Splunk server setup, we need to customize it to report our probe data. The steps required are all via the Splunk web GUI and are the same for all OS flavours.","title":"Splunk Configuration"},{"location":"splunk_configure/#configure-data-input-to-splunk","text":"We need to tell Splunk how we\u2019ll be sending the data from our probe to Splunk. We need to configure a data input that will prepare SPlunk to receive the data and generate an authorization key to be used by the probe when sending data.","title":"Configure Data Input To Splunk"},{"location":"splunk_configure/#log-in-to-splunk","text":"The first step is to login to Splunk using the credentials created during the Splunk install. The URL to use is: http://<Splunk_server_IP>:8000","title":"Log In To Splunk"},{"location":"splunk_configure/#configure-http-event-collector-global-options","text":"After login, the following page will be seen: Follow the \u201cSettings > Data > Data Inputs\u201d menu options : Click on the HTTP Event Collector link in the Data Inputs page shown: Click on the \u201cGlobal Settings\u201d button as indicated in the graphic above to reveal the global configuration panel for the HTTP Event Collector: Ensure the panel is configured to look like the example shown above. This should require the following steps: Make sure you hit the All Tokens > Enable button (disabled by default which stops everything working) Default Source Type: Structured > _json Hit: Save to take you back to the HTTP Event Collector page","title":"Configure  HTTP Event Collector Global Options"},{"location":"splunk_configure/#create-a-hec-token","text":"After returning to the HTTP Event Collector page, hit the New Token button. This will start a token creation wizard. Enter a name for the token (probe HEC Token) then hit Next > : In the next wizard panel select Source type: Select > Structured > _json : Scroll down to the indexes and make the following selections: Select Allowed Indexes > add all Default Index : main Next, hit the Review > button: The token review panel is now should and should look like the graphic below. Finally hit the Submit > button: A final confirmation message will be provided as shown below: If you return to Settings > Data Input > HTTPS Event Collector, you will now see the token your probe will need to communicate with the Splunk server: At this point, the Splunk server is ready to receive data from the probe. Ensure that your probe has been configured with the correct server IP address, port number and the token we have just created above (copy and paste the \u201cToken Value\u201d in to your probe config.ini file). If using the WLAN Pi, ensure that it is flipped in to wiperf mode. On the RPI, ensure that the required cron job has been configured to start polling.","title":"Create a HEC Token"},{"location":"splunk_configure/#perform-a-test-search","text":"After a few minutes, when the probe has run a test cycle, data should start to appear in Splunk. The quickest way to check is to do a general search for data in Splunk and see what is being received. Go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and enter a \u201c*\u201d in the \u201cNew Search\u201d text box. Results data should be seen as shown below: If your search result looks like this (no results found message), then you need to wait a little longer for data to arrive, or there is likely a comms problem between your probe and Splunk:","title":"Perform a Test Search"},{"location":"splunk_configure/#create-a-dashboard","text":"Now that we have data arriving at our Splunk server, we need to view the data in an interesting format. Splunk allows us to create a number of dashboards to visualize our data. We will now create a simple dashboard to demonstrate the visualization capabilities. In the WLANPi\u2019s /usr/share/wiperf/dashboards directory, a number of pre-canned dashboard files have been provided to allow a dashboard to be copied and pasted easily. These are also available on the GitHub page of the Wiperf project: https://github.com/wifinigel/wiperf/tree/master/dashboards Use an SFTP client to pull the \u201cprobe_summary.xml\u201d file from your probe or open the file on the GitHub page and select \u201cRaw\u201d to copy and paste the code in to a local file on your laptop. In the Splunk GUI, go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and hit the \u201cDashboards\u201d link: Hit the \"Create New Dashboard\" button: In the pop-up panel, enter a dashboard name and hit the \"Create Dashboard\" button: In the \u201cEdit Dashboard\u201d panel that opens, hit the \u201cSource\u201d button: By default, some basic XML configuration will exist in the dashboard definition: Open up the dashboard definition file previously downloaded from your probe or the GitHub site in a text editor. Then simply paste in the code as shown below (make sure the original code was all removed): After hitting the Save button, the dashboard will now be shown: Using the hostname and time period selector above the graphs, different probes and reporting periods may be viewed. The process above may be repeated using each of the xml files found in the dashboards folder to create a series of separate dashboards that focus on different aspects of data available using wiperf.","title":"Create a Dashboard"},{"location":"splunk_install/","text":"Splunk Installation Once the software is downloaded, follow the instructions that are appropriate for your OS in the Splunk installation manual (you can look for the latest guides via Google, but at the time of writing it was this manual): https://docs.splunk.com/Documentation/Splunk/latest/Installation/Chooseyourplatform The installation process for all platforms is very straightforward and is covered well in he official install guides, so will not be covered in detail here. Note When installing the Linux flavour of Splunk, make sure you do not miss the additional step required to ensure that Splunk starts after a server reboot. The following command needs to be executred after the software is installed (but please verify this isn the official installation documents): sudo /opt/splunk/bin/splunk enable boot-start Once installation has been completed, it should be possible to access the web dashboard of Splunk at the URL: http://<Splunk_server_IP>:8000","title":"Splunk Installation"},{"location":"splunk_install/#splunk-installation","text":"Once the software is downloaded, follow the instructions that are appropriate for your OS in the Splunk installation manual (you can look for the latest guides via Google, but at the time of writing it was this manual): https://docs.splunk.com/Documentation/Splunk/latest/Installation/Chooseyourplatform The installation process for all platforms is very straightforward and is covered well in he official install guides, so will not be covered in detail here. Note When installing the Linux flavour of Splunk, make sure you do not miss the additional step required to ensure that Splunk starts after a server reboot. The following command needs to be executred after the software is installed (but please verify this isn the official installation documents): sudo /opt/splunk/bin/splunk enable boot-start Once installation has been completed, it should be possible to access the web dashboard of Splunk at the URL: http://<Splunk_server_IP>:8000","title":"Splunk Installation"},{"location":"splunk_platform/","text":"Splunk Platform To collect and view the test results data, an instance of Splunk is required. Splunk is a very flexible data collection and reporting package that can take data sent by the wiperf probe and present it in a very nice report format. Splunk can be installed on a wide variety of platforms that can be viewed at : https://www.splunk.com/en_us/download/splunk-enterprise.html This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. Note that a free account sign-up is required when downloading the software from the link listed above. To install Splunk and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . The product being installed is Splunk Enterprise. This is a paid-for product, but it has a free-tier for low data volumes (500Mbytes per day). Install initially with all the licensing defaults and then drop back to the free-tier by selecting Settings > Licensing and selecting the free tier. The free tier is plenty for the low volume rates that the wiperf probe generates when deploying probes at small-scale. Attention If you forget to select the free tier and your trial license expires, you may become locked out of the GUI with a \u201clicense expired\u201d message. If this happens, from the CLI of your Splunk server, find the file \u201cserver.conf\u201d and add the following line to the bottom of the file: [license] active_group = Lite_Free Then, restart the Splunk server and the login issue should be fixed. (The file is /opt/splunk/etc/system/local/server.conf on Linux) Connectivity Planning One area to consider is connectivity between the wiperf probe and the Splunk instance. The wiperf probe needs to be able to access the Splunk server to send its data. If the wiperf probe probe is being deployed on a wireless network, how is the performance data generated going to get back to the Splunk server? If the probe is being deployed on a customer network to perform temporary monitoring, it will obviously join the wireless network under test. But how is the wiperf probe going to send its data to the Splunk server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, hence (potentially) bridging wired and wireless networks. Therefore, in many instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier. Three topology deployment options are supported: - Results data over wireless - Results data over Ethernet - Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the Splunk server. The 3 connectivity options are discussed below. Results Data Over Wireless In this topology the wiperf probe is configured to join an SSID that has the Splunk server accessible via its WLAN interface. Typically, the Splunk server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the Splunk server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: \\<public IP address of Splunk server\\> Results data over Ethernet If the Splunk server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of Splunk server> Results data over Zerotier/wireless A very simple way of getting the wiperf probe talking with your Splunk server is to use the Zerotier service to create a virtual network. In summary, both the Splunk server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same VLAN in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the Splunk server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees, It\u2019s very easy to use and get going, plus your Splunk server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the Splunk server and wiperf probe when the client is installed. Seriously, give it a go...it's quicker to try it than me explaining it here: https://www.zerotier.com/ config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of Splunk server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Splunk Platform"},{"location":"splunk_platform/#splunk-platform","text":"To collect and view the test results data, an instance of Splunk is required. Splunk is a very flexible data collection and reporting package that can take data sent by the wiperf probe and present it in a very nice report format. Splunk can be installed on a wide variety of platforms that can be viewed at : https://www.splunk.com/en_us/download/splunk-enterprise.html This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. Note that a free account sign-up is required when downloading the software from the link listed above. To install Splunk and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . The product being installed is Splunk Enterprise. This is a paid-for product, but it has a free-tier for low data volumes (500Mbytes per day). Install initially with all the licensing defaults and then drop back to the free-tier by selecting Settings > Licensing and selecting the free tier. The free tier is plenty for the low volume rates that the wiperf probe generates when deploying probes at small-scale. Attention If you forget to select the free tier and your trial license expires, you may become locked out of the GUI with a \u201clicense expired\u201d message. If this happens, from the CLI of your Splunk server, find the file \u201cserver.conf\u201d and add the following line to the bottom of the file: [license] active_group = Lite_Free Then, restart the Splunk server and the login issue should be fixed. (The file is /opt/splunk/etc/system/local/server.conf on Linux)","title":"Splunk Platform"},{"location":"splunk_platform/#connectivity-planning","text":"One area to consider is connectivity between the wiperf probe and the Splunk instance. The wiperf probe needs to be able to access the Splunk server to send its data. If the wiperf probe probe is being deployed on a wireless network, how is the performance data generated going to get back to the Splunk server? If the probe is being deployed on a customer network to perform temporary monitoring, it will obviously join the wireless network under test. But how is the wiperf probe going to send its data to the Splunk server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, hence (potentially) bridging wired and wireless networks. Therefore, in many instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier. Three topology deployment options are supported: - Results data over wireless - Results data over Ethernet - Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the Splunk server. The 3 connectivity options are discussed below.","title":"Connectivity Planning"},{"location":"splunk_platform/#results-data-over-wireless","text":"In this topology the wiperf probe is configured to join an SSID that has the Splunk server accessible via its WLAN interface. Typically, the Splunk server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the Splunk server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: \\<public IP address of Splunk server\\>","title":"Results Data Over Wireless"},{"location":"splunk_platform/#results-data-over-ethernet","text":"If the Splunk server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of Splunk server>","title":"Results data over Ethernet"},{"location":"splunk_platform/#results-data-over-zerotierwireless","text":"A very simple way of getting the wiperf probe talking with your Splunk server is to use the Zerotier service to create a virtual network. In summary, both the Splunk server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same VLAN in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the Splunk server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees, It\u2019s very easy to use and get going, plus your Splunk server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the Splunk server and wiperf probe when the client is installed. Seriously, give it a go...it's quicker to try it than me explaining it here: https://www.zerotier.com/ config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of Splunk server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Results data over Zerotier/wireless"},{"location":"splunk_software/","text":"Splunk Software To obtain the Splunk software for your data server, get along to the Splunk web site and sign up for an account if you don\u2019t already have one: https://www.splunk.com/en_us/download/splunk-enterprise.html Once you\u2019re logged in to the Splunk site, you\u2019ll have a number of OS options, so go ahead and choose your OS option ( supported platforms can be viewed here ). There are options for Windows, Linux & Mac OS: Once you've hit the download button, the Splunk Enterprise software chosen will start to download to your local machine, ready for installation. It\u2019s worth checking the download page to see if there are further download options. If you check the graphic below, you can see there is a \u201cDownload via Command Line (wget)\u201d option, which is a much easier way to get the code directly on to your server. The options you will see here will vary between OS selections:","title":"Splunk Software"},{"location":"splunk_software/#splunk-software","text":"To obtain the Splunk software for your data server, get along to the Splunk web site and sign up for an account if you don\u2019t already have one: https://www.splunk.com/en_us/download/splunk-enterprise.html Once you\u2019re logged in to the Splunk site, you\u2019ll have a number of OS options, so go ahead and choose your OS option ( supported platforms can be viewed here ). There are options for Windows, Linux & Mac OS: Once you've hit the download button, the Splunk Enterprise software chosen will start to download to your local machine, ready for installation. It\u2019s worth checking the download page to see if there are further download options. If you check the graphic below, you can see there is a \u201cDownload via Command Line (wget)\u201d option, which is a much easier way to get the code directly on to your server. The options you will see here will vary between OS selections:","title":"Splunk Software"},{"location":"troubleshooting/","text":"Troubleshooting: If things seem to be going wrong, try the following: Connect to the WLAN Pi using the USB OTG connection to check log files: cat /home/wlanpi/wiperf/logs/agent.log cat /home/wlanpi/wiperf/wiperf.log SSH to the device & tail the agent log file in real-time, watching for errors and dumps of test results being performed: tail -f /home/wlanpi/wiperf/logs/agent.log Flip back in to classic mode and activate Wiperf mode from the CLI of the WLAN Pi, watching for errors: cd /home/wlanpi/wiperf sudo ./wiperf_switcher Try disabling tests & see if one specific test is causing an issue Make sure all pre-reqs have definitely been fulfilled Make sure your WLAN Pi and Splunk servers are NTP sync'ed Flip back to classic mode and re-check the edits made to the config.ini & wpa_supplicant.conf files If you have changed the WLAN Pi hostname from its default, make sure you have updated both the /etc/hosts AND the /etc/hostname file as per the instructions [here][hostname_faq] (this can cause some very weird issues!) Check the order of DNS servers being used by running the command cat /etc/resolv.conf on the CLI of the WLAN Pi when it is in wiperf mode and connected to the wireless network. If there are multiple servers shown and you see 8.8.8.8 at the top of the list, you may need to move the 8.8.8.8 entry from the file /etc/resolvconf/resolv.conf.d/head to /etc/resolvconf/resolv.conf.d/tail and then reboot. This should shift 8.8.8.8 to be bottom of the list in cat /etc/resolv.conf and may fix your name resolution issues","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"If things seem to be going wrong, try the following: Connect to the WLAN Pi using the USB OTG connection to check log files: cat /home/wlanpi/wiperf/logs/agent.log cat /home/wlanpi/wiperf/wiperf.log SSH to the device & tail the agent log file in real-time, watching for errors and dumps of test results being performed: tail -f /home/wlanpi/wiperf/logs/agent.log Flip back in to classic mode and activate Wiperf mode from the CLI of the WLAN Pi, watching for errors: cd /home/wlanpi/wiperf sudo ./wiperf_switcher Try disabling tests & see if one specific test is causing an issue Make sure all pre-reqs have definitely been fulfilled Make sure your WLAN Pi and Splunk servers are NTP sync'ed Flip back to classic mode and re-check the edits made to the config.ini & wpa_supplicant.conf files If you have changed the WLAN Pi hostname from its default, make sure you have updated both the /etc/hosts AND the /etc/hostname file as per the instructions [here][hostname_faq] (this can cause some very weird issues!) Check the order of DNS servers being used by running the command cat /etc/resolv.conf on the CLI of the WLAN Pi when it is in wiperf mode and connected to the wireless network. If there are multiple servers shown and you see 8.8.8.8 at the top of the list, you may need to move the 8.8.8.8 entry from the file /etc/resolvconf/resolv.conf.d/head to /etc/resolvconf/resolv.conf.d/tail and then reboot. This should shift 8.8.8.8 to be bottom of the list in cat /etc/resolv.conf and may fix your name resolution issues","title":"Troubleshooting:"}]}